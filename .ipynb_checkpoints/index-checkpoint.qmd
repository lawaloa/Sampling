---
title: "COURSE 19 | SAMPLING AND POINT IN PYTHON"
author:
  - name:  "Lawal's Note"
    affiliation: "Associate Data Science Course in Python by DataCamp Inc"
date: "2024-12-26"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html:
    code-fold: true
    code-tools: true
  pdf:
    geometry:
      - top=30mm
      - left=20mm
  docx: default
execute:
  warning: false
  echo: true   
  eval: true  
  output: true 
  error: false   
  cache: false
  include_metadata: false
jupyter: python3
---

![](Sampling.jpg)

## Chapter 1: Introduction to Sampling

Learn what sampling is and why it is so powerful. You’ll also learn about the problems caused by convenience sampling and the differences between true randomness and pseudo-randomness.

### Chapter 1.1: Sampling and point estimates

Hi! Welcome to the course! I’m James, and I’ll be your host as we delve into the world of sampling data with Python. To start, let’s look at what sampling is and why it might be useful.

#### Estimating the population of France {.unnumbered}

Let's consider the problem of counting how many people live in France. The standard approach is to take a census. This means contacting every household and asking how many people live there. There are lots of people in France. Since there are millions of people in France, this is a really expensive process. Even with modern data collection technology, most countries will only conduct a census every five or ten years due to the cost.

#### Sampling households {.unnumbered}

In 1786, Pierre-Simon Laplace realized you could estimate the population with less effort. Rather than asking every household who lived there, he asked a small number of households and used statistics to estimate the number of people in the whole population. This technique of working with a subset of the whole population is called sampling.

#### Population vs. sample {.unnumbered}

Two definitions are important for this course. The population is the complete set of data that we are interested in. The previous example involved the literal population of France, but in statistics, it doesn't have to refer to people. One thing to bear in mind is that there is usually no equivalent of the census, so typically, we won't know what the whole population is like - more on this in a moment. 
The sample is the subset of data that we are working with.

#### Coffee rating dataset {.unnumbered}

Picture a dataset of professional ratings of coffees. Each row corresponds to one coffee, and there are thirteen hundred and thirty-eight rows in the dataset. The coffee is given a score from zero to one hundred, which is stored in the total_cup_points column. Other columns contain contextual information like the variety and country of origin and scores between zero and ten for attributes of the coffee such as aroma and body. These scores are averaged across all the reviewers for that particular coffee. It doesn't contain every coffee in the world, so we don't know exactly what the population of coffees is. However, there are enough here that we can think of it as our population of interest.

#### Points vs. flavor: population {.unnumbered}

Let's consider the relationship between cup points and flavor by selecting those two columns. This dataset contains all thirteen hundred and thirty-eight rows from the original dataset.

#### Points vs. flavor: 10 row sample {.unnumbered}

The pandas `.sample` method returns a random subset of rows. Setting n to ten means ten random rows are returned. By default, rows from the original dataset can't appear in the sample dataset multiple times, so we are guaranteed to have ten unique rows in our sample.

#### Python sampling for Series {.unnumbered}

The `.sample` method also works on pandas Series. Here, using square-bracket subsetting retrieves the `total_cup_points` column as a Series, and the `n` argument specifies how many random values to return.

#### Population parameters & point estimates {.unnumbered}

A population parameter is a calculation made on the population dataset. We aren't limited to counting values either; here, we calculate the mean of the cup points using NumPy. By contrast, a point estimate, or sample statistic, is a calculation based on the sample dataset. Here, the mean of the total cup points is calculated on the sample. Notice that the means are very similar but not identical.

#### Point estimates with pandas {.unnumbered}

Working with pandas can be easier than working with NumPy. These mean calculations can be performed using the `.mean` pandas method.

### Exercise 1.1.1

#### Simple sampling with pandas {.unnumbered}

Throughout this chapter, you'll be exploring song data from Spotify. Each row of this population dataset represents a song, and there are over 40,000 rows. Columns include the song name, the artists who performed it, the release year, and attributes of the song like its duration, tempo, and danceability. You'll start by looking at the durations.

Your first task is to sample the Spotify dataset and compare the mean duration of the population with the sample.

#### Instructions {.unnumbered}

1. Sample 1000 rows from `spotify`, assigning to `spotify_sample`.
2. Calculate the mean duration in minutes from `spotify` using pandas.
3. Calculate the mean duration in minutes from `spotify_sample` using pandas.

```{python}
# Importing pandas
import pandas as pd

# Importing the course arrays
spotify = pd.read_feather("datasets/spotify_2000_2020.feather")


# Sample 1000 rows from spotify_population
spotify_sample = spotify.sample(n=1000)

# Print the sample
print(spotify_sample)

# Calculate the mean duration in mins from spotify_population
mean_dur_pop = spotify['duration_minutes'].mean()

# Calculate the mean duration in mins from spotify_sample
mean_dur_samp = spotify_sample['duration_minutes'].mean()

# Print the means
print(mean_dur_pop)
print(mean_dur_samp)
```

::: {.callout-note}
*Notice that the mean song duration in the sample is similar, but not identical to the mean song duration in the whole population.*
:::

### Exercise 1.1.2

#### Simple sampling and calculating with NumPy {.unnumbered}

You can also use numpy to calculate parameters or statistics from a list or pandas Series.

You'll be turning it up to eleven and looking at the loudness property of each song.


#### Instructions {.unnumbered}

1. Create a pandas Series, `loudness_pop`, by subsetting the `loudness` column from `spotify`.
  - Sample `loudness_pop` to get 100 random values, assigning to `loudness_samp`.

2. Calculate the mean of `loudness_pop` using `numpy`.
3. Calculate the mean of `loudness_samp` using `numpy`.

```{python}
# Importing pandas
import pandas as pd
import numpy as np

# Importing the course arrays
spotify = pd.read_feather("datasets/spotify_2000_2020.feather")

# Create a pandas Series from the loudness column of spotify_population
loudness_pop = spotify['loudness']

# Sample 100 values of loudness_pop
loudness_samp = loudness_pop.sample(n=100)

print(loudness_samp)

# Calculate the mean of loudness_pop
mean_loudness_pop = np.mean(loudness_pop)

# Calculate the mean of loudness_samp
mean_loudness_samp = np.mean(loudness_samp)

print(mean_loudness_pop)
print(mean_loudness_samp)
```

::: {.callout-note}
*Again, notice that the calculated value (the mean) is close but not identical in each case.*
:::

### Chapter 1.2: Convenience sampling

The point estimates you calculated in the previous exercises were very close to the population parameters that they were based on, but is this always the case?

#### The Literary Digest election prediction {.unnumbered}

In 1936, a newspaper called The Literary Digest ran an extensive poll to try to predict the next US presidential election. They phoned ten million voters and had over two million responses. About one-point-three million people said they would vote for Landon, and just under one million people said they would vote for Roosevelt. That is, Landon was predicted to get fifty-seven percent of the vote, and Roosevelt was predicted to get forty-three percent of the vote. Since the sample size was so large, it was presumed that this poll would be very accurate. However, in the election, Roosevelt won by a landslide with sixty-two percent of the vote. So what went wrong? Well, in 1936, telephones were a luxury, so the only people who had been contacted by The Literary Digest were relatively rich. The sample of voters was not representative of the whole population of voters, and so the poll suffered from sample bias. The data was collected by the easiest method, in this case, telephoning people. This is called convenience sampling and is often prone to sample bias. Before sampling, we need to think about our data collection process to avoid biased results.

#### Finding the mean age of French people {.unnumbered}

Let's look at another example. While on vacation at Disneyland Paris, you start wondering about the mean age of French people. To get an answer, you ask ten people stood nearby about their ages. Their mean age is twenty-four-point-six years old. Do you think this will be a good estimate of the mean age of all French citizens?

#### How accurate was the survey? {.unnumbered}

On the left, you can see mean ages taken from the French census. Notice that the population has been gradually getting older as birth rates decrease and life expectancy increases. In 2015, the mean age was over forty, so our estimate of twenty-four-point-six is way off. The problem is that the family-friendly fun at Disneyland means that the sample ages weren't representative of the general population. There are generally more eight-year-olds than eighty-year-olds riding rollercoasters.

#### Convenience sampling coffee ratings {.unnumbered}

Let's return to the coffee ratings dataset and look at the mean cup points population parameter. The mean is about eighty-two. One form of convenience sampling would be to take the first ten rows, rather than the random rows we saw in the previous video. We can take the first 10 rows with the pandas `head` method. The mean cup points from this sample is higher at eighty-nine. The discrepancy suggests that coffees with higher cup points appear near the start of the dataset. Again, the convenience sample isn't representative of the whole population.

#### Visualizing selection bias {.unnumbered}

Histograms are a great way to visualize the selection bias. We can create a histogram of the total cup points from the population, which contains values ranging from around 59 to around 91. The `np.arange` function can be used to create bins of width 2 from 59 to 91. Recall that the stop value in `np.arange` is exclusive, so we specify 93, not 91. Here's the same code to generate a histogram for the convenience sample.

#### Distribution of a population and of a convenience sample {.unnumbered}

Comparing the two histograms, it is clear that the distribution of the sample is not the same as the population: all of the sample values are on the right-hand side of the plot.

#### Visualizing selection bias for a random sample {.unnumbered}

This time, we'll compare the `total_cup_points` distribution of the population with a random sample of 10 coffees.

#### Distribution of a population and of a simple random sample {.unnumbered}

Notice how the shape of the distributions is more closely aligned when random sampling is used.

### Exercise 1.2.1

#### Are findings from the sample generalizable? {.unnumbered}

You just saw how convenience sampling—collecting data using the easiest method—can result in samples that aren't representative of the population. Equivalently, this means findings from the sample are not generalizable to the population. Visualizing the distributions of the population and the sample can help determine whether or not the sample is representative of the population.

The Spotify dataset contains an `acousticness` column, which is a confidence measure from zero to one of whether the track was made with instruments that aren't plugged in. You'll compare the `acousticness` distribution of the total population of songs with a sample of those songs.

#### Instructions {.unnumbered}

1. Plot a histogram of the `acousticness` from `spotify` with bins of width `0.01` from `0` to `1` using pandas `.hist()`.
2. Update the histogram code to use the `spotify_mysterious_sample` dataset.

```{python}
# Importing pandas
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Importing the course arrays
spotify = pd.read_feather("datasets/spotify_2000_2020.feather")

# Visualize the distribution of acousticness with a histogram
spotify['acousticness'].hist(bins=np.arange(0,1.01,0.01))
plt.show()

# Generate a convenience sample where acousticness is consistently higher
spotify_high_acousticness = spotify[(spotify['acousticness'] >= 0.85) & (spotify['acousticness'] <= 1.0)]

# Sample 1107 entries from the high acousticness subset
spotify_mysterious_sample = spotify_high_acousticness.sample(n=1107)

# Update the histogram to use spotify_mysterious_sample
spotify_mysterious_sample['acousticness'].hist(bins=np.arange(0, 1.01, 0.01))
plt.show()
```

### Question 

*Compare the two histograms you drew*. Are the `acousticness` values in the sample generalizable to the general population?

**No. The acousticness samples are consistently higher than those in the general population.**

**The acousticness values in the sample are all greater than 0.85, whereas they range from 0 to 1 in the whole population.**

### Exercise 1.2.2

#### Are these findings generalizable? {.unnumbered}

Let's look at another sample to see if it is representative of the population. This time, you'll look at the `duration_minutes` column of the `Spotify` dataset, which contains the length of the song in minutes.

#### Instructions {.unnumbered}

- Plot a histogram of `duration_minutes` from `spotify` with bins of width `0.5` from `0` to `15` using pandas `.hist()`.
- Update the histogram code to use the `spotify_mysterious_sample2` dataset.

```{python}
# Importing pandas
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Importing the course arrays
spotify = pd.read_feather("datasets/spotify_2000_2020.feather")

# Generate a convenience sample where duration_minutes is within the specified range
spotify_duration_range = spotify[(spotify['duration_minutes'] >= 0.8079999999) & (spotify['duration_minutes'] <= 9.822)]

# Sample 50 entries from the spotify_mysterious_sample2 dataset
spotify_mysterious_sample2 = spotify_duration_range.sample(n=50)

# Visualize the distribution of duration_minutes in the population with a histogram
spotify['duration_minutes'].hist(bins=np.arange(0,15.5,0.5))
plt.show()

# Visualize the distribution of duration_minutes as a histogram
spotify_mysterious_sample2['duration_minutes'].hist(bins=np.arange(0, 15.5, 0.5))
plt.show()
```

### Question

*Compare the two histograms you drew*. Are the duration values in the sample generalizable to the general population?

#### Answer

Yes. The sample selected is likely a random sample of all songs in the population.

*The duration values in the sample show a similar distribution to those in the whole population, so the results are generalizable.*

### Chapter 1.3: Pseudo-random number generation

You previously saw how to use a random sample to get results similar to those in the population. But how does a computer actually do this random sampling?

#### What does random mean? {.unnumbered}

There are several meanings of random in English. This definition from Oxford Languages is the most interesting for us. If we want to choose data points at random from a population, we shouldn't be able to predict which data points would be selected ahead of time in some systematic way.

#### True random numbers {.unnumbered}

To generate truly random numbers, we typically have to use a physical process like flipping coins or rolling dice. The Hotbits service generates numbers from radioactive decay, and RANDOM.ORG generates numbers from atmospheric noise, which are radio signals generated by lightning. Unfortunately, these processes are fairly slow and expensive for generating random numbers.

[https://www.fourmilab.ch/hotbits](https://www.fourmilab.ch/hotbits)

[https://www.random.org](https://www.random.org)

#### Pseudo-random number generation {.unnumbered}

For most use cases, pseudo-random number generation is better since it is cheap and fast. Pseudo-random means that although each value appears to be random, it is actually calculated from the previous random number. Since you have to start the calculations somewhere, the first random number is calculated from what is known as a seed value. The word random is in quotes to emphasize that this process isn't really random. If we start from a particular seed value, all future numbers will be the same.

#### Pseudo-random number generation example {.unnumbered}

For example, suppose we have a function to generate pseudo-random values called `calc_next_random`. To begin, we pick a seed number, in this case, one. `calc_next_random` does some calculations and returns three. We then feed three into `calc_next_random`, and it does the same set of calculations and returns two. And if we can keep feeding in the last number, it will return something apparently random. Although the process is deterministic, the trick to a random number generator is to make it look like the values are random.

#### Random number generating functions {.unnumbered}

NumPy has many functions for generating random numbers from statistical distributions. To use each of these, make sure to prepend each function name with `numpy.random` or `np.random`. Some of them, like `.uniform` and `.normal`, may be familiar. Others have more niche applications.

#### Visualizing random numbers {.unnumbered}

Let's generate some pseudo-random numbers. The first arguments to each random number function specify distribution parameters. The size argument specifies how many numbers to generate, in this case, five thousand. We've chosen the beta distribution, and its parameters are named a and b. These random numbers come from a continuous distribution, so a great way to visualize them is with a histogram. Here, because the numbers were generated from the beta distribution, all the values are between zero and one.

#### Random numbers seeds {.unnumbered}

To set a random seed with NumPy, we use the `.random.seed` method. `Random.seed` takes an integer for the seed number, which can be any number you like. `.normal` generates pseudo-random numbers from the normal distribution. The loc and scale arguments set the mean and standard deviation of the distribution, and the size argument determines how many random numbers from that distribution will be returned. If we call `.normal` a second time, we get two different random numbers. If we reset the seed by calling `random.seed` with the same seed number, then call `.normal` again, we get the same numbers as before. This makes our code reproducible.

#### Using a different seed {.unnumbered}

Now let's try a different seed. This time, calling `.normal` generates different numbers.

### Exercise 1.3.1

#### Generating random numbers {.unnumbered}

You've used `.sample()` to generate pseudo-random numbers from a set of values in a DataFrame. A related task is to generate random numbers that follow a statistical distribution, like the uniform distribution or the normal distribution.

Each random number generation function has distribution-specific arguments and an argument for specifying the number of random numbers to generate.

#### Instructions {.unnumbered}

1. Generate 5000 numbers from a uniform distribution, setting the parameters `low` to `-3` and `high` to `3`.
2. Generate 5000 numbers from a normal distribution, setting the parameters `loc` to `5` and `scale` to `2`.
3. Plot a histogram of `uniforms` with `bins` of width of `0.25` from `-3` to `3` using `plt.hist()`.
4. Plot a histogram of `normals` with `bins` of width of `0.5` from `-2` to `13` using `plt.hist()`.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate random numbers from a Uniform(-3, 3)
uniforms = np.random.uniform(low=-3, high=3, size=5000)

# Print uniforms
print(uniforms)

# Generate random numbers from a Normal(5, 2)
normals = np.random.normal(loc=5, scale = 2, size= 5000)

# Print normals
print(normals)

# Plot a histogram of uniform values, binwidth 0.25
plt.hist(uniforms, bins=np.arange(-3,3.25,0.25))
plt.show()

# Plot a histogram of normal values, binwidth 0.5
plt.hist(normals, bins = np.arange(-2, 13.5, 0.5))
plt.show()
```

### Exercise 1.3.2

#### Understanding random seeds {.unnumbered}

While random numbers are important for many analyses, they create a problem: the results you get can vary slightly. This can cause awkward conversations with your boss when your script for calculating the sales forecast gives different answers each time.

Setting the seed for `numpy`'s random number generator helps avoid such problems by making the random number generation reproducible. 

##### Question 1 {.unnumbered}

Which statement about x and y is true?

```
import numpy as np
np.random.seed(123)
x = np.random.normal(size=5)
y = np.random.normal(size=5)
```

*The values of x are different from those of y*

##### Question 2 {.unnumbered}

Which statement about x and y is true?

```
import numpy as np
np.random.seed(123)
x = np.random.normal(size=5)
np.random.seed(123)
y = np.random.normal(size=5)
```

*x and y have identical values.*

##### Question 3 {.unnumbered}

Which statement about x and y is true?

```
import numpy as np
np.random.seed(123)
x = np.random.normal(size=5)
np.random.seed(456)
y = np.random.normal(size=5)
```

*The values of x are different from those of y.*

## CHAPTER 2: Sampling Methods 

It’s time to get hands-on and perform the four random sampling methods in Python: simple, systematic, stratified, and cluster.

### Chapter 2.1: Simple random and systematic sampling

There are several methods of sampling from a population. In this video, we'll look at simple random sampling and systematic random sampling.

#### Simple random sampling {.unnumbered}

Simple random sampling works like a raffle or lottery. We start with our population of raffle tickets or lottery balls and randomly pick them out one at a time.

#### Simple random sampling of coffees {.unnumbered}

In our coffee ratings dataset, instead of raffle tickets or lottery balls, the population consists of coffee varieties. To perform simple random sampling, we take some at random, one at a time. Each coffee has the same chance as any other of being picked. When using this technique, sometimes we might end up with two coffees that were next to each other in the dataset, and sometimes we might end up with large areas of the dataset that were not selected from at all.

#### Simple random sampling with pandas {.unnumbered}

We've already seen how to do simple random sampling with pandas. We call `.sample` and set `n` to the size of the sample. We can also set the seed using the random_state argument to generate reproducible results, just like we did pseudo-random number generation. Previously, by not setting `random_state` when sampling, our code would generate a different random sample each time it was run.

#### Systematic sampling {.unnumbered}

Another sampling method is known as systematic sampling. This samples the population at regular intervals. Here, looking from top to bottom and left to right within each row, every fifth coffee is sampled.

#### Systematic sampling - defining the interval {.unnumbered}

Systematic sampling with pandas is slightly trickier than simple random sampling. The tricky part is determining how big the interval between each row should be for a given sample size. Suppose we want a sample size of five coffees. The population size is the number of rows in the whole dataset, and in this case, it's one thousand three hundred and thirty-eight. The interval is the population size divided by the sample size, but because we want the answer to be an integer, we perform integer division with two forward slashes. This is like standard division but discards any fractional part. One-three-three-eight divided by five is actually two hundred and sixty-seven-point-six, and discarding the fractional part leaves two hundred and sixty-seven. Thus, to get a systematic sample of five coffees, we will select every two hundred sixty-seventh coffee in the dataset.

#### Systematic sampling - selecting the rows {.unnumbered}

To select every two hundred and sixty-seventh row, we call dot-iloc on coffee_ratings and pass double-colons and the interval, which is 267 in this case. Double-colon interval tells pandas to select every two hundred and sixty-seventh row from zero to the end of the DataFrame.

#### The trouble with systematic sampling {.unnumbered}

There is a problem with systematic sampling, though. Suppose we are interested in statistics about the aftertaste attribute of the coffees. To examine this, first, we use `reset_index` to create a column of index values in our DataFrame that we can plot. Plotting aftertaste against index shows a pattern. Earlier rows generally have higher aftertaste scores than later rows. This introduces bias into the statistics that we calculate. In general, it is only safe to use systematic sampling if a plot like this has no pattern; that is, it just looks like noise.

#### Making systematic sampling safe {.unnumbered}

To ensure that systematic sampling is safe, we can randomize the row order before sampling. dot-sample has an argument named frac that lets us specify the proportion of the dataset to return in the sample, rather than the absolute number of rows that n specifies. Setting frac to one randomly samples the whole dataset. In effect, this randomly shuffles the rows. Next, the indices need to be reset so that they go in order from zero again. Specifying drop equals True clears the previous row indexes, and chaining to another `reset_index` call creates a column containing these new indexes. Redrawing the plot with the shuffled dataset shows no pattern between aftertaste and index. This is great, but note that once we've shuffled the rows, systematic sampling is essentially the same as simple random sampling.

### Exercise 2.1.1

#### Simple random sampling {.unnumbered}

The simplest method of sampling a population is the one you've seen already. It is known as *simple random sampling* (sometimes abbreviated to "SRS"), and involves picking rows at random, one at a time, where each row has the same chance of being picked as any other.

In this chapter, you'll apply sampling methods to a synthetic (fictional) employee attrition dataset from IBM, where "attrition" in this context means leaving the company.

#### Instructions {.unnumbered}

- Sample 70 rows from `attrition` using simple random sampling, setting the random seed to 18900217.
- Print the sample dataset, `attrition_samp`. What do you notice about the indices?

```{python}
# Importing pandas
import pandas as pd

# Importing the course arrays
attrition = pd.read_feather("datasets/attrition.feather")

# Sample 70 rows using simple random sampling and set the seed
attrition_samp = attrition.sample(n=70, random_state=18900217)

# Print the sample
print(attrition_samp)
```

### Exercise 2.1.2

#### Systematic sampling {.unnumbered}

One sampling method that avoids randomness is called systematic sampling. Here, you pick rows from the population at regular intervals.

For example, if the population dataset had one thousand rows, and you wanted a sample size of five, you could pick rows 0, 200, 400, 600, and 800.

#### Instructions {.unnumbered}

1.Set the sample size to 70.
   - Calculate the population size from `attrition`.
   - Calculate the interval between the rows to be sampled.

2. Systematically sample `attrition` to get the rows of the population at each `interval`, starting at 0; assign the rows to `attrition_sys_samp`

```{python}
# Importing pandas
import pandas as pd

# Importing the course arrays
attrition = pd.read_feather("datasets/attrition.feather")

# Set the sample size to 70
sample_size = 70

# Calculate the population size from attrition_pop
pop_size = len(attrition)

# Calculate the interval
interval = pop_size//sample_size

# Systematically sample 70 rows
attrition_sys_samp = attrition.iloc[::interval]

# Print the sample
print(attrition_sys_samp)
```

### Exercise 2.1.3

#### Is systematic sampling OK? {.unnumbered}

Systematic sampling has a problem: if the data has been sorted, or there is some sort of pattern or meaning behind the row order, then the resulting sample may not be representative of the whole population. The problem can be solved by shuffling the rows, but then systematic sampling is equivalent to simple random sampling.

Here you'll look at how to determine whether or not there is a problem.

#### Instructions {.unnumbered}

1. Add an index column to `attrition`, assigning the result to `attrition_id`.
    - Create a scatter plot of `YearsAtCompany` versus `index` for `attrition_id` using pandas `.plot()`.
2. Randomly shuffle the rows of `attrition`.
    - Reset the row indexes, and add an `index` column to `attrition`.
    - Repeat the scatter plot of `YearsAtCompany` versus `index`, this time using `attrition_shuffled`.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Importing the course arrays
attrition = pd.read_feather("datasets/attrition.feather")

# Add an index column to attrition_pop
attrition_id = attrition.reset_index()

# Plot YearsAtCompany vs. index for attrition_pop_id
attrition_id.plot(x="index", y="YearsAtCompany", kind="scatter")
plt.show()

# Shuffle the rows of attrition_pop
attrition_shuffled = attrition.sample(frac=1)

# Reset the row indexes and create an index column
attrition_shuffled = attrition_shuffled.reset_index(drop=True).reset_index()

# Plot YearsAtCompany vs. index for attrition_shuffled
attrition_shuffled.plot(x="index", y="YearsAtCompany", kind="scatter")
plt.show()
```

::: {.callout-note}
### Question {.unnumbered}

Does a systematic sample always produce a sample similar to a simple random sample?

*No, Systematic sampling has problems when the data are sorted or contain a pattern. Shuffling the rows makes it equivalent to simple random sampling*.
:::

### Chapter 2.2: Stratified and weighted random sampling 

Stratified sampling is a technique that allows us to sample a population that contains subgroups.

#### Coffees by country  {.unnumbered}

For example, we could group the coffee ratings by country. If we count the number of coffees by country using the `value_counts` method, we can see that these six countries have the most data.

1.	1 The dataset lists Hawaii and Taiwan as countries for convenience, as they are notable coffee-growing regions.

#### Filtering for 6 countries  {.unnumbered}

To make it easier to think about sampling subgroups, let's limit our analysis to these six countries. We can use the `.isin` method to filter the population and only return the rows corresponding to these six countries. This filtered dataset is stored as `coffee_ratings_top`.

#### Counts of a simple random sample  {.unnumbered}

Let's take a ten percent simple random sample of the dataset using `.sample` with `frac` set to 0.1. We also set the `random_state` argument to ensure reproducibility. As with the whole dataset, we can look at the counts for each country. To make comparisons easier, we set `normalize` to `True` to convert the counts into a proportion, which shows what proportion of coffees in the sample came from each country.

#### Comparing proportions  {.unnumbered}

Here are the proportions for the population and the ten percent sample side by side. Just by chance, in this sample, Taiwanese coffees form a disproportionately low percentage. The different makeup of the sample compared to the population could be a problem if we want to analyze the country of origin, for example.

#### Proportional stratified sampling  {.unnumbered}

If we care about the proportions of each country in the sample closely matching those in the population, then we can group the data by country before taking the simple random sample. Note that we used the Python line continuation backslash here, which can be useful for breaking up longer chains of pandas code like this. Calling the `.sample` method after grouping takes a simple random sample within each country. Now the proportions of each country in the stratified sample are much closer to those in the population.

#### Equal counts stratified sampling  {.unnumbered}

One variation of stratified sampling is to sample equal counts from each group, rather than an equal proportion. The code only has one change from before. This time, we use the `n` argument in `.sample` instead of `frac` to extract fifteen randomly-selected rows from each country. Here, the resulting sample has equal proportions of one-sixth from each country.

#### Weighted random sampling {.unnumbered}

A close relative of stratified sampling that provides even more flexibility is weighted random sampling. In this variant, we create a column of weights that adjust the relative probability of sampling each row. For example, suppose we thought that it was important to have a higher proportion of Taiwanese coffees in the sample than in the population. We create a condition, in this case, rows where the country of origin is Taiwan. Using the `where` function from NumPy, we can set a weight of two for rows that match the condition and a weight of one for rows that don't match the condition. This means when each row is randomly sampled, Taiwanese coffees have two times the chance of being picked compared to other coffees. When we call `.sample`, we pass the column of weights to the weights argument.

#### Weighted random sampling results  {.unnumbered}

Here, we can see that Taiwan now contains seventeen percent of the sampled dataset, compared to eight-point-five percent in the population. This sort of weighted sampling is common in political polling, where we need to correct for under- or over-representation of demographic groups.

#### Exercise 2.2.1

#### Proportional stratified sampling {.unnumbered}

If you are interested in subgroups within the population, then you may need to carefully control the counts of each subgroup within the population. *Proportional stratified sampling* results in subgroup sizes within the sample that are representative of the subgroup sizes within the population. It is equivalent to performing a simple random sample on each subgroup.

#### Instructions {.unnumbered}

1. Get the proportion of employees by Education level from `attrition`.
2. Use proportional stratified sampling on `attrition_pop` to sample 40% of each `Education` group, setting the seed to `2022`.
3. Get the proportion of employees by `Education` level from `attrition_strat`.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Importing the course arrays
attrition = pd.read_feather("datasets/attrition.feather")

# Proportion of employees by Education level
education_counts_pop = attrition['Education'].value_counts(normalize=True)

# Print education_counts_pop
print(education_counts_pop)

# Proportional stratified sampling for 40% of each Education group
attrition_strat = attrition.groupby('Education')\
.sample(frac=0.4, random_state=2022)

# Print the sample
print(attrition_strat)

# Calculate the Education level proportions from attrition_strat
education_counts_strat = attrition_strat['Education'].value_counts(normalize=True)

# Print education_counts_strat
print(education_counts_strat)
```

::: {.callout-note}
*By grouping then sampling, the size of each group in the sample is representative of the size of the sample in the population*.
:::

### Exercise 2.2.2

#### Equal counts stratified sampling {.unnumbered}

If one subgroup is larger than another subgroup in the population, but you don't want to reflect that difference in your analysis, then you can use *equal counts stratified sampling* to generate samples where each subgroup has the same amount of data. For example, if you are analyzing blood types, O is the most common blood type worldwide, but you may wish to have equal amounts of O, A, B, and AB in your sample.

#### Instructions {.unnumbered}

1. Use equal counts stratified sampling on `attrition` to get 30 employees from each `Education` group, setting the seed to `2022`.
2. Get the proportion of employees by `Education` level from `attrition_eq`.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Importing the course arrays
attrition = pd.read_feather("datasets/attrition.feather")

# Proportion of employees by Education level
education_counts_pop = attrition['Education'].value_counts(normalize=True)

# Print education_counts_pop
print(education_counts_pop)

# Get 30 employees from each Education group
attrition_eq = attrition.groupby('Education')\
.sample(n=30, random_state=2022)

# Print the sample
print(attrition_eq)

# Get the proportions from attrition_eq
education_counts_eq = attrition_eq['Education'].value_counts(normalize=True)

# Print the results
print(education_counts_eq)
```

::: {.callout-note}
*If you want each subgroup to have equal weight in your analysis, then equal counts stratified sampling is the appropriate technique.*
:::

### Exercise 2.2.3

#### Weighted sampling {.unnumbered}

Stratified sampling provides rules about the probability of picking rows from your dataset at the subgroup level. A generalization of this is weighted sampling, which lets you specify rules about the probability of picking rows at the row level. The probability of picking any given row is proportional to the weight value for that row.

#### Instructions {.unnumbered}

1. Plot `YearsAtCompany` from `attrition` as a histogram with bins of width `1` from `0` to `40`.
2. Sample 400 employees from `attrition` weighted by `YearsAtCompany`.
3. Plot `YearsAtCompany` from `attrition_weight` as a histogram with bins of width `1` from `0` to `40`.
4. Which is higher? The mean `YearsAtCompany` from `attrition` or the mean `YearsAtCompany` from `attrition_weight`? **Answer**: *The weighted sample mean is around 11, which is higher than the population mean of around 7. The fact that the two numbers are different means that the weighted simple random sample is biased.*

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Importing the course arrays
attrition = pd.read_feather("datasets/attrition.feather")

# Plot YearsAtCompany from attrition_pop as a histogram
attrition['YearsAtCompany'].hist(bins=np.arange(0,41,1))

# Sample 400 employees weighted by YearsAtCompany
attrition_weight = attrition.sample(n=400, weights='YearsAtCompany')

# Print the sample
print(attrition_weight)

# Plot YearsAtCompany from attrition_weight as a histogram
attrition_weight['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))
plt.show()

# The mean YearsAtCompany from attrition dataset 
print(attrition['YearsAtCompany'].mean())

# The mean YearsAtCompany from attrition_weight 
print(attrition_weight['YearsAtCompany'].mean())
```

### Chapter 2.3: Cluster sampling

One problem with stratified sampling is that we need to collect data from every subgroup. In cases where collecting data is expensive, for example, when we have to physically travel to a location to collect it, it can make our analysis prohibitively expensive. There's a cheaper alternative called cluster sampling.

#### Stratified sampling vs. cluster sampling {.unnumbered}

The stratified sampling approach was to split the population into subgroups, then use simple random sampling on each of them. Cluster sampling means that we limit the number of subgroups in the analysis by picking a few of them with simple random sampling. We then perform simple random sampling on each subgroup as before.

#### Varieties of coffee {.unnumbered}

Let's return to the coffee dataset and look at the varieties of coffee. In this image, each bean represents the whole subgroup rather than an individual coffee, and there are twenty-eight of them. To extract unique varieties, we use the `.unique` method. This returns an array, so wrapping it in the list function creates a list of unique varieties. Let's suppose that it's expensive to work with all of the different varieties. Enter cluster sampling.

#### Stage 1: sampling for subgroups {.unnumbered}

The first stage of cluster sampling is to randomly cut down the number of varieties, and we do this by randomly selecting them. Here, we've used the `random.sample` function from the random package to get three varieties, specified using the argument k.

#### Stage 2: sampling each group {.unnumbered}

The second stage of cluster sampling is to perform simple random sampling on each of the three varieties we randomly selected. We first filter the dataset for rows where the variety is one of the three selected, using the `.isin` method. To ensure that the `isin` filtering removes levels with zero rows, we apply the `cat.remove_unused_categories` method on the Series of focus, which is variety here. If we exclude this method, we might receive an error when sampling by variety level. The pandas code is the same as for stratified sampling. Here, we've opted for equal counts sampling, with five rows from each remaining variety.

#### Stage 2 output {.unnumbered}

Here's the first few columns of the result. Notice that there are the fifteen rows, which we'd expect from sampling five rows from three varieties.

#### Multistage sampling {.unnumbered}

Note that we had two stages in the cluster sampling. We randomly sampled the subgroups to include, then we randomly sampled rows from those subgroups. Cluster sampling is a special case of multistage sampling. It's possible to use more than two stages. A common example is national surveys, which can include several levels of administrative regions, like states, counties, cities, and neighborhoods.

### Exercise 2.3.1

#### Performing cluster sampling {.unnumbered}

Now that you know when to use cluster sampling, it's time to put it into action. In this exercise, you'll explore the JobRole column of the attrition dataset. You can think of each job role as a subgroup of the whole population of employees.

Use a seed of 19790801 to set the seed with `random.seed()`.

#### Instructions {.unnumbered}

1. 
 - Create a list of unique `JobRole` values from `attrition`, and assign to `job_roles_pop`.
 - Randomly sample four `JobRole` values from `job_roles_pop`.
2. Subset attrition_pop for the sampled job roles by filtering for rows where `JobRole` is in `job_roles_samp`.
3.
- Remove any unused categories from `JobRole`.
- For each job role in the filtered dataset, take a random sample of ten rows, setting the seed to `2022`.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Importing the course arrays
attrition = pd.read_feather("datasets/attrition.feather")

# Set the seed
random.seed(19790801)

# Create a list of unique JobRole values
job_roles_pop = list(attrition['JobRole'].unique())

# Randomly sample four JobRole values
job_roles_samp = random.sample(job_roles_pop, k=4)

# Print the result
print(job_roles_samp)

# Filter for rows where JobRole is in job_roles_samp
jobrole_condition = attrition['JobRole'].isin(job_roles_samp)
attrition_filtered = attrition[jobrole_condition]

# Print the result
print(attrition_filtered)

# Remove categories with no rows
attrition_filtered['JobRole'] = attrition_filtered['JobRole'].cat.remove_unused_categories()

# Randomly sample 10 employees from each sampled job role
attrition_clust = attrition_filtered.groupby('JobRole')\
.sample(n=10, random_state=2022)


# Print the sample
print(attrition_clust)
```

### Chapter 2.4: Comparing sampling methods

Let's review the various sampling techniques we learned about.

#### Review of sampling techniques - setup {.unnumbered}

For convenience, we'll stick to the six countries with the most coffee varieties that we used before. This corresponds to eight hundred and eighty rows and eight columns, retrieved using the `.shape` attribute.

#### Review of simple random sampling {.unnumbered}

Simple random sampling uses `.sample` with either `n` or `frac` set to determine how many rows to pseudo-randomly choose, given a seed value set with `random_state`. The simple random sample returns two hundred and ninety-three rows because we specified `frac` as one-third, and one-third of eight hundred and eighty is just over two hundred and ninety-three.

#### Review of stratified sampling {.unnumbered}

Stratified sampling groups by the country subgroup before performing simple random sampling on each subgroup. Given each of these top countries have quite a few rows, stratifying produces the same number of rows as the simple random sample.

#### Review of cluster sampling {.unnumbered}

In the cluster sample, we've used two out of six countries to roughly mimic frac equals one-third from the other sample types. Setting n equal to one-sixth of the total number of rows gives roughly equal sample sizes in each of the two subgroups. Using `.shape` again, we see that this cluster sample has close to the same number of rows: two-hundred-ninety-two compared to two-hundred-ninety-three for the other sample types.

#### Calculating mean cup points {.unnumbered}

Let's calculate a population parameter, the mean of the total cup points. When the population parameter is the mean of a field, it's often called the population mean. Remember that in real-life scenarios, we typically wouldn't know what the population mean is. Since we have it here, though, we can use this value of eighty-one-point-nine as a gold standard to measure against. Now we'll calculate the same value using each of the sampling techniques we've discussed. These are point estimates of the mean, often called sample means. The simple and stratified sample means are really close to the population mean. Cluster sampling isn't quite as close, but that's typical. Cluster sampling is designed to give us an answer that's almost as good while using less data.

#### Mean cup points by country: simple random {.unnumbered}

Here's a slightly more complicated calculation of the mean total cup points for each country. We group by country before calculating the mean to return six numbers. So how do the numbers from the simple random sample compare? The sample means are pretty close to the population means.

#### Mean cup points by country: stratified {.unnumbered}

The same is true of the sample means from the stratified technique. Each sample mean is relatively close to the population mean.

#### Mean cup points by country: cluster {.unnumbered}

With cluster sampling, while the sample means are pretty close to the population means, the obvious limitation is that we only get values for the two countries that were included in the sample. If the mean cup points for each country is an important metric in our analysis, cluster sampling would be a bad idea.

### Exercise 2.4.1

#### 3 kinds of sampling {.unnumbered}

You're going to compare the performance of point estimates using simple, stratified, and cluster sampling. Before doing that, you'll have to set up the samples.

You'll use the `RelationshipSatisfaction` column of the `attrition` dataset, which categorizes the employee's relationship with the company. It has four levels: `Low`, `Medium`, `High`, and `Very_High`. 

#### Instructions {.unnumbered}

1. Perform simple random sampling on `attrition` to get one-quarter of the population, setting the seed to `2022`.
2. Perform stratified sampling on `attrition` to sample one-quarter of each `RelationshipSatisfaction` group, setting the seed to `2022`.
3. Create a list of unique values from `attrition`'s `RelationshipSatisfaction` column.
Randomly sample `satisfaction_unique` to get two values.
Subset the population for rows where `RelationshipSatisfaction` is in `satisfaction_samp` and clear any unused categories from `RelationshipSatisfaction`; assign to `attrition_clust_prep`.
Perform cluster sampling on the selected satisfaction groups, sampling one quarter of the *population* and setting the seed to `2022`.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Importing the course arrays
attrition = pd.read_feather("datasets/attrition.feather")

# Perform simple random sampling to get 0.25 of the population
attrition_srs = attrition.sample(frac=1/4, random_state=2022)

# Perform stratified sampling to get 0.25 of each relationship group
attrition_strat = attrition.groupby('RelationshipSatisfaction')\
.sample(frac=1/4, random_state=2022)

# Create a list of unique RelationshipSatisfaction values
satisfaction_unique = list(attrition['RelationshipSatisfaction'].unique())

# Randomly sample 2 unique satisfaction values
satisfaction_samp = random.sample(satisfaction_unique, k=2)

# Filter for satisfaction_samp and clear unused categories from RelationshipSatisfaction
satis_condition = attrition['RelationshipSatisfaction'].isin(satisfaction_samp)
attrition_clust_prep = attrition[satis_condition]
attrition_clust_prep['RelationshipSatisfaction'] = attrition_clust_prep['RelationshipSatisfaction'].cat.remove_unused_categories()

# Perform cluster sampling on the selected group, getting 0.25 of attrition_clust_prep
attrition_clust = attrition_clust_prep.groupby("RelationshipSatisfaction")\
.sample(n=len(attrition) // 6, random_state=2022)

print(attrition_clust)
```

### Exercise 2.4.4

#### Comparing point estimates {.unnumbered}

Now that you have three types of sample (simple, stratified, and cluster), you can compare point estimates from each sample to the population parameter. That is, you can calculate the same summary statistic on each sample and see how it compares to the summary statistic for the population.

Here, we'll look at how satisfaction with the company affects whether or not the employee leaves the company. That is, you'll calculate the proportion of employees who left the company (they have an Attrition value of `1`) for each value of `RelationshipSatisfaction`.

#### Instructions {.unnumbered}

1. Group `attrition` by `RelationshipSatisfaction` levels and calculate the mean of Attrition for each level.
2. Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the simple random sample, `attrition_srs`.
3. Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the stratified sample, `attrition_strat`.
4. Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the cluster sample, `attrition_clust`.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Importing the course arrays
attrition = pd.read_feather("datasets/attrition.feather")

# Perform simple random sampling to get 0.25 of the population
attrition_srs = attrition.sample(frac=1/4, random_state=2022)

# Perform stratified sampling to get 0.25 of each relationship group
attrition_strat = attrition.groupby('RelationshipSatisfaction')\
.sample(frac=1/4, random_state=2022)

# Mean Attrition by RelationshipSatisfaction group
mean_attrition_pop = attrition.groupby('RelationshipSatisfaction')\
['Attrition'].mean()

# Print the result
print(mean_attrition_pop)

# Calculate the same thing for the simple random sample 
mean_attrition_srs = attrition_srs.groupby('RelationshipSatisfaction')\
['Attrition'].mean()


# Print the result
print(mean_attrition_srs)

# Calculate the same thing for the stratified sample 
mean_attrition_strat = attrition_strat.groupby('RelationshipSatisfaction')\
['Attrition'].mean()


# Print the result
print(mean_attrition_strat)

# Calculate the same thing for the cluster sample 
mean_attrition_clust = attrition_clust.groupby('RelationshipSatisfaction')\
['Attrition'].mean()

# Print the result
print(mean_attrition_clust)
```

## CHAPTER 3: Sampling Distributions

Let’s test your sampling. In this chapter, you’ll discover how to quantify the accuracy of sample statistics using relative errors, and measure variation in your estimates by generating sampling distributions.

### Chapter 3.1: Relative error of point estimates

Let's see how the size of the sample affects the accuracy of the point estimates we calculate.

#### Sample size is number of rows {.unnumbered}

The sample size, calculated here with the `len` function, is the number of observations, that is, the number of rows in the sample. That's true whichever method we use to create the sample. We'll stick to looking at simple random sampling since it works well in most cases and it's easier to reason about.

#### Various sample sizes {.unnumbered}

Let's calculate a population parameter, the mean cup points of the coffees. It's around eighty-two-point-one-five. This is our gold standard to compare against. If we take a sample size of ten, the point estimate of this parameter is wrong by about point-eight-eight. Increasing the sample size to one hundred gets us closer; the estimate is only wrong by about point-three-four. Increasing the sample size further to one thousand brings the estimate to about point-zero-three away from the population parameter. In general, larger sample sizes will give us more accurate results.

#### Relative errors {.unnumbered}

For any of these sample sizes, we want to compare the population mean to the sample mean. This is the same code we just saw, but with the numerical sample size replaced with a variable named `sample_size`. The most common metric for assessing the difference between the population and a sample mean is the relative error. The relative error is the absolute difference between the two numbers; that is, we ignore any minus signs, divided by the population mean. Here, we also multiply by one hundred to make it a percentage.

#### Relative error vs. sample size {.unnumbered}

Here's a line plot of relative error versus sample size. We see that the relative error decreases as the sample size increases, and beyond that, the plot has other important properties. Firstly, the blue line is really noisy, particularly for small sample sizes. If our sample size is small, the sample mean we calculate can be wildly different by adding one or two more random rows to the sample. Secondly, the amplitude of the line is quite steep, to begin with. When we have a small sample size, adding just a few more samples can give us much better accuracy. Further to the right of the plot, the line is less steep. If we already have a large sample size, adding a few more rows to the sample doesn't bring as much benefit. Finally, at the far right of the plot, where the sample size is the whole population, the relative error decreases to zero.

### Exercise 3.1.1

#### Calculating relative errors {.unnumbered}

The size of the sample you take affects how accurately the point estimates reflect the corresponding population parameter. For example, when you calculate a sample mean, you want it to be close to the population mean. However, if your sample is too small, this might not be the case.

The most common metric for assessing accuracy is *relative error*. This is the absolute difference between the population parameter and the point estimate, all divided by the population parameter. It is sometimes expressed as a percentage.

#### Instructions {.unnumbered}

1. Generate a simple random sample from attrition_pop of fifty rows, setting the seed to 2022.
- Calculate the mean employee `Attrition` in the sample.
- Calculate the relative error between `mean_attrition_srs50` and `mean_attrition_pop` as a *percentage*.
2. Calculate the *relative error percentage* again. This time, use a simple random sample of one hundred rows of `attrition`.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Importing the course arrays
attrition = pd.read_feather("datasets/attrition.feather")

# Population Attrtion mean 
mean_attrition_pop = attrition['Attrition'].mean()

# Print the result
print(mean_attrition_pop)

# Generate a simple random sample of 50 rows, with seed 2022
attrition_srs50 = attrition.sample(n=50, random_state = 2022)

# Calculate the mean employee attrition in the sample
mean_attrition_srs50 = attrition_srs50['Attrition'].mean()

# Calculate the relative error percentage
rel_error_pct50 = 100 * abs(mean_attrition_pop - mean_attrition_srs50)/mean_attrition_pop

# Print rel_error_pct50
print(rel_error_pct50)

# Generate a simple random sample of 100 rows, with seed 2022
attrition_srs100 = attrition.sample(n=100, random_state = 2022)

# Calculate the mean employee attrition in the sample
mean_attrition_srs100 = attrition_srs100['Attrition'].mean()

# Calculate the relative error percentage
rel_error_pct100 = 100 * abs(mean_attrition_pop - mean_attrition_srs100)/mean_attrition_pop

# Print rel_error_pct100
print(rel_error_pct100)
```

### Chapter 3.2: Creating a sampling distribution

We just saw how point estimates like the sample mean will vary depending on which rows end up in the sample.

#### Same code, different answer {.unnumbered}

For example, this same code to calculate the mean cup points from a simple random sample of thirty coffees gives a slightly different answer each time. Let's try to visualize and quantify this variation.

#### Same code, 1000 times {.unnumbered}

A for loop lets us run the same code many times. It's especially useful for situations like this where the result contains some randomness. We start by creating an empty list to store the means. Then, we set up the for loop to repeatedly sample 30 coffees from coffee_ratings a total of 1000 times, calculating the mean cup points each time. After each calculation, we append the result, also called a replicate, to the list. Each time the code is run, we get one sample mean, so running the code a thousand times generates a list of one thousand sample means.

#### Distribution of sample means for size 30 {.unnumbered}

The one thousand sample means form a distribution of sample means. To visualize a distribution, the best plot is often a histogram. Here we can see that most of the results lie between eighty-one and eighty-three, and they roughly follow a bell-shaped curve, like a normal distribution. There's an important piece of jargon we need to know here. A distribution of replicates of sample means, or other point estimates, is known as a sampling distribution.

#### Different sample sizes {.unnumbered}

Here are histograms from running the same code again with different sample sizes. When we decrease the original sample size of thirty to six, we can see from the x-values that the range of the results is broader. The bulk of the results now lie between eighty and eighty-four. On the other hand, increasing the sample size to one hundred and fifty results in a much narrower range. Now most of the results are between eighty-one-point-eight and eighty-two-point-six. As we saw previously, bigger sample sizes give us more accurate results. By replicating the sampling many times, as we've done here, we can quantify that accuracy.

### Exercise 3.2.1

#### Replicating samples {.unnumbered}

When you calculate a point estimate such as a sample mean, the value you calculate depends on the rows that were included in the sample. That means that there is some randomness in the answer. In order to quantify the variation caused by this randomness, you can create many samples and calculate the sample mean (or another statistic) for each sample.

#### Instructions {.unnumbered}

1. Replicate the provided code so that it runs 500 times. Assign the resulting list of sample means to `mean_attritions`.
2. Draw a histogram of the `mean_attritions` list with 16 bins.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Importing the course arrays
attrition = pd.read_feather("datasets/attrition.feather")

# Create an empty list
mean_attritions = []
# Loop 500 times to create 500 sample means
for i in range(500):
	mean_attritions.append(
    	attrition.sample(n=60)['Attrition'].mean()
	)
  
# Print out the first few entries of the list
print(mean_attritions[0:5])

# Create a histogram of the 500 sample means
plt.hist(mean_attritions, bins=16)
plt.show()
```

### Chapter 3.3: Approximate sampling distributions

In the last exercise, we saw that while increasing the number of replicates didn't affect the relative error of the sample means; it did result in a more consistent shape to the distribution.

#### 4 dice {.unnumbered}

Let's consider the case of four six-sided dice rolls. We can generate all possible combinations of rolls using the `expand_grid` function, which is defined in the pandas documentation, and uses the `itertools` package. There are six to the power four, or one-thousand-two-hundred-ninety-six possible dice roll combinations.

#### Mean roll {.unnumbered}

Let's consider the mean of the four rolls by adding a column to our DataFrame called `mean_roll`. `mean_roll` ranges from 1, when four ones are rolled, to 6, when four sixes are rolled.

#### Exact sampling distribution {.unnumbered}

Since the mean roll takes discrete values instead of continuous values, the best way to see the distribution of mean_roll is to draw a bar plot. First, we convert mean_roll to a categorical by setting its type to category. We are interested in the counts of each value, so we use dot-`value_counts`, passing the sort equals False argument. This ensures the x-axis ranges from one to six instead of sorting the bars by frequency. Chaining `.plot` to `value_counts`, and setting kind to `"bar"`, produces a bar plot of the mean roll distribution. This is the exact sampling distribution of the mean roll because it contains every single combination of die rolls.

#### The number of outcomes increases fast {.unnumbered}

If we increase the number of dice in our scenario, the number of possible outcomes increases by a factor of six each time. These values can be shown by creating a DataFrame with two columns: `n_dice`, ranging from 1 to 100, and `n_outcomes`, which is the number of possible outcomes, calculated using six to the power of the number of dice. With just one hundred dice, the number of outcomes is about the same as the number of atoms in the universe: six-point-five times ten to the seventy-seventh power. Long before you start dealing with big datasets, it becomes computationally impossible to calculate the exact sampling distribution. That means we need to rely on approximations.

#### Simulating the mean of four dice rolls {.unnumbered}

We can generate a sample mean of four dice rolls using NumPy's `random.choice` method, specifying size as four. This will randomly choose values from a specified list, in this case, four values from the numbers one to six, which is created using a range from one to seven wrapped in the list function. Notice that we set `replace` equals `True` because we can roll the same number several times.

#### Simulating the mean of four dice rolls {.unnumbered}

Then we use a for loop to generate lots of sample means, in this case, one thousand. We again use the `.append` method to populate the sample means list with our simulated sample means. The output contains a sampling of many of the same values we saw with the exact sampling distribution.

#### Approximate sampling distribution {.unnumbered}

Here's a histogram of the approximate sampling distribution of mean rolls. This time, it uses the simulated rather than the exact values. It's known as an approximate sampling distribution. Notice that although it isn't perfect, it's pretty close to the exact sampling distribution. Usually, we don't have access to the whole population, so we can't calculate the exact sampling distribution. However, we can feel relatively confident that using an approximation will provide a good guess as to how the sampling distribution will behave.

### Exercise 3.3.1

#### Exact sampling distribution {.unnumbered}
To quantify how the point estimate (sample statistic) you are interested in varies, you need to know all the possible values it can take and how often. That is, you need to know its distribution.

The distribution of a sample statistic is called the *sampling distribution*. When we can calculate this exactly, rather than using an approximation, it is known as the *exact sampling distribution*.

Let's take another look at the sampling distribution of dice rolls. This time, we'll look at five eight-sided dice. (These have the numbers one to eight.)

#### Instructions {.unnumbered}

1. Expand a grid representing 5 8-sided dice. That is, create a DataFrame with five columns from a dictionary, named `die1` to `die5`. The rows should contain all possibilities for throwing five dice, each numbered `1` to `8`.
2. Add a column, `mean_roll`, to dice, that contains the mean of the five rolls as a categorical.
3. Create a bar plot of the `mean_roll` categorical column, so it displays the count of each `mean_roll` in increasing order from `1.0` to `8.0`.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Function to create a grid of all possible combinations
def expand_grid(dictionary):
    from itertools import product
    return pd.DataFrame([row for row in product(*dictionary.values())], columns=dictionary.keys())

# Expand a grid representing 5 8-sided dice
dice = expand_grid(
    {'die1': range(1, 9),
     'die2': range(1, 9),
     'die3': range(1, 9),
     'die4': range(1, 9),
     'die5': range(1, 9)}
)

# Print the result
print(dice)

# Add a column of mean rolls and convert to a categorical
dice['mean_roll'] = (dice['die1']+ dice['die2']+ dice['die3']+ dice['die4']+ dice['die5'])/5
                     
                    
dice['mean_roll'] = dice['mean_roll'].astype('category')

# Print result
print(dice)

# Draw a bar plot of mean_roll
dice['mean_roll'].value_counts(sort=False).plot(kind='bar')
plt.show()
```

### Exercise 3.3.2

#### Generating an approximate sampling distribution {.unnumbered}

Calculating the exact sampling distribution is only possible in very simple situations. With just five eight-sided dice, the number of possible rolls is `8**5`, which is over thirty thousand. When the dataset is more complicated, for example, where a variable has hundreds or thousands of categories, the number of possible outcomes becomes too difficult to compute exactly.

In this situation, you can calculate an *approximate sampling distribution* by simulating the exact sampling distribution. That is, you can repeat a procedure over and over again to simulate both the sampling process and the sample statistic calculation process.

#### Instructions {.unnumbered}

1. Sample one to eight, five times, with replacement. Assign to `five_rolls`.
 - Calculate the mean of `five_rolls`.
2. Replicate the sampling code 1000 times, assigning each result to the list `sample_means_1000`.
3. Plot `sample_means_1000` as a histogram with 20 bins.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Sample one to eight, five times, with replacement
five_rolls = np.random.choice(list(range(1, 9)), size=5, replace=True)

# Print the mean of five_rolls
print(five_rolls.mean())

# Replicate the sampling code 1000 times
sample_means_1000 = []
for i in range(1000):
    sample_means_1000.append(
  		np.random.choice(list(range(1, 9)), size=5, replace=True).mean()
    )
    
# Print the first 10 entries of the result
print(sample_means_1000[0:10])

# Draw a histogram of sample_means_1000 with 20 bins
plt.hist(sample_means_1000, bins=20)
plt.show()
```

### Chapter 3.4: Standard errors and the Central Limit Theorem

The Gaussian distribution (also known as the normal distribution) plays an important role in statistics. Its distinctive bell-shaped curve has been cropping up throughout this course.

#### Sampling distribution of mean cup points {.unnumbered}

Here are approximate sampling distributions of the mean cup points from the coffee dataset. Each histogram shows five thousand replicates, with different sample sizes in each case. Look at the x-axis labels. We already saw how increasing the sample size results in greater accuracy in our estimates of the population parameter, so the width of the distribution shrinks as the sample size increases. When the sample size is five, the x-axis ranges from seventy-six to eighty-six, whereas, for a sample size of three hundred and twenty, the range is from eighty-one-point-six to eighty-two-point-six. Now, look at the shape of each distribution. As the sample size increases, we can see that the shape of the curve gets closer and closer to being a normal distribution. At sample size five, the curve is only a very loose approximation since it isn't very symmetric. By sample size eighty, it is a very reasonable approximation.

#### Consequences of the central limit theorem {.unnumbered}

What we just saw is, in essence, what the central limit theorem tells us. The means of independent samples have normal distributions. Then, as the sample size increases, we see two things. The distribution of these averages gets closer to being normal, and the width of this sampling distribution gets narrower.

#### Population & sampling distribution means {.unnumbered}

Recall the population parameter of the mean cup points. We've seen this calculation before, and its value is eighty-two-point-one-five. We can also calculate summary statistics on our sampling distributions to see how they compare. For each of our four sampling distributions, if we take the mean of our sample means, we can see that we get values that are pretty close to the population parameter that the sampling distributions are trying to estimate.

#### Population & sampling distribution standard deviations {.unnumbered}

Now let's consider the standard deviation of the population cup points. It's about two-point-seven. By comparison, if we take the standard deviation of the sample means from each of the sampling distributions using NumPy, we get much smaller numbers, and they decrease as the sample size increases. Note that when we are calculating a population standard deviation with pandas `.std`, we must specify `ddof` equals zero, as `.std` calculates a sample standard deviation by default. When we are calculating a standard deviation on a sample of the population using NumPy's std function, like in these calculations on the sampling distribution, we must specify a `ddof` of one. So what are these smaller standard deviation values?

#### Population mean over square root sample size {.unnumbered}

One other consequence of the central limit theorem is that if we divide the population standard deviation, in this case around 2.7, by the square root of the sample size, we get an estimate of the standard deviation of the sampling distribution for that sample size. It isn't exact because of the randomness involved in the sampling process, but it's pretty close.

#### Standard error {.unnumbered}

We just saw the impact of the sample size on the standard deviation of the sampling distribution. This standard deviation of the sampling distribution has a special name: the standard error. It is useful in a variety of contexts, from estimating population standard deviation to setting expectations on what level of variability we would expect from the sampling process.


### Exercise 3.4.1

#### Population & sampling distribution means {.unnumbered}

One of the useful features of sampling distributions is that you can quantify them. Specifically, you can calculate summary statistics on them. Here, you'll look at the relationship between the mean of the sampling distribution and the population parameter's mean.

Three sampling distributions are provided. For each, the employee attrition dataset was sampled using simple random sampling, then the mean attrition was calculated. This was done 1000 times to get a sampling distribution of mean attritions. One sampling distribution used a sample size of 5 for each replicate, one used 50, and one used 500.

#### Instructions {.unnumbered}

1. Calculate the mean of `sampling_distribution_5`, `sampling_distribution_50`, and `sampling_distribution_500` (a mean of sample means).

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Importing the course arrays
attrition = pd.read_feather("datasets/attrition.feather")

# Set a seed for reproducibility
random_seed = 2021

# Create three empty lists to hold the sampling distributions
sampling_distribution_5 = []   # Sample size of 5
sampling_distribution_50 = []  # Sample size of 50
sampling_distribution_500 = [] # Sample size of 500

# Perform biased sampling and calculate mean attrition 1000 times for each sample size
for i in range(1000):
    # Sample size = 5 (heavier weights toward high attrition)
    sampling_distribution_5.append(
        attrition.sample(n=5, random_state=random_seed + i)['Attrition'].mean()
    )
    
    # Sample size = 50 (bias reduces as sample size increases)
    sampling_distribution_50.append(
        attrition.sample(n=50, random_state=random_seed + i)['Attrition'].mean()
    )
    
    # Sample size = 500 (approaching unbiased mean)
    sampling_distribution_500.append(
        attrition.sample(n=500, random_state=random_seed + i)['Attrition'].mean()
    )

# Optional: Convert the sampling distributions to DataFrame for analysis
sampling_df = pd.DataFrame({
    'Sample_Size_5': sampling_distribution_5,
    'Sample_Size_50': sampling_distribution_50,
    'Sample_Size_500': sampling_distribution_500
})

# Calculate the mean of the mean attritions for each sampling distribution
mean_of_means_5 = np.mean(sampling_distribution_5)
mean_of_means_50 = np.mean(sampling_distribution_50)
mean_of_means_500 = np.mean(sampling_distribution_500)

# Print the results
print(mean_of_means_5)
print(mean_of_means_50)
print(mean_of_means_500)
```

::: {.callout-note}
*Even for small sample sizes, the mean of the sampling distribution is a good approximation of the population mean.*
:::

### Exercise 3.4.2 

#### Population & sampling distribution variation {.unnumbered}

You just calculated the mean of the sampling distribution and saw how it is an estimate of the corresponding population parameter. Similarly, as a result of the central limit theorem, the standard deviation of the sampling distribution has an interesting relationship with the population parameter's standard deviation and the sample size.

#### Instructions {.unnumbered}

1. Calculate the standard deviation of `sampling_distribution_5`, `sampling_distribution_50`, and `sampling_distribution_500` (a standard deviation of sample means).

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Importing the course arrays
attrition = pd.read_feather("datasets/attrition.feather")

# Set a seed for reproducibility
random_seed = 2021

# Create three empty lists to hold the sampling distributions
sampling_distribution_5 = []   # Sample size of 5
sampling_distribution_50 = []  # Sample size of 50
sampling_distribution_500 = [] # Sample size of 500

# Perform biased sampling and calculate mean attrition 1000 times for each sample size
for i in range(1000):
    # Sample size = 5 (heavier weights toward high attrition)
    sampling_distribution_5.append(
        attrition.sample(n=5, random_state=random_seed + i)['Attrition'].mean()
    )
    
    # Sample size = 50 (bias reduces as sample size increases)
    sampling_distribution_50.append(
        attrition.sample(n=50, random_state=random_seed + i)['Attrition'].mean()
    )
    
    # Sample size = 500 (approaching unbiased mean)
    sampling_distribution_500.append(
        attrition.sample(n=500, random_state=random_seed + i)['Attrition'].mean()
    )

# Optional: Convert the sampling distributions to DataFrame for analysis
sampling_df = pd.DataFrame({
    'Sample_Size_5': sampling_distribution_5,
    'Sample_Size_50': sampling_distribution_50,
    'Sample_Size_500': sampling_distribution_500
})

# Calculate the std. dev. of the mean attritions for each sampling distribution
sd_of_means_5 = np.std(sampling_distribution_5, ddof = 1)
sd_of_means_50 = np.std(sampling_distribution_50, ddof = 1)
sd_of_means_500 = np.std(sampling_distribution_500, ddof = 1)

# Print the results
print(sd_of_means_5)
print(sd_of_means_50)
print(sd_of_means_500)
```

::: {.callout-note}
*The amount of variation in the sampling distribution is related to the amount of variation in the population and the sample size. This is another consequence of the Central Limit Theorem.*
:::

## CHAPTER 4: Bootstrap Distributions

You’ll get to grips with resampling to perform bootstrapping and estimate variation in an unknown population. You’ll learn the difference between sampling distributions and bootstrap distributions using resampling.

### Chapter 4.1: Introduction to bootstrapping

So far, we've mostly focused on the idea of sampling without replacement.

#### With or without {.unnumbered}

Sampling without replacement is like dealing a pack of cards. When we deal the ace of spades to one player, we can't then deal the ace of spades to another player. Sampling with replacement is like rolling dice. If we roll a six, we can still get a six on the next roll. Sampling with replacement is sometimes called resampling. We'll use the terms interchangeably.

#### Simple random sampling without replacement {.unnumbered}

If we take a simple random sample without replacement, each row of the dataset, or each type of coffee, can only appear once in the sample.

#### Simple random sampling with replacement {.unnumbered}

If we sample with replacement, it means that each row of the dataset, or each coffee, can be sampled multiple times.

#### Why sample with replacement? {.unnumbered}

So far, we've been treating the `coffee_ratings` dataset as the population of all coffees. Of course, it doesn't include every coffee in the world, so we could treat the coffee dataset as just being a big sample of coffees. To imagine what the whole population is like, we need to approximate the other coffees that aren't in the dataset. Each of the coffees in the sample dataset will have properties that are representative of the coffees that we don't have. Resampling lets us use the existing coffees to approximate those other theoretical coffees.

#### Coffee data preparation {.unnumbered}

To keep it simple, let's focus on three columns of the coffee dataset. To make it easier to see which rows ended up in the sample, we'll add a row index column called index using the `reset_index` method.

#### Resampling with `.sample()` {.unnumbered}

To sample with replacement, we call sample as usual but set the `replace` argument to `True`. Setting `frac` to `1` produces a sample of the same size as the original dataset.

#### Repeated coffees {.unnumbered}

Counting the values of the index column shows how many times each coffee ended up in the resampled dataset. Some coffees were sampled four or five times.

#### Missing coffees {.unnumbered}

That means that some coffees didn't end up in the resample. By taking the number of distinct index values in the resampled dataset, using `len` on `drop_duplicates`, we see that eight hundred and sixty-eight different coffees were included. By comparing this number with the total number of coffees, we can see that four hundred and seventy coffees weren't included in the resample.

#### Bootstrapping {.unnumbered}

We're going to use resampling for a technique called bootstrapping. In some sense, bootstrapping is the opposite of sampling from a population. With sampling, we treat the dataset as the population and move to a smaller sample. With bootstrapping, we treat the dataset as a sample and use it to build up a theoretical population. A use case of bootstrapping is to try to understand the variability due to sampling. This is important in cases where we aren't able to sample the population multiple times to create a sampling distribution.

#### Bootstrapping process {.unnumbered}

The bootstrapping process has three steps. First, randomly sample with replacement to get a resample the same size as the original dataset. Then, calculate a statistic, such as a mean of one of the columns. Note that the mean isn't always the choice here and bootstrapping allows for complex statistics to be computed, too. Then, replicate this many times to get lots of these bootstrap statistics. Earlier in the course, we did something similar. We took a simple random sample, then calculated a summary statistic, then repeated those two steps to form a sampling distribution. This time, when we've used resampling instead of sampling, we get a bootstrap distribution.

#### Bootstrapping coffee mean flavor {.unnumbered}

The resampling step uses the code we just saw: calling sample with `frac` set to `one` and `replace` set to `True`. Calculating a bootstrap statistic can be done with mean from NumPy. In this case, we're calculating the mean flavor score. To repeat steps one and two one thousand times, we can wrap the code in a for loop and append the statistics to a list.

#### Bootstrap distribution histogram {.unnumbered}

Here's a histogram of the bootstrap distribution of the sample mean. Notice that it is close to following a normal distribution.

### Exercise 4.1.1 

#### Generating a bootstrap distribution {.unnumbered}

The process for generating a bootstrap distribution is similar to the process for generating a sampling distribution; only the first step is different.

To make a sampling distribution, you start with the population and sample without replacement. To make a bootstrap distribution, you start with a sample and sample that with replacement. After that, the steps are the same: calculate the summary statistic that you are interested in on that sample/resample, then replicate the process many times. In each case, you can visualize the distribution with a histogram.

Here, `spotify_sample` is a subset of the `spotify` dataset. To make it easier to see how resampling works, a row index column called `'index'` has been added, and only the artist name, song name, and `danceability` columns have been included.

#### Instructions {.unnumbered}

1. Generate a single bootstrap resample from `spotify_sample`.
2. Calculate the mean of the `danceability` column of `spotify_1_resample` using `numpy`.
3. Replicate the expression provided 1000 times.
4. Create a bootstrap distribution by drawing a histogram of `mean_danceability_1000`.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Importing the course array
spotify = pd.read_feather("datasets/spotify_2000_2020.feather")

# Subset of spotify sample to use
spotify_sample = spotify.sample(n=41656)[['artists', 'name', 'danceability']]
spotify_sample['index'] = spotify_sample.index

# Reorder columns to make 'index' the first column
spotify_sample = spotify_sample[['index', 'artists', 'name', 'danceability']]

# Generate 1 bootstrap resample
spotify_1_resample = spotify_sample.sample(frac=1, replace = True)

# Print the resample
print(spotify_1_resample)

# Calculate of the danceability column of spotify_1_resample
mean_danceability_1 = np.mean(spotify_1_resample['danceability'])

# Print the result
print(mean_danceability_1)

# Replicate this 1000 times
mean_danceability_1000 = []
for i in range(1000):
	mean_danceability_1000.append(
        np.mean(spotify_sample.sample(frac=1, replace=True)['danceability'])
	)
  
# Print the result
print(mean_danceability_1000)

# Draw a histogram of the resample means
plt.hist(mean_danceability_1000)
plt.show()
```

### Chapter 4.2: Comparing sampling and bootstrap distributions

#### Coffee focused subset {.unnumbered}

we took a focused subset of the coffee dataset. Here's a five hundred row sample from it.

#### The bootstrap of mean coffee flavors {.unnumbered}

Here, we generate a bootstrap distribution of the mean coffee flavor scores from that sample. `.sample` generates a resample, `np.mean` calculates the statistic, and the for loop with `.append` repeats these steps to produce a distribution of bootstrap statistics.

#### Mean flavor bootstrap distribution {.unnumbered}

Observing the histogram of the bootstrap distribution, which is close to a normal distribution.

#### Sample, bootstrap distribution, population means {.unnumbered}

Here's the mean flavor score from the original sample. In the bootstrap distribution, each value is an estimate of the mean flavor score. Recall that each of these values corresponds to one potential sample mean from the theoretical population. If we take the mean of those means, we get our best guess of the population mean. The two values are really close. However, there's a problem. The true population mean is actually a little different.

#### Interpreting the means {.unnumbered}

The behavior that you just saw is typical. The bootstrap distribution mean is usually almost identical to the original sample mean. However, that is not often a good thing. If the original sample wasn't closely representative of the population, then the bootstrap distribution mean won't be a good estimate of the population mean. Bootstrapping cannot correct any potential biases due to differences between the sample and the population.

#### Sample `sd` vs. bootstrap distribution `sd` {.unnumbered}

While we do have that limitation in estimating the population mean, one great thing about distributions is that we can also quantify variation. The standard deviation of the sample flavors is around `0.354`. Recall that pandas `.std` calculates a sample standard deviation by default. If we calculate the standard deviation of the bootstrap distribution, specifying a `ddof` of one, then we get a completely different number. So what's going on here?

#### Sample, bootstrap dist'n, pop'n standard deviations {.unnumbered}

Remember that one goal of bootstrapping is to quantify what variability we might expect in our sample statistic as we go from one sample to another. Recall that this quantity is called the standard error as measured by the standard deviation of the sampling distribution of that statistic. The standard deviation of the bootstrap means can be used as a way to estimate this measure of uncertainty. If we multiply that standard error by the square root of the sample size, we get an estimate of the standard deviation in the original population. Our estimate of the standard deviation is around point-three-five-three. The true standard deviation is around point-three-four-one, so our estimate is pretty close. In fact, it is closer than just using the sample standard deviation alone.

#### Interpreting the standard errors {.unnumbered}

To recap, the estimated standard error is the standard deviation of the bootstrap distribution values for our statistic of interest. This estimated standard error times the square root of the sample size gives a really good estimate of the standard deviation of the population. That is, although bootstrapping was poor at estimating the population mean, it is generally great for estimating the population standard deviation.

### Exercise 4.2.1

#### Sampling distribution vs. bootstrap distribution {.unnumbered}

The sampling distribution and bootstrap distribution are closely linked. In situations where you can repeatedly sample from a population (these occasions are rare), it's helpful to generate both the sampling distribution and the bootstrap distribution, one after the other, to see how they are related.

Here, the statistic you are interested in is the mean `popularity` score of the songs.

#### Instructions {.unnumbered}

1. Generate a sampling distribution of 2000 replicates.
  - Sample 500 rows of the population without replacement and calculate the mean `popularity`.
2. Generate a bootstrap distribution of 2000 replicates.
  - Sample 500 rows of the sample with replacement and calculate the mean `popularity`.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Importing the course array
spotify = pd.read_feather("datasets/spotify_2000_2020.feather")

spotify_sample = spotify.sample(n=500)

mean_popularity_2000_samp = []

# Generate a sampling distribution of 2000 replicates
for i in range(2000):
    mean_popularity_2000_samp.append(
    	# Sample 500 rows and calculate the mean popularity 
    	spotify.sample(n=500)['popularity'].mean()
    )

# Print the sampling distribution results
print(mean_popularity_2000_samp)

mean_popularity_2000_boot = []

# Generate a bootstrap distribution of 2000 replicates
for i in range(2000):
    mean_popularity_2000_boot.append(
    	# Resample 500 rows and calculate the mean popularity     
    	np.mean(spotify_sample.sample(frac=1, replace=True)['popularity'])
    )

# Print the bootstrap distribution results
print(mean_popularity_2000_boot)
```

::: {.callout-note}
*The sampling distribution and bootstrap distribution are closely related, and so is the code to generate them.*
:::

### Exercise 4.2.2

#### Compare sampling and bootstrap means {.unnumbered}

To make calculation easier, distributions similar to those calculated from the previous exercise have been included, this time using a sample size of 5000.

spotify_population, spotify_sample, sampling_distribution, and bootstrap_distribution are available; pandas and numpy are loaded with their usual aliases.

#### Instructions {.unnumbered}

1. Calculate the mean `popularity` in 4 ways:

- Population: from `spotify`, take the mean of `popularity`.
- Sample: from `spotify_sample`, take the mean of `popularity`.
- Sampling distribution: from `sampling_distribution`, take its mean.
- Bootstrap distribution: from ``bootstrap_distribution`, take its mean.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Importing the course array
spotify = pd.read_feather("datasets/spotify_2000_2020.feather")

spotify_sample = spotify.sample(n=500)

mean_popularity_2000_samp = []

# Generate a sampling distribution of 2000 replicates
for i in range(2000):
    mean_popularity_2000_samp.append(
    	# Sample 500 rows and calculate the mean popularity 
    	spotify.sample(n=500)['popularity'].mean()
    )

# The sampling distribution results
sampling_distribution = mean_popularity_2000_samp 

mean_popularity_2000_boot = []

# Generate a bootstrap distribution of 2000 replicates
for i in range(2000):
    mean_popularity_2000_boot.append(
    	# Resample 500 rows and calculate the mean popularity     
    	np.mean(spotify_sample.sample(frac=1, replace=True)['popularity'])
    )

# The bootstrap distribution results
bootstrap_distribution = mean_popularity_2000_boot

# Calculate the population mean popularity
pop_mean = spotify['popularity'].mean()

# Calculate the original sample mean popularity
samp_mean = spotify_sample['popularity'].mean()

# Calculate the sampling dist'n estimate of mean popularity
samp_distn_mean = np.mean(sampling_distribution)

# Calculate the bootstrap dist'n estimate of mean popularity
boot_distn_mean = np.mean(bootstrap_distribution)

# Print the means
print([pop_mean, samp_mean, samp_distn_mean, boot_distn_mean])
```

::: {.callout-note}
*The sampling distribution mean can be used to estimate the population mean, but that is not the case with the bootstrap distribution.*
:::

### Exercise 4.2.3

#### Compare sampling and bootstrap standard deviations {.unnumbered}

In the same way that you looked at how the sampling distribution and bootstrap distribution could be used to estimate the population mean, you'll now take a look at how they can be used to estimate variation, or more specifically, the standard deviation, in the population.

Recall that the sample size is 5000.

#### Instructions {.unnumbered}

Calculate the standard deviation of `popularity` in 4 ways.
- Population: from `spotify`, take the standard deviation of `popularity`.
- Original sample: from `spotify_sample`, take the standard deviation of `popularity`.
- Sampling distribution: from `sampling_distribution`, take its standard deviation and multiply by the square root of the sample size (5000).
- Bootstrap distribution: from `bootstrap_distribution`, take its standard deviation and multiply by the square root of the sample size.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

# Importing the course array
spotify = pd.read_feather("datasets/spotify_2000_2020.feather")

spotify_sample = spotify.sample(n=5000, random_state=2022)

mean_popularity_2000_samp = []

# Generate a sampling distribution of 2000 replicates
for i in range(2000):
    mean_popularity_2000_samp.append(
    	# Sample 500 rows and calculate the mean popularity 
    	spotify.sample(n=5000)['popularity'].mean()
    )

# The sampling distribution results
sampling_distribution = mean_popularity_2000_samp 

mean_popularity_2000_boot = []

# Generate a bootstrap distribution of 2000 replicates
for i in range(2000):
    mean_popularity_2000_boot.append(
    	# Resample 500 rows and calculate the mean popularity     
    	np.mean(spotify_sample.sample(frac=1, replace=True)['popularity'])
    )

# The bootstrap distribution results
bootstrap_distribution = mean_popularity_2000_boot

# Calculate the population std dev popularity
pop_sd = spotify['popularity'].std(ddof=0)

# Calculate the original sample std dev popularity
samp_sd = spotify_sample['popularity'].std(ddof=1)

# Calculate the sampling dist'n estimate of std dev popularity
samp_distn_sd = np.std(sampling_distribution, ddof=1) * np.sqrt(5000)

# Calculate the bootstrap dist'n estimate of std dev popularity
boot_distn_sd = np.std(bootstrap_distribution, ddof=1) * np.sqrt(5000)

# Print the standard deviations
print([pop_sd, samp_sd, samp_distn_sd, boot_distn_sd])
```

### Chapter 4.3: Confidence intervals

In the last few exercises, you looked at relationships between the sampling distribution and the bootstrap distribution.

One way to quantify these distributions is the idea of "values within one standard deviation of the mean", which gives a good sense of where most of the values in a distribution lie. In this final lesson, we'll formalize the idea of values close to a statistic by defining the term "confidence interval".

#### Predicting the weather {.unnumbered}

Consider meteorologists predicting weather in one of the world's most unpredictable regions - the northern Great Plains of the US and Canada. Rapid City, South Dakota was ranked as the least predictable of the 120 US cities with a National Weather Service forecast office. Suppose we've taken a job as a meteorologist at a news station in Rapid City. Our job is to predict tomorrow's high temperature.

#### Our weather prediction {.unnumbered}

We analyze the weather data using the best forecasting tools available to us and predict a high temperature of 47 degrees Fahrenheit. In this case, 47 degrees is our point estimate. Since the weather is variable, and many South Dakotans will plan their day tomorrow based on our forecast, we'd instead like to present a range of plausible values for the high temperature. On our weather show, we report that the high temperature will be between forty and fifty-four degrees tomorrow.

#### We just reported a confidence interval! {.unnumbered}

This prediction of forty to fifty-four degrees can be thought of as a confidence interval for the unknown quantity of tomorrow's high temperature. Although we can't be sure of the exact temperature, we are confident that it will be in that range. These results are often written as the point estimate followed by the confidence interval's lower and upper bounds in parentheses or square brackets. When the confidence interval is symmetric around the point estimate, we can represent it as the point estimate plus or minus the margin of error, in this case, seven degrees.

#### Bootstrap distribution of mean flavor {.unnumbered}

Here's the bootstrap distribution of the mean flavor from the coffee dataset.

#### Mean of the resamples {.unnumbered}

We can calculate the mean of these resampled mean flavors.

#### Mean plus or minus one standard deviation {.unnumbered}

If we create a confidence interval by adding and subtracting one standard deviation from the mean, we see that there are lots of values in the bootstrap distribution outside of this one standard deviation confidence interval.

#### Quantile method for confidence intervals {.unnumbered}

If we want to include ninety-five percent of the values in the confidence interval, we can use quantiles. Recall that quantiles split distributions into sections containing a particular proportion of the total data. To get the middle ninety-five percent of values, we go from the point-zero-two-five quantile to the point-nine-seven-five quantile since the difference between those two numbers is point-nine-five. To calculate the lower and upper bounds for this confidence interval, we call quantile from NumPy, passing the distribution values and the quantile values to use. The confidence interval is from around seven-point-four-eight to seven-point-five-four.

#### Inverse cumulative distribution function {.unnumbered}

There is a second method to calculate confidence intervals. To understand it, we need to be familiar with the normal distribution's inverse cumulative distribution function. The bell curve we've seen before is the probability density function or PDF. Using calculus, if we integrate this, we get the cumulative distribution function or CDF. If we flip the x and y axes, we get the inverse `CDF`. We can use `scipy.stats` and call `norm.ppf` to get the inverse `CDF`. It takes a quantile between zero and one and returns the values of the normal distribution for that quantile. The parameters of `loc` and `scale` are set to 0 and 1 by default, corresponding to the standard normal distribution. Notice that the values corresponding to point-zero-two-five and point-nine-seven-five are about minus and plus two for the standard normal distribution.

#### Standard error method for confidence interval {.unnumbered}

This second method for calculating a confidence interval is called the standard error method. First, we calculate the point estimate, which is the mean of the bootstrap distribution, and the standard error, which is estimated by the standard deviation of the bootstrap distribution. Then we call `norm.ppf` to get the inverse `CDF` of the normal distribution with the same mean and standard deviation as the bootstrap distribution. Again, the confidence interval is from seven-point-four-eight to seven-point-five-four, though the numbers differ slightly from last time since our bootstrap distribution isn't perfectly normal.

### Exercise 4.3.1

#### Calculating confidence intervals

You have learned about two methods for calculating confidence intervals: *the quantile method* and *the standard error method*. The standard error method involves using the inverse cumulative distribution function (inverse CDF) of the normal distribution to calculate confidence intervals. In this exercise, you'll perform these two methods on the Spotify data.

#### Instructions 

1. Generate a 95% confidence interval using the quantile method on the bootstrap distribution, setting the `0.025` quantile as `lower_quant` and the `0.975` quantile as `upper_quant`.

2. Generate a 95% confidence interval using the standard error method from the bootstrap distribution.
- Calculate `point_estimate` as the mean of `bootstrap_distribution`, and `standard_error` as the standard deviation of `bootstrap_distribution`.
- Calculate `lower_se` as the `0.025` quantile of an inv. CDF from a normal distribution with mean `point_estimate` and standard deviation `standard_error`.
- Calculate `upper_se` as the `0.975` quantile of that same inv. CDF.

```{python}
# Importing libraries
import pandas as pd
import numpy as np
from scipy.stats import norm

# Importing the course array
spotify = pd.read_feather("datasets/spotify_2000_2020.feather")

spotify_sample = spotify.sample(n=5000, random_state=2022)

mean_popularity_2000_boot = []

# Generate a bootstrap distribution of 2000 replicates
for i in range(2000):
    mean_popularity_2000_boot.append(
    	# Resample 500 rows and calculate the mean popularity     
    	np.mean(spotify_sample.sample(frac=1, replace=True)['popularity'])
    )

# The bootstrap distribution results
bootstrap_distribution = mean_popularity_2000_boot

# Generate a 95% confidence interval using the quantile method
lower_quant = np.quantile(bootstrap_distribution, 0.025)
upper_quant = np.quantile(bootstrap_distribution, 0.975)

# Print quantile method confidence interval
print((lower_quant, upper_quant))

# Find the mean and std dev of the bootstrap distribution
point_estimate = np.mean(bootstrap_distribution)
standard_error = np.std(bootstrap_distribution, ddof=1)

# Find the lower limit of the confidence interval
lower_se = norm.ppf(0.025, loc=point_estimate, scale=standard_error)

# Find the upper limit of the confidence interval
upper_se = norm.ppf(0.975, loc=point_estimate, scale=standard_error)

# Print standard error method confidence interval
print((lower_se, upper_se))
```

## Reference

Sampling in Python in Intermediate Python Course for Associate Data Scientist in Python Carrer Track in DataCamp Inc by James Chapman.