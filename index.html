<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lawal’s Note">
<meta name="dcterms.date" content="2024-12-26">

<title>COURSE 19 | SAMPLING AND POINT IN PYTHON</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#chapter-1-introduction-to-sampling" id="toc-chapter-1-introduction-to-sampling" class="nav-link active" data-scroll-target="#chapter-1-introduction-to-sampling"><span class="header-section-number">1</span> Chapter 1: Introduction to Sampling</a>
  <ul class="collapse">
  <li><a href="#chapter-1.1-sampling-and-point-estimates" id="toc-chapter-1.1-sampling-and-point-estimates" class="nav-link" data-scroll-target="#chapter-1.1-sampling-and-point-estimates"><span class="header-section-number">1.1</span> Chapter 1.1: Sampling and point estimates</a></li>
  <li><a href="#exercise-1.1.1" id="toc-exercise-1.1.1" class="nav-link" data-scroll-target="#exercise-1.1.1"><span class="header-section-number">1.2</span> Exercise 1.1.1</a></li>
  <li><a href="#exercise-1.1.2" id="toc-exercise-1.1.2" class="nav-link" data-scroll-target="#exercise-1.1.2"><span class="header-section-number">1.3</span> Exercise 1.1.2</a></li>
  <li><a href="#chapter-1.2-convenience-sampling" id="toc-chapter-1.2-convenience-sampling" class="nav-link" data-scroll-target="#chapter-1.2-convenience-sampling"><span class="header-section-number">1.4</span> Chapter 1.2: Convenience sampling</a></li>
  <li><a href="#exercise-1.2.1" id="toc-exercise-1.2.1" class="nav-link" data-scroll-target="#exercise-1.2.1"><span class="header-section-number">1.5</span> Exercise 1.2.1</a></li>
  <li><a href="#question" id="toc-question" class="nav-link" data-scroll-target="#question"><span class="header-section-number">1.6</span> Question</a></li>
  <li><a href="#exercise-1.2.2" id="toc-exercise-1.2.2" class="nav-link" data-scroll-target="#exercise-1.2.2"><span class="header-section-number">1.7</span> Exercise 1.2.2</a></li>
  <li><a href="#question-1" id="toc-question-1" class="nav-link" data-scroll-target="#question-1"><span class="header-section-number">1.8</span> Question</a></li>
  <li><a href="#chapter-1.3-pseudo-random-number-generation" id="toc-chapter-1.3-pseudo-random-number-generation" class="nav-link" data-scroll-target="#chapter-1.3-pseudo-random-number-generation"><span class="header-section-number">1.9</span> Chapter 1.3: Pseudo-random number generation</a></li>
  <li><a href="#exercise-1.3.1" id="toc-exercise-1.3.1" class="nav-link" data-scroll-target="#exercise-1.3.1"><span class="header-section-number">1.10</span> Exercise 1.3.1</a></li>
  <li><a href="#exercise-1.3.2" id="toc-exercise-1.3.2" class="nav-link" data-scroll-target="#exercise-1.3.2"><span class="header-section-number">1.11</span> Exercise 1.3.2</a></li>
  </ul></li>
  <li><a href="#chapter-2-sampling-methods" id="toc-chapter-2-sampling-methods" class="nav-link" data-scroll-target="#chapter-2-sampling-methods"><span class="header-section-number">2</span> CHAPTER 2: Sampling Methods</a>
  <ul class="collapse">
  <li><a href="#chapter-2.1-simple-random-and-systematic-sampling" id="toc-chapter-2.1-simple-random-and-systematic-sampling" class="nav-link" data-scroll-target="#chapter-2.1-simple-random-and-systematic-sampling"><span class="header-section-number">2.1</span> Chapter 2.1: Simple random and systematic sampling</a></li>
  <li><a href="#exercise-2.1.1" id="toc-exercise-2.1.1" class="nav-link" data-scroll-target="#exercise-2.1.1"><span class="header-section-number">2.2</span> Exercise 2.1.1</a></li>
  <li><a href="#exercise-2.1.2" id="toc-exercise-2.1.2" class="nav-link" data-scroll-target="#exercise-2.1.2"><span class="header-section-number">2.3</span> Exercise 2.1.2</a></li>
  <li><a href="#exercise-2.1.3" id="toc-exercise-2.1.3" class="nav-link" data-scroll-target="#exercise-2.1.3"><span class="header-section-number">2.4</span> Exercise 2.1.3</a></li>
  <li><a href="#chapter-2.2-stratified-and-weighted-random-sampling" id="toc-chapter-2.2-stratified-and-weighted-random-sampling" class="nav-link" data-scroll-target="#chapter-2.2-stratified-and-weighted-random-sampling"><span class="header-section-number">2.5</span> Chapter 2.2: Stratified and weighted random sampling</a></li>
  <li><a href="#exercise-2.2.2" id="toc-exercise-2.2.2" class="nav-link" data-scroll-target="#exercise-2.2.2"><span class="header-section-number">2.6</span> Exercise 2.2.2</a></li>
  <li><a href="#exercise-2.2.3" id="toc-exercise-2.2.3" class="nav-link" data-scroll-target="#exercise-2.2.3"><span class="header-section-number">2.7</span> Exercise 2.2.3</a></li>
  <li><a href="#chapter-2.3-cluster-sampling" id="toc-chapter-2.3-cluster-sampling" class="nav-link" data-scroll-target="#chapter-2.3-cluster-sampling"><span class="header-section-number">2.8</span> Chapter 2.3: Cluster sampling</a></li>
  <li><a href="#exercise-2.3.1" id="toc-exercise-2.3.1" class="nav-link" data-scroll-target="#exercise-2.3.1"><span class="header-section-number">2.9</span> Exercise 2.3.1</a></li>
  <li><a href="#chapter-2.4-comparing-sampling-methods" id="toc-chapter-2.4-comparing-sampling-methods" class="nav-link" data-scroll-target="#chapter-2.4-comparing-sampling-methods"><span class="header-section-number">2.10</span> Chapter 2.4: Comparing sampling methods</a></li>
  <li><a href="#exercise-2.4.1" id="toc-exercise-2.4.1" class="nav-link" data-scroll-target="#exercise-2.4.1"><span class="header-section-number">2.11</span> Exercise 2.4.1</a></li>
  <li><a href="#exercise-2.4.4" id="toc-exercise-2.4.4" class="nav-link" data-scroll-target="#exercise-2.4.4"><span class="header-section-number">2.12</span> Exercise 2.4.4</a></li>
  </ul></li>
  <li><a href="#chapter-3-sampling-distributions" id="toc-chapter-3-sampling-distributions" class="nav-link" data-scroll-target="#chapter-3-sampling-distributions"><span class="header-section-number">3</span> CHAPTER 3: Sampling Distributions</a>
  <ul class="collapse">
  <li><a href="#chapter-3.1-relative-error-of-point-estimates" id="toc-chapter-3.1-relative-error-of-point-estimates" class="nav-link" data-scroll-target="#chapter-3.1-relative-error-of-point-estimates"><span class="header-section-number">3.1</span> Chapter 3.1: Relative error of point estimates</a></li>
  <li><a href="#exercise-3.1.1" id="toc-exercise-3.1.1" class="nav-link" data-scroll-target="#exercise-3.1.1"><span class="header-section-number">3.2</span> Exercise 3.1.1</a></li>
  <li><a href="#chapter-3.2-creating-a-sampling-distribution" id="toc-chapter-3.2-creating-a-sampling-distribution" class="nav-link" data-scroll-target="#chapter-3.2-creating-a-sampling-distribution"><span class="header-section-number">3.3</span> Chapter 3.2: Creating a sampling distribution</a></li>
  <li><a href="#exercise-3.2.1" id="toc-exercise-3.2.1" class="nav-link" data-scroll-target="#exercise-3.2.1"><span class="header-section-number">3.4</span> Exercise 3.2.1</a></li>
  <li><a href="#chapter-3.3-approximate-sampling-distributions" id="toc-chapter-3.3-approximate-sampling-distributions" class="nav-link" data-scroll-target="#chapter-3.3-approximate-sampling-distributions"><span class="header-section-number">3.5</span> Chapter 3.3: Approximate sampling distributions</a></li>
  <li><a href="#exercise-3.3.1" id="toc-exercise-3.3.1" class="nav-link" data-scroll-target="#exercise-3.3.1"><span class="header-section-number">3.6</span> Exercise 3.3.1</a></li>
  <li><a href="#exercise-3.3.2" id="toc-exercise-3.3.2" class="nav-link" data-scroll-target="#exercise-3.3.2"><span class="header-section-number">3.7</span> Exercise 3.3.2</a></li>
  <li><a href="#chapter-3.4-standard-errors-and-the-central-limit-theorem" id="toc-chapter-3.4-standard-errors-and-the-central-limit-theorem" class="nav-link" data-scroll-target="#chapter-3.4-standard-errors-and-the-central-limit-theorem"><span class="header-section-number">3.8</span> Chapter 3.4: Standard errors and the Central Limit Theorem</a></li>
  <li><a href="#exercise-3.4.1" id="toc-exercise-3.4.1" class="nav-link" data-scroll-target="#exercise-3.4.1"><span class="header-section-number">3.9</span> Exercise 3.4.1</a></li>
  <li><a href="#exercise-3.4.2" id="toc-exercise-3.4.2" class="nav-link" data-scroll-target="#exercise-3.4.2"><span class="header-section-number">3.10</span> Exercise 3.4.2</a></li>
  </ul></li>
  <li><a href="#chapter-4-bootstrap-distributions" id="toc-chapter-4-bootstrap-distributions" class="nav-link" data-scroll-target="#chapter-4-bootstrap-distributions"><span class="header-section-number">4</span> CHAPTER 4: Bootstrap Distributions</a>
  <ul class="collapse">
  <li><a href="#chapter-4.1-introduction-to-bootstrapping" id="toc-chapter-4.1-introduction-to-bootstrapping" class="nav-link" data-scroll-target="#chapter-4.1-introduction-to-bootstrapping"><span class="header-section-number">4.1</span> Chapter 4.1: Introduction to bootstrapping</a></li>
  <li><a href="#exercise-4.1.1" id="toc-exercise-4.1.1" class="nav-link" data-scroll-target="#exercise-4.1.1"><span class="header-section-number">4.2</span> Exercise 4.1.1</a></li>
  <li><a href="#chapter-4.2-comparing-sampling-and-bootstrap-distributions" id="toc-chapter-4.2-comparing-sampling-and-bootstrap-distributions" class="nav-link" data-scroll-target="#chapter-4.2-comparing-sampling-and-bootstrap-distributions"><span class="header-section-number">4.3</span> Chapter 4.2: Comparing sampling and bootstrap distributions</a></li>
  <li><a href="#exercise-4.2.1" id="toc-exercise-4.2.1" class="nav-link" data-scroll-target="#exercise-4.2.1"><span class="header-section-number">4.4</span> Exercise 4.2.1</a></li>
  <li><a href="#exercise-4.2.2" id="toc-exercise-4.2.2" class="nav-link" data-scroll-target="#exercise-4.2.2"><span class="header-section-number">4.5</span> Exercise 4.2.2</a></li>
  <li><a href="#exercise-4.2.3" id="toc-exercise-4.2.3" class="nav-link" data-scroll-target="#exercise-4.2.3"><span class="header-section-number">4.6</span> Exercise 4.2.3</a></li>
  <li><a href="#chapter-4.3-confidence-intervals" id="toc-chapter-4.3-confidence-intervals" class="nav-link" data-scroll-target="#chapter-4.3-confidence-intervals"><span class="header-section-number">4.7</span> Chapter 4.3: Confidence intervals</a></li>
  <li><a href="#exercise-4.3.1" id="toc-exercise-4.3.1" class="nav-link" data-scroll-target="#exercise-4.3.1"><span class="header-section-number">4.8</span> Exercise 4.3.1</a></li>
  </ul></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference"><span class="header-section-number">5</span> Reference</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li><li><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">COURSE 19 | SAMPLING AND POINT IN PYTHON</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Lawal’s Note </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Associate Data Science Course in Python by DataCamp Inc
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 26, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><img src="Sampling.jpg" class="img-fluid"></p>
<section id="chapter-1-introduction-to-sampling" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="chapter-1-introduction-to-sampling"><span class="header-section-number">1</span> Chapter 1: Introduction to Sampling</h2>
<p>Learn what sampling is and why it is so powerful. You’ll also learn about the problems caused by convenience sampling and the differences between true randomness and pseudo-randomness.</p>
<section id="chapter-1.1-sampling-and-point-estimates" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="chapter-1.1-sampling-and-point-estimates"><span class="header-section-number">1.1</span> Chapter 1.1: Sampling and point estimates</h3>
<p>Hi! Welcome to the course! I’m James, and I’ll be your host as we delve into the world of sampling data with Python. To start, let’s look at what sampling is and why it might be useful.</p>
<section id="estimating-the-population-of-france" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="estimating-the-population-of-france">Estimating the population of France</h4>
<p>Let’s consider the problem of counting how many people live in France. The standard approach is to take a census. This means contacting every household and asking how many people live there. There are lots of people in France. Since there are millions of people in France, this is a really expensive process. Even with modern data collection technology, most countries will only conduct a census every five or ten years due to the cost.</p>
</section>
<section id="sampling-households" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sampling-households">Sampling households</h4>
<p>In 1786, Pierre-Simon Laplace realized you could estimate the population with less effort. Rather than asking every household who lived there, he asked a small number of households and used statistics to estimate the number of people in the whole population. This technique of working with a subset of the whole population is called sampling.</p>
</section>
<section id="population-vs.-sample" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="population-vs.-sample">Population vs.&nbsp;sample</h4>
<p>Two definitions are important for this course. The population is the complete set of data that we are interested in. The previous example involved the literal population of France, but in statistics, it doesn’t have to refer to people. One thing to bear in mind is that there is usually no equivalent of the census, so typically, we won’t know what the whole population is like - more on this in a moment. The sample is the subset of data that we are working with.</p>
</section>
<section id="coffee-rating-dataset" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="coffee-rating-dataset">Coffee rating dataset</h4>
<p>Picture a dataset of professional ratings of coffees. Each row corresponds to one coffee, and there are thirteen hundred and thirty-eight rows in the dataset. The coffee is given a score from zero to one hundred, which is stored in the total_cup_points column. Other columns contain contextual information like the variety and country of origin and scores between zero and ten for attributes of the coffee such as aroma and body. These scores are averaged across all the reviewers for that particular coffee. It doesn’t contain every coffee in the world, so we don’t know exactly what the population of coffees is. However, there are enough here that we can think of it as our population of interest.</p>
</section>
<section id="points-vs.-flavor-population" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="points-vs.-flavor-population">Points vs.&nbsp;flavor: population</h4>
<p>Let’s consider the relationship between cup points and flavor by selecting those two columns. This dataset contains all thirteen hundred and thirty-eight rows from the original dataset.</p>
</section>
<section id="points-vs.-flavor-10-row-sample" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="points-vs.-flavor-10-row-sample">Points vs.&nbsp;flavor: 10 row sample</h4>
<p>The pandas <code>.sample</code> method returns a random subset of rows. Setting n to ten means ten random rows are returned. By default, rows from the original dataset can’t appear in the sample dataset multiple times, so we are guaranteed to have ten unique rows in our sample.</p>
</section>
<section id="python-sampling-for-series" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="python-sampling-for-series">Python sampling for Series</h4>
<p>The <code>.sample</code> method also works on pandas Series. Here, using square-bracket subsetting retrieves the <code>total_cup_points</code> column as a Series, and the <code>n</code> argument specifies how many random values to return.</p>
</section>
<section id="population-parameters-point-estimates" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="population-parameters-point-estimates">Population parameters &amp; point estimates</h4>
<p>A population parameter is a calculation made on the population dataset. We aren’t limited to counting values either; here, we calculate the mean of the cup points using NumPy. By contrast, a point estimate, or sample statistic, is a calculation based on the sample dataset. Here, the mean of the total cup points is calculated on the sample. Notice that the means are very similar but not identical.</p>
</section>
<section id="point-estimates-with-pandas" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="point-estimates-with-pandas">Point estimates with pandas</h4>
<p>Working with pandas can be easier than working with NumPy. These mean calculations can be performed using the <code>.mean</code> pandas method.</p>
</section>
</section>
<section id="exercise-1.1.1" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="exercise-1.1.1"><span class="header-section-number">1.2</span> Exercise 1.1.1</h3>
<section id="simple-sampling-with-pandas" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="simple-sampling-with-pandas">Simple sampling with pandas</h4>
<p>Throughout this chapter, you’ll be exploring song data from Spotify. Each row of this population dataset represents a song, and there are over 40,000 rows. Columns include the song name, the artists who performed it, the release year, and attributes of the song like its duration, tempo, and danceability. You’ll start by looking at the durations.</p>
<p>Your first task is to sample the Spotify dataset and compare the mean duration of the population with the sample.</p>
</section>
<section id="instructions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions">Instructions</h4>
<ol type="1">
<li>Sample 1000 rows from <code>spotify</code>, assigning to <code>spotify_sample</code>.</li>
<li>Calculate the mean duration in minutes from <code>spotify</code> using pandas.</li>
<li>Calculate the mean duration in minutes from <code>spotify_sample</code> using pandas.</li>
</ol>
<div id="24689dae" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 1000 rows from spotify_population</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify.sample(n<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(spotify_sample)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean duration in mins from spotify_population</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>mean_dur_pop <span class="op">=</span> spotify[<span class="st">'duration_minutes'</span>].mean()</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean duration in mins from spotify_sample</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>mean_dur_samp <span class="op">=</span> spotify_sample[<span class="st">'duration_minutes'</span>].mean()</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the means</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_dur_pop)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_dur_samp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>       acousticness                                            artists  \
6301       0.847000  ['Johann Sebastian Bach', 'Karl Kaiser', 'Koln...   
4932       0.001090                                      ['The Shins']   
10011      0.567000                                ['Imagine Dragons']   
9734       0.012300                    ['Jhené Aiko', 'Vince Staples']   
7704       0.023600                                       ['Skrillex']   
...             ...                                                ...   
9330       0.000066                        ['Five Finger Death Punch']   
23603      0.027300                                     ['Steve Holy']   
6567       0.746000                                ['Sammy Davis Jr.']   
22665      0.676000                                     ['Luke Combs']   
31567      0.000038                                       ['Godsmack']   

       danceability  duration_ms  duration_minutes  energy  explicit  \
6301          0.515      90107.0          1.501783   0.269       0.0   
4932          0.565     221840.0          3.697333   0.881       0.0   
10011         0.462     261080.0          4.351333   0.387       0.0   
9734          0.612     210773.0          3.512883   0.494       0.0   
7704          0.658     237680.0          3.961333   0.930       0.0   
...             ...          ...               ...     ...       ...   
9330          0.378     174160.0          2.902667   0.898       1.0   
23603         0.669     220187.0          3.669783   0.861       0.0   
6567          0.536     343120.0          5.718667   0.182       0.0   
22665         0.552     193200.0          3.220000   0.402       0.0   
31567         0.310     247067.0          4.117783   0.973       1.0   

                           id  instrumentalness   key  liveness  loudness  \
6301   5rvxeH1d6EctGOog8UEvFx          0.089700  11.0    0.0757   -21.859   
4932   3P0fLlpjHwB6zGb7OT9dbJ          0.000022   0.0    0.1060    -4.475   
10011  4kDTvLhGF29gFsqceuxBSC          0.000000   0.0    0.1640    -7.401   
9734   7mnJgJhtuBEMRNoERn1OOa          0.000027  11.0    0.1020   -10.365   
7704   1RJZLVGpBG9nNZiHRQSWTp          0.000000   5.0    0.5780    -2.912   
...                       ...               ...   ...       ...       ...   
9330   0U3aP0QocitGhyqIZOfe4R          0.000082  11.0    0.2800    -3.855   
23603  4mpUaApNea2QhQshM4xyr4          0.000000   2.0    0.0586    -3.547   
6567   1DGaqkDcIYIBZSpW5vyoOL          0.000118   0.0    0.1110   -13.262   
22665  2rxQMGVafnNaRaXlRMWPde          0.000000  11.0    0.0928    -7.431   
31567  2Uilp8alSjAxV0IXorUk9l          0.000001   0.0    0.0973    -4.721   

       mode                                               name  popularity  \
6301    0.0  Orchestral Suite No. 2 in B Minor, BWV 1067: V...        48.0   
4932    1.0                                         Turn On Me        45.0   
10011   1.0                                          Not Today        68.0   
9734    0.0                                         The Vapors        50.0   
7704    0.0                                             Recess        59.0   
...     ...                                                ...         ...   
9330    0.0                                      Crossing Over        42.0   
23603   1.0                               Brand New Girlfriend        59.0   
6567    0.0                     Mr. Bojangles - Single Version        54.0   
22665   1.0                                    Beautiful Crazy        81.0   
31567   1.0                                 I Fucking Hate You        49.0   

      release_date  speechiness    tempo  valence    year  
6301    2000-12-02       0.0370  130.998   0.9640  2000.0  
4932    2007-01-23       0.0356  121.972   0.4270  2007.0  
10011   2016-06-03       0.0292  122.698   0.0428  2016.0  
9734    2013-01-01       0.0606  147.964   0.1470  2013.0  
7704    2014-03-14       0.1410  104.018   0.1320  2014.0  
...            ...          ...      ...      ...     ...  
9330    2009-09-22       0.0639  195.747   0.1280  2009.0  
23603   2006-08-08       0.0929  133.820   0.7200  2006.0  
6567    2002-01-01       0.0458  117.361   0.1320  2002.0  
22665   2018-06-01       0.0262  103.313   0.3820  2018.0  
31567   2003-04-08       0.1850  190.270   0.2260  2003.0  

[1000 rows x 20 columns]
3.8521519140900073
3.9113962</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>Notice that the mean song duration in the sample is similar, but not identical to the mean song duration in the whole population.</em></p>
</div>
</div>
</section>
</section>
<section id="exercise-1.1.2" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="exercise-1.1.2"><span class="header-section-number">1.3</span> Exercise 1.1.2</h3>
<section id="simple-sampling-and-calculating-with-numpy" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="simple-sampling-and-calculating-with-numpy">Simple sampling and calculating with NumPy</h4>
<p>You can also use numpy to calculate parameters or statistics from a list or pandas Series.</p>
<p>You’ll be turning it up to eleven and looking at the loudness property of each song.</p>
</section>
<section id="instructions-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-1">Instructions</h4>
<ol type="1">
<li>Create a pandas Series, <code>loudness_pop</code>, by subsetting the <code>loudness</code> column from <code>spotify</code>.</li>
</ol>
<ul>
<li>Sample <code>loudness_pop</code> to get 100 random values, assigning to <code>loudness_samp</code>.</li>
</ul>
<ol start="2" type="1">
<li>Calculate the mean of <code>loudness_pop</code> using <code>numpy</code>.</li>
<li>Calculate the mean of <code>loudness_samp</code> using <code>numpy</code>.</li>
</ol>
<div id="3ca52795" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a pandas Series from the loudness column of spotify_population</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>loudness_pop <span class="op">=</span> spotify[<span class="st">'loudness'</span>]</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 100 values of loudness_pop</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>loudness_samp <span class="op">=</span> loudness_pop.sample(n<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loudness_samp)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean of loudness_pop</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>mean_loudness_pop <span class="op">=</span> np.mean(loudness_pop)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean of loudness_samp</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>mean_loudness_samp <span class="op">=</span> np.mean(loudness_samp)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_loudness_pop)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_loudness_samp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>17126    -2.535
24473    -4.464
31830    -8.839
7261     -6.206
36359    -5.428
          ...  
22905    -6.266
33343    -7.180
4955     -4.311
13845   -15.764
33324    -5.749
Name: loudness, Length: 100, dtype: float64
-7.366856851353947
-7.9239999999999995</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>Again, notice that the calculated value (the mean) is close but not identical in each case.</em></p>
</div>
</div>
</section>
</section>
<section id="chapter-1.2-convenience-sampling" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="chapter-1.2-convenience-sampling"><span class="header-section-number">1.4</span> Chapter 1.2: Convenience sampling</h3>
<p>The point estimates you calculated in the previous exercises were very close to the population parameters that they were based on, but is this always the case?</p>
<section id="the-literary-digest-election-prediction" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-literary-digest-election-prediction">The Literary Digest election prediction</h4>
<p>In 1936, a newspaper called The Literary Digest ran an extensive poll to try to predict the next US presidential election. They phoned ten million voters and had over two million responses. About one-point-three million people said they would vote for Landon, and just under one million people said they would vote for Roosevelt. That is, Landon was predicted to get fifty-seven percent of the vote, and Roosevelt was predicted to get forty-three percent of the vote. Since the sample size was so large, it was presumed that this poll would be very accurate. However, in the election, Roosevelt won by a landslide with sixty-two percent of the vote. So what went wrong? Well, in 1936, telephones were a luxury, so the only people who had been contacted by The Literary Digest were relatively rich. The sample of voters was not representative of the whole population of voters, and so the poll suffered from sample bias. The data was collected by the easiest method, in this case, telephoning people. This is called convenience sampling and is often prone to sample bias. Before sampling, we need to think about our data collection process to avoid biased results.</p>
</section>
<section id="finding-the-mean-age-of-french-people" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="finding-the-mean-age-of-french-people">Finding the mean age of French people</h4>
<p>Let’s look at another example. While on vacation at Disneyland Paris, you start wondering about the mean age of French people. To get an answer, you ask ten people stood nearby about their ages. Their mean age is twenty-four-point-six years old. Do you think this will be a good estimate of the mean age of all French citizens?</p>
</section>
<section id="how-accurate-was-the-survey" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="how-accurate-was-the-survey">How accurate was the survey?</h4>
<p>On the left, you can see mean ages taken from the French census. Notice that the population has been gradually getting older as birth rates decrease and life expectancy increases. In 2015, the mean age was over forty, so our estimate of twenty-four-point-six is way off. The problem is that the family-friendly fun at Disneyland means that the sample ages weren’t representative of the general population. There are generally more eight-year-olds than eighty-year-olds riding rollercoasters.</p>
</section>
<section id="convenience-sampling-coffee-ratings" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="convenience-sampling-coffee-ratings">Convenience sampling coffee ratings</h4>
<p>Let’s return to the coffee ratings dataset and look at the mean cup points population parameter. The mean is about eighty-two. One form of convenience sampling would be to take the first ten rows, rather than the random rows we saw in the previous video. We can take the first 10 rows with the pandas <code>head</code> method. The mean cup points from this sample is higher at eighty-nine. The discrepancy suggests that coffees with higher cup points appear near the start of the dataset. Again, the convenience sample isn’t representative of the whole population.</p>
</section>
<section id="visualizing-selection-bias" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-selection-bias">Visualizing selection bias</h4>
<p>Histograms are a great way to visualize the selection bias. We can create a histogram of the total cup points from the population, which contains values ranging from around 59 to around 91. The <code>np.arange</code> function can be used to create bins of width 2 from 59 to 91. Recall that the stop value in <code>np.arange</code> is exclusive, so we specify 93, not 91. Here’s the same code to generate a histogram for the convenience sample.</p>
</section>
<section id="distribution-of-a-population-and-of-a-convenience-sample" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="distribution-of-a-population-and-of-a-convenience-sample">Distribution of a population and of a convenience sample</h4>
<p>Comparing the two histograms, it is clear that the distribution of the sample is not the same as the population: all of the sample values are on the right-hand side of the plot.</p>
</section>
<section id="visualizing-selection-bias-for-a-random-sample" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-selection-bias-for-a-random-sample">Visualizing selection bias for a random sample</h4>
<p>This time, we’ll compare the <code>total_cup_points</code> distribution of the population with a random sample of 10 coffees.</p>
</section>
<section id="distribution-of-a-population-and-of-a-simple-random-sample" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="distribution-of-a-population-and-of-a-simple-random-sample">Distribution of a population and of a simple random sample</h4>
<p>Notice how the shape of the distributions is more closely aligned when random sampling is used.</p>
</section>
</section>
<section id="exercise-1.2.1" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="exercise-1.2.1"><span class="header-section-number">1.5</span> Exercise 1.2.1</h3>
<section id="are-findings-from-the-sample-generalizable" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="are-findings-from-the-sample-generalizable">Are findings from the sample generalizable?</h4>
<p>You just saw how convenience sampling—collecting data using the easiest method—can result in samples that aren’t representative of the population. Equivalently, this means findings from the sample are not generalizable to the population. Visualizing the distributions of the population and the sample can help determine whether or not the sample is representative of the population.</p>
<p>The Spotify dataset contains an <code>acousticness</code> column, which is a confidence measure from zero to one of whether the track was made with instruments that aren’t plugged in. You’ll compare the <code>acousticness</code> distribution of the total population of songs with a sample of those songs.</p>
</section>
<section id="instructions-2" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-2">Instructions</h4>
<ol type="1">
<li>Plot a histogram of the <code>acousticness</code> from <code>spotify</code> with bins of width <code>0.01</code> from <code>0</code> to <code>1</code> using pandas <code>.hist()</code>.</li>
<li>Update the histogram code to use the <code>spotify_mysterious_sample</code> dataset.</li>
</ol>
<div id="37b2ea53" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the distribution of acousticness with a histogram</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>spotify[<span class="st">'acousticness'</span>].hist(bins<span class="op">=</span>np.arange(<span class="dv">0</span>,<span class="fl">1.01</span>,<span class="fl">0.01</span>))</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a convenience sample where acousticness is consistently higher</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>spotify_high_acousticness <span class="op">=</span> spotify[(spotify[<span class="st">'acousticness'</span>] <span class="op">&gt;=</span> <span class="fl">0.85</span>) <span class="op">&amp;</span> (spotify[<span class="st">'acousticness'</span>] <span class="op">&lt;=</span> <span class="fl">1.0</span>)]</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 1107 entries from the high acousticness subset</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>spotify_mysterious_sample <span class="op">=</span> spotify_high_acousticness.sample(n<span class="op">=</span><span class="dv">1107</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the histogram to use spotify_mysterious_sample</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>spotify_mysterious_sample[<span class="st">'acousticness'</span>].hist(bins<span class="op">=</span>np.arange(<span class="dv">0</span>, <span class="fl">1.01</span>, <span class="fl">0.01</span>))</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-4-output-1.png" width="583" height="411" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-4-output-2.png" width="575" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="question" class="level3" data-number="1.6">
<h3 data-number="1.6" class="anchored" data-anchor-id="question"><span class="header-section-number">1.6</span> Question</h3>
<p><em>Compare the two histograms you drew</em>. Are the <code>acousticness</code> values in the sample generalizable to the general population?</p>
<p><strong>No.&nbsp;The acousticness samples are consistently higher than those in the general population.</strong></p>
<p><strong>The acousticness values in the sample are all greater than 0.85, whereas they range from 0 to 1 in the whole population.</strong></p>
</section>
<section id="exercise-1.2.2" class="level3" data-number="1.7">
<h3 data-number="1.7" class="anchored" data-anchor-id="exercise-1.2.2"><span class="header-section-number">1.7</span> Exercise 1.2.2</h3>
<section id="are-these-findings-generalizable" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="are-these-findings-generalizable">Are these findings generalizable?</h4>
<p>Let’s look at another sample to see if it is representative of the population. This time, you’ll look at the <code>duration_minutes</code> column of the <code>Spotify</code> dataset, which contains the length of the song in minutes.</p>
</section>
<section id="instructions-3" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-3">Instructions</h4>
<ul>
<li>Plot a histogram of <code>duration_minutes</code> from <code>spotify</code> with bins of width <code>0.5</code> from <code>0</code> to <code>15</code> using pandas <code>.hist()</code>.</li>
<li>Update the histogram code to use the <code>spotify_mysterious_sample2</code> dataset.</li>
</ul>
<div id="9e58bc71" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a convenience sample where duration_minutes is within the specified range</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>spotify_duration_range <span class="op">=</span> spotify[(spotify[<span class="st">'duration_minutes'</span>] <span class="op">&gt;=</span> <span class="fl">0.8079999999</span>) <span class="op">&amp;</span> (spotify[<span class="st">'duration_minutes'</span>] <span class="op">&lt;=</span> <span class="fl">9.822</span>)]</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 50 entries from the spotify_mysterious_sample2 dataset</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>spotify_mysterious_sample2 <span class="op">=</span> spotify_duration_range.sample(n<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the distribution of duration_minutes in the population with a histogram</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>spotify[<span class="st">'duration_minutes'</span>].hist(bins<span class="op">=</span>np.arange(<span class="dv">0</span>,<span class="fl">15.5</span>,<span class="fl">0.5</span>))</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the distribution of duration_minutes as a histogram</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>spotify_mysterious_sample2[<span class="st">'duration_minutes'</span>].hist(bins<span class="op">=</span>np.arange(<span class="dv">0</span>, <span class="fl">15.5</span>, <span class="fl">0.5</span>))</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-5-output-1.png" width="592" height="411" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-5-output-2.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="question-1" class="level3" data-number="1.8">
<h3 data-number="1.8" class="anchored" data-anchor-id="question-1"><span class="header-section-number">1.8</span> Question</h3>
<p><em>Compare the two histograms you drew</em>. Are the duration values in the sample generalizable to the general population?</p>
<section id="answer" class="level4" data-number="1.8.1">
<h4 data-number="1.8.1" class="anchored" data-anchor-id="answer"><span class="header-section-number">1.8.1</span> Answer</h4>
<p>Yes. The sample selected is likely a random sample of all songs in the population.</p>
<p><em>The duration values in the sample show a similar distribution to those in the whole population, so the results are generalizable.</em></p>
</section>
</section>
<section id="chapter-1.3-pseudo-random-number-generation" class="level3" data-number="1.9">
<h3 data-number="1.9" class="anchored" data-anchor-id="chapter-1.3-pseudo-random-number-generation"><span class="header-section-number">1.9</span> Chapter 1.3: Pseudo-random number generation</h3>
<p>You previously saw how to use a random sample to get results similar to those in the population. But how does a computer actually do this random sampling?</p>
<section id="what-does-random-mean" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="what-does-random-mean">What does random mean?</h4>
<p>There are several meanings of random in English. This definition from Oxford Languages is the most interesting for us. If we want to choose data points at random from a population, we shouldn’t be able to predict which data points would be selected ahead of time in some systematic way.</p>
</section>
<section id="true-random-numbers" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="true-random-numbers">True random numbers</h4>
<p>To generate truly random numbers, we typically have to use a physical process like flipping coins or rolling dice. The Hotbits service generates numbers from radioactive decay, and RANDOM.ORG generates numbers from atmospheric noise, which are radio signals generated by lightning. Unfortunately, these processes are fairly slow and expensive for generating random numbers.</p>
<p><a href="https://www.fourmilab.ch/hotbits">https://www.fourmilab.ch/hotbits</a></p>
<p><a href="https://www.random.org">https://www.random.org</a></p>
</section>
<section id="pseudo-random-number-generation" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="pseudo-random-number-generation">Pseudo-random number generation</h4>
<p>For most use cases, pseudo-random number generation is better since it is cheap and fast. Pseudo-random means that although each value appears to be random, it is actually calculated from the previous random number. Since you have to start the calculations somewhere, the first random number is calculated from what is known as a seed value. The word random is in quotes to emphasize that this process isn’t really random. If we start from a particular seed value, all future numbers will be the same.</p>
</section>
<section id="pseudo-random-number-generation-example" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="pseudo-random-number-generation-example">Pseudo-random number generation example</h4>
<p>For example, suppose we have a function to generate pseudo-random values called <code>calc_next_random</code>. To begin, we pick a seed number, in this case, one. <code>calc_next_random</code> does some calculations and returns three. We then feed three into <code>calc_next_random</code>, and it does the same set of calculations and returns two. And if we can keep feeding in the last number, it will return something apparently random. Although the process is deterministic, the trick to a random number generator is to make it look like the values are random.</p>
</section>
<section id="random-number-generating-functions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="random-number-generating-functions">Random number generating functions</h4>
<p>NumPy has many functions for generating random numbers from statistical distributions. To use each of these, make sure to prepend each function name with <code>numpy.random</code> or <code>np.random</code>. Some of them, like <code>.uniform</code> and <code>.normal</code>, may be familiar. Others have more niche applications.</p>
</section>
<section id="visualizing-random-numbers" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="visualizing-random-numbers">Visualizing random numbers</h4>
<p>Let’s generate some pseudo-random numbers. The first arguments to each random number function specify distribution parameters. The size argument specifies how many numbers to generate, in this case, five thousand. We’ve chosen the beta distribution, and its parameters are named a and b. These random numbers come from a continuous distribution, so a great way to visualize them is with a histogram. Here, because the numbers were generated from the beta distribution, all the values are between zero and one.</p>
</section>
<section id="random-numbers-seeds" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="random-numbers-seeds">Random numbers seeds</h4>
<p>To set a random seed with NumPy, we use the <code>.random.seed</code> method. <code>Random.seed</code> takes an integer for the seed number, which can be any number you like. <code>.normal</code> generates pseudo-random numbers from the normal distribution. The loc and scale arguments set the mean and standard deviation of the distribution, and the size argument determines how many random numbers from that distribution will be returned. If we call <code>.normal</code> a second time, we get two different random numbers. If we reset the seed by calling <code>random.seed</code> with the same seed number, then call <code>.normal</code> again, we get the same numbers as before. This makes our code reproducible.</p>
</section>
<section id="using-a-different-seed" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="using-a-different-seed">Using a different seed</h4>
<p>Now let’s try a different seed. This time, calling <code>.normal</code> generates different numbers.</p>
</section>
</section>
<section id="exercise-1.3.1" class="level3" data-number="1.10">
<h3 data-number="1.10" class="anchored" data-anchor-id="exercise-1.3.1"><span class="header-section-number">1.10</span> Exercise 1.3.1</h3>
<section id="generating-random-numbers" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="generating-random-numbers">Generating random numbers</h4>
<p>You’ve used <code>.sample()</code> to generate pseudo-random numbers from a set of values in a DataFrame. A related task is to generate random numbers that follow a statistical distribution, like the uniform distribution or the normal distribution.</p>
<p>Each random number generation function has distribution-specific arguments and an argument for specifying the number of random numbers to generate.</p>
</section>
<section id="instructions-4" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-4">Instructions</h4>
<ol type="1">
<li>Generate 5000 numbers from a uniform distribution, setting the parameters <code>low</code> to <code>-3</code> and <code>high</code> to <code>3</code>.</li>
<li>Generate 5000 numbers from a normal distribution, setting the parameters <code>loc</code> to <code>5</code> and <code>scale</code> to <code>2</code>.</li>
<li>Plot a histogram of <code>uniforms</code> with <code>bins</code> of width of <code>0.25</code> from <code>-3</code> to <code>3</code> using <code>plt.hist()</code>.</li>
<li>Plot a histogram of <code>normals</code> with <code>bins</code> of width of <code>0.5</code> from <code>-2</code> to <code>13</code> using <code>plt.hist()</code>.</li>
</ol>
<div id="3cdd5813" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random numbers from a Uniform(-3, 3)</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>uniforms <span class="op">=</span> np.random.uniform(low<span class="op">=-</span><span class="dv">3</span>, high<span class="op">=</span><span class="dv">3</span>, size<span class="op">=</span><span class="dv">5000</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Print uniforms</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(uniforms)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random numbers from a Normal(5, 2)</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>normals <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">5</span>, scale <span class="op">=</span> <span class="dv">2</span>, size<span class="op">=</span> <span class="dv">5000</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Print normals</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(normals)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot a histogram of uniform values, binwidth 0.25</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>plt.hist(uniforms, bins<span class="op">=</span>np.arange(<span class="op">-</span><span class="dv">3</span>,<span class="fl">3.25</span>,<span class="fl">0.25</span>))</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot a histogram of normal values, binwidth 0.5</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>plt.hist(normals, bins <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">2</span>, <span class="fl">13.5</span>, <span class="fl">0.5</span>))</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[ 1.73504282  0.72457959 -1.34150253 ... -1.0571522  -0.12274863
  0.83445873]
[5.12140432 4.87589927 2.53710178 ... 6.39845768 2.70745544 6.79054579]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-6-output-2.png" width="575" height="411" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-6-output-3.png" width="575" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-1.3.2" class="level3" data-number="1.11">
<h3 data-number="1.11" class="anchored" data-anchor-id="exercise-1.3.2"><span class="header-section-number">1.11</span> Exercise 1.3.2</h3>
<section id="understanding-random-seeds" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="understanding-random-seeds">Understanding random seeds</h4>
<p>While random numbers are important for many analyses, they create a problem: the results you get can vary slightly. This can cause awkward conversations with your boss when your script for calculating the sales forecast gives different answers each time.</p>
<p>Setting the seed for <code>numpy</code>’s random number generator helps avoid such problems by making the random number generation reproducible.</p>
<section id="question-1-1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="question-1-1">Question 1</h5>
<p>Which statement about x and y is true?</p>
<pre><code>import numpy as np
np.random.seed(123)
x = np.random.normal(size=5)
y = np.random.normal(size=5)</code></pre>
<p><em>The values of x are different from those of y</em></p>
</section>
<section id="question-2" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="question-2">Question 2</h5>
<p>Which statement about x and y is true?</p>
<pre><code>import numpy as np
np.random.seed(123)
x = np.random.normal(size=5)
np.random.seed(123)
y = np.random.normal(size=5)</code></pre>
<p><em>x and y have identical values.</em></p>
</section>
<section id="question-3" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="question-3">Question 3</h5>
<p>Which statement about x and y is true?</p>
<pre><code>import numpy as np
np.random.seed(123)
x = np.random.normal(size=5)
np.random.seed(456)
y = np.random.normal(size=5)</code></pre>
<p><em>The values of x are different from those of y.</em></p>
</section>
</section>
</section>
</section>
<section id="chapter-2-sampling-methods" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="chapter-2-sampling-methods"><span class="header-section-number">2</span> CHAPTER 2: Sampling Methods</h2>
<p>It’s time to get hands-on and perform the four random sampling methods in Python: simple, systematic, stratified, and cluster.</p>
<section id="chapter-2.1-simple-random-and-systematic-sampling" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="chapter-2.1-simple-random-and-systematic-sampling"><span class="header-section-number">2.1</span> Chapter 2.1: Simple random and systematic sampling</h3>
<p>There are several methods of sampling from a population. In this video, we’ll look at simple random sampling and systematic random sampling.</p>
<section id="simple-random-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="simple-random-sampling">Simple random sampling</h4>
<p>Simple random sampling works like a raffle or lottery. We start with our population of raffle tickets or lottery balls and randomly pick them out one at a time.</p>
</section>
<section id="simple-random-sampling-of-coffees" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="simple-random-sampling-of-coffees">Simple random sampling of coffees</h4>
<p>In our coffee ratings dataset, instead of raffle tickets or lottery balls, the population consists of coffee varieties. To perform simple random sampling, we take some at random, one at a time. Each coffee has the same chance as any other of being picked. When using this technique, sometimes we might end up with two coffees that were next to each other in the dataset, and sometimes we might end up with large areas of the dataset that were not selected from at all.</p>
</section>
<section id="simple-random-sampling-with-pandas" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="simple-random-sampling-with-pandas">Simple random sampling with pandas</h4>
<p>We’ve already seen how to do simple random sampling with pandas. We call <code>.sample</code> and set <code>n</code> to the size of the sample. We can also set the seed using the random_state argument to generate reproducible results, just like we did pseudo-random number generation. Previously, by not setting <code>random_state</code> when sampling, our code would generate a different random sample each time it was run.</p>
</section>
<section id="systematic-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="systematic-sampling">Systematic sampling</h4>
<p>Another sampling method is known as systematic sampling. This samples the population at regular intervals. Here, looking from top to bottom and left to right within each row, every fifth coffee is sampled.</p>
</section>
<section id="systematic-sampling---defining-the-interval" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="systematic-sampling---defining-the-interval">Systematic sampling - defining the interval</h4>
<p>Systematic sampling with pandas is slightly trickier than simple random sampling. The tricky part is determining how big the interval between each row should be for a given sample size. Suppose we want a sample size of five coffees. The population size is the number of rows in the whole dataset, and in this case, it’s one thousand three hundred and thirty-eight. The interval is the population size divided by the sample size, but because we want the answer to be an integer, we perform integer division with two forward slashes. This is like standard division but discards any fractional part. One-three-three-eight divided by five is actually two hundred and sixty-seven-point-six, and discarding the fractional part leaves two hundred and sixty-seven. Thus, to get a systematic sample of five coffees, we will select every two hundred sixty-seventh coffee in the dataset.</p>
</section>
<section id="systematic-sampling---selecting-the-rows" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="systematic-sampling---selecting-the-rows">Systematic sampling - selecting the rows</h4>
<p>To select every two hundred and sixty-seventh row, we call dot-iloc on coffee_ratings and pass double-colons and the interval, which is 267 in this case. Double-colon interval tells pandas to select every two hundred and sixty-seventh row from zero to the end of the DataFrame.</p>
</section>
<section id="the-trouble-with-systematic-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-trouble-with-systematic-sampling">The trouble with systematic sampling</h4>
<p>There is a problem with systematic sampling, though. Suppose we are interested in statistics about the aftertaste attribute of the coffees. To examine this, first, we use <code>reset_index</code> to create a column of index values in our DataFrame that we can plot. Plotting aftertaste against index shows a pattern. Earlier rows generally have higher aftertaste scores than later rows. This introduces bias into the statistics that we calculate. In general, it is only safe to use systematic sampling if a plot like this has no pattern; that is, it just looks like noise.</p>
</section>
<section id="making-systematic-sampling-safe" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="making-systematic-sampling-safe">Making systematic sampling safe</h4>
<p>To ensure that systematic sampling is safe, we can randomize the row order before sampling. dot-sample has an argument named frac that lets us specify the proportion of the dataset to return in the sample, rather than the absolute number of rows that n specifies. Setting frac to one randomly samples the whole dataset. In effect, this randomly shuffles the rows. Next, the indices need to be reset so that they go in order from zero again. Specifying drop equals True clears the previous row indexes, and chaining to another <code>reset_index</code> call creates a column containing these new indexes. Redrawing the plot with the shuffled dataset shows no pattern between aftertaste and index. This is great, but note that once we’ve shuffled the rows, systematic sampling is essentially the same as simple random sampling.</p>
</section>
</section>
<section id="exercise-2.1.1" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="exercise-2.1.1"><span class="header-section-number">2.2</span> Exercise 2.1.1</h3>
<section id="simple-random-sampling-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="simple-random-sampling-1">Simple random sampling</h4>
<p>The simplest method of sampling a population is the one you’ve seen already. It is known as <em>simple random sampling</em> (sometimes abbreviated to “SRS”), and involves picking rows at random, one at a time, where each row has the same chance of being picked as any other.</p>
<p>In this chapter, you’ll apply sampling methods to a synthetic (fictional) employee attrition dataset from IBM, where “attrition” in this context means leaving the company.</p>
</section>
<section id="instructions-5" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-5">Instructions</h4>
<ul>
<li>Sample 70 rows from <code>attrition</code> using simple random sampling, setting the random seed to 18900217.</li>
<li>Print the sample dataset, <code>attrition_samp</code>. What do you notice about the indices?</li>
</ul>
<div id="3932a015" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 70 rows using simple random sampling and set the seed</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>attrition_samp <span class="op">=</span> attrition.sample(n<span class="op">=</span><span class="dv">70</span>, random_state<span class="op">=</span><span class="dv">18900217</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_samp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>      Age  Attrition     BusinessTravel  DailyRate            Department  \
1134   35        0.0      Travel_Rarely        583  Research_Development   
1150   52        0.0         Non-Travel        585                 Sales   
531    33        0.0      Travel_Rarely        931  Research_Development   
395    31        0.0      Travel_Rarely       1332  Research_Development   
392    29        0.0      Travel_Rarely        942  Research_Development   
...   ...        ...                ...        ...                   ...   
361    27        0.0  Travel_Frequently       1410                 Sales   
1180   36        0.0      Travel_Rarely        530                 Sales   
230    26        0.0      Travel_Rarely       1443                 Sales   
211    29        0.0  Travel_Frequently        410  Research_Development   
890    30        0.0  Travel_Frequently       1312  Research_Development   

      DistanceFromHome      Education    EducationField  \
1134                25         Master           Medical   
1150                29         Master     Life_Sciences   
531                 14       Bachelor           Medical   
395                 11        College           Medical   
392                 15  Below_College     Life_Sciences   
...                ...            ...               ...   
361                  3  Below_College           Medical   
1180                 2         Master     Life_Sciences   
230                 23       Bachelor         Marketing   
211                  2  Below_College     Life_Sciences   
890                  2         Master  Technical_Degree   

     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \
1134                    High  Female  ...          Excellent   
1150                     Low    Male  ...          Excellent   
531                Very_High  Female  ...          Excellent   
395                     High    Male  ...          Excellent   
392                   Medium  Female  ...          Excellent   
...                      ...     ...  ...                ...   
361                Very_High  Female  ...        Outstanding   
1180                    High  Female  ...          Excellent   
230                     High  Female  ...          Excellent   
211                Very_High  Female  ...          Excellent   
890                Very_High  Female  ...          Excellent   

     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \
1134                     High                 1                16   
1150                   Medium                 2                16   
531                 Very_High                 1                 8   
395                 Very_High                 0                 6   
392                       Low                 1                 6   
...                       ...               ...               ...   
361                    Medium                 2                 6   
1180                     High                 0                17   
230                      High                 1                 5   
211                      High                 3                 4   
890                 Very_High                 0                10   

     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \
1134                     3            Good              16   
1150                     3            Good               9   
531                      5          Better               8   
395                      2            Good               6   
392                      2            Good               5   
...                    ...             ...             ...   
361                      3          Better               6   
1180                     2            Good              13   
230                      2            Good               2   
211                      3          Better               3   
890                      2          Better               9   

      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  
1134                  10                       10                    1  
1150                   8                        0                    0  
531                    7                        1                    6  
395                    5                        0                    1  
392                    4                        1                    3  
...                  ...                      ...                  ...  
361                    5                        0                    4  
1180                   7                        6                    7  
230                    2                        0                    0  
211                    2                        0                    2  
890                    7                        0                    7  

[70 rows x 31 columns]</code></pre>
</div>
</div>
</section>
</section>
<section id="exercise-2.1.2" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="exercise-2.1.2"><span class="header-section-number">2.3</span> Exercise 2.1.2</h3>
<section id="systematic-sampling-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="systematic-sampling-1">Systematic sampling</h4>
<p>One sampling method that avoids randomness is called systematic sampling. Here, you pick rows from the population at regular intervals.</p>
<p>For example, if the population dataset had one thousand rows, and you wanted a sample size of five, you could pick rows 0, 200, 400, 600, and 800.</p>
</section>
<section id="instructions-6" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-6">Instructions</h4>
<p>1.Set the sample size to 70. - Calculate the population size from <code>attrition</code>. - Calculate the interval between the rows to be sampled.</p>
<ol start="2" type="1">
<li>Systematically sample <code>attrition</code> to get the rows of the population at each <code>interval</code>, starting at 0; assign the rows to <code>attrition_sys_samp</code></li>
</ol>
<div id="d2c96731" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the sample size to 70</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>sample_size <span class="op">=</span> <span class="dv">70</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the population size from attrition_pop</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>pop_size <span class="op">=</span> <span class="bu">len</span>(attrition)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the interval</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>interval <span class="op">=</span> pop_size<span class="op">//</span>sample_size</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Systematically sample 70 rows</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>attrition_sys_samp <span class="op">=</span> attrition.iloc[::interval]</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_sys_samp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>      Age  Attrition BusinessTravel  DailyRate            Department  \
0      21        0.0  Travel_Rarely        391  Research_Development   
21     19        0.0  Travel_Rarely       1181  Research_Development   
42     45        0.0  Travel_Rarely        252  Research_Development   
63     23        0.0  Travel_Rarely        373  Research_Development   
84     30        1.0  Travel_Rarely        945                 Sales   
...   ...        ...            ...        ...                   ...   
1365   48        0.0  Travel_Rarely        715  Research_Development   
1386   48        0.0  Travel_Rarely       1355  Research_Development   
1407   50        0.0  Travel_Rarely        989  Research_Development   
1428   50        0.0     Non-Travel        881  Research_Development   
1449   52        0.0  Travel_Rarely        699  Research_Development   

      DistanceFromHome      Education EducationField EnvironmentSatisfaction  \
0                   15        College  Life_Sciences                    High   
21                   3  Below_College        Medical                  Medium   
42                   2       Bachelor  Life_Sciences                  Medium   
63                   1        College  Life_Sciences               Very_High   
84                   9       Bachelor        Medical                  Medium   
...                ...            ...            ...                     ...   
1365                 1       Bachelor  Life_Sciences               Very_High   
1386                 4         Master  Life_Sciences                    High   
1407                 7        College        Medical                  Medium   
1428                 2         Master  Life_Sciences                     Low   
1449                 1         Master  Life_Sciences                    High   

      Gender  ...  PerformanceRating RelationshipSatisfaction  \
0       Male  ...          Excellent                Very_High   
21    Female  ...          Excellent                Very_High   
42    Female  ...          Excellent                Very_High   
63      Male  ...        Outstanding                Very_High   
84      Male  ...          Excellent                     High   
...      ...  ...                ...                      ...   
1365    Male  ...          Excellent                     High   
1386    Male  ...          Excellent                   Medium   
1407  Female  ...          Excellent                Very_High   
1428    Male  ...          Excellent                Very_High   
1449    Male  ...          Excellent                      Low   

      StockOptionLevel TotalWorkingYears TrainingTimesLastYear  \
0                    0                 0                     6   
21                   0                 1                     3   
42                   0                 1                     3   
63                   1                 1                     2   
84                   0                 1                     3   
...                ...               ...                   ...   
1365                 0                25                     3   
1386                 0                27                     3   
1407                 1                29                     2   
1428                 1                31                     3   
1449                 1                34                     5   

     WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \
0             Better               0                   0   
21            Better               1                   0   
42            Better               1                   0   
63            Better               1                   0   
84              Good               1                   0   
...              ...             ...                 ...   
1365            Best               1                   0   
1386          Better              15                  11   
1407            Good              27                   3   
1428          Better              31                   6   
1449          Better              33                  18   

      YearsSinceLastPromotion YearsWithCurrManager  
0                           0                    0  
21                          0                    0  
42                          0                    0  
63                          0                    1  
84                          0                    0  
...                       ...                  ...  
1365                        0                    0  
1386                        4                    8  
1407                       13                    8  
1428                       14                    7  
1449                       11                    9  

[70 rows x 31 columns]</code></pre>
</div>
</div>
</section>
</section>
<section id="exercise-2.1.3" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="exercise-2.1.3"><span class="header-section-number">2.4</span> Exercise 2.1.3</h3>
<section id="is-systematic-sampling-ok" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="is-systematic-sampling-ok">Is systematic sampling OK?</h4>
<p>Systematic sampling has a problem: if the data has been sorted, or there is some sort of pattern or meaning behind the row order, then the resulting sample may not be representative of the whole population. The problem can be solved by shuffling the rows, but then systematic sampling is equivalent to simple random sampling.</p>
<p>Here you’ll look at how to determine whether or not there is a problem.</p>
</section>
<section id="instructions-7" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-7">Instructions</h4>
<ol type="1">
<li>Add an index column to <code>attrition</code>, assigning the result to <code>attrition_id</code>.
<ul>
<li>Create a scatter plot of <code>YearsAtCompany</code> versus <code>index</code> for <code>attrition_id</code> using pandas <code>.plot()</code>.</li>
</ul></li>
<li>Randomly shuffle the rows of <code>attrition</code>.
<ul>
<li>Reset the row indexes, and add an <code>index</code> column to <code>attrition</code>.</li>
<li>Repeat the scatter plot of <code>YearsAtCompany</code> versus <code>index</code>, this time using <code>attrition_shuffled</code>.</li>
</ul></li>
</ol>
<div id="0941c707" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Add an index column to attrition_pop</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>attrition_id <span class="op">=</span> attrition.reset_index()</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot YearsAtCompany vs. index for attrition_pop_id</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>attrition_id.plot(x<span class="op">=</span><span class="st">"index"</span>, y<span class="op">=</span><span class="st">"YearsAtCompany"</span>, kind<span class="op">=</span><span class="st">"scatter"</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the rows of attrition_pop</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>attrition_shuffled <span class="op">=</span> attrition.sample(frac<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Reset the row indexes and create an index column</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>attrition_shuffled <span class="op">=</span> attrition_shuffled.reset_index(drop<span class="op">=</span><span class="va">True</span>).reset_index()</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot YearsAtCompany vs. index for attrition_shuffled</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>attrition_shuffled.plot(x<span class="op">=</span><span class="st">"index"</span>, y<span class="op">=</span><span class="st">"YearsAtCompany"</span>, kind<span class="op">=</span><span class="st">"scatter"</span>)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-1.png" width="585" height="429" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-2.png" width="585" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Question
</div>
</div>
<div class="callout-body-container callout-body">
<p>Does a systematic sample always produce a sample similar to a simple random sample?</p>
<p><em>No, Systematic sampling has problems when the data are sorted or contain a pattern. Shuffling the rows makes it equivalent to simple random sampling</em>.</p>
</div>
</div>
</section>
</section>
<section id="chapter-2.2-stratified-and-weighted-random-sampling" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="chapter-2.2-stratified-and-weighted-random-sampling"><span class="header-section-number">2.5</span> Chapter 2.2: Stratified and weighted random sampling</h3>
<p>Stratified sampling is a technique that allows us to sample a population that contains subgroups.</p>
<section id="coffees-by-country" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="coffees-by-country">Coffees by country</h4>
<p>For example, we could group the coffee ratings by country. If we count the number of coffees by country using the <code>value_counts</code> method, we can see that these six countries have the most data.</p>
<ol type="1">
<li>1 The dataset lists Hawaii and Taiwan as countries for convenience, as they are notable coffee-growing regions.</li>
</ol>
</section>
<section id="filtering-for-6-countries" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="filtering-for-6-countries">Filtering for 6 countries</h4>
<p>To make it easier to think about sampling subgroups, let’s limit our analysis to these six countries. We can use the <code>.isin</code> method to filter the population and only return the rows corresponding to these six countries. This filtered dataset is stored as <code>coffee_ratings_top</code>.</p>
</section>
<section id="counts-of-a-simple-random-sample" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="counts-of-a-simple-random-sample">Counts of a simple random sample</h4>
<p>Let’s take a ten percent simple random sample of the dataset using <code>.sample</code> with <code>frac</code> set to 0.1. We also set the <code>random_state</code> argument to ensure reproducibility. As with the whole dataset, we can look at the counts for each country. To make comparisons easier, we set <code>normalize</code> to <code>True</code> to convert the counts into a proportion, which shows what proportion of coffees in the sample came from each country.</p>
</section>
<section id="comparing-proportions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="comparing-proportions">Comparing proportions</h4>
<p>Here are the proportions for the population and the ten percent sample side by side. Just by chance, in this sample, Taiwanese coffees form a disproportionately low percentage. The different makeup of the sample compared to the population could be a problem if we want to analyze the country of origin, for example.</p>
</section>
<section id="proportional-stratified-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="proportional-stratified-sampling">Proportional stratified sampling</h4>
<p>If we care about the proportions of each country in the sample closely matching those in the population, then we can group the data by country before taking the simple random sample. Note that we used the Python line continuation backslash here, which can be useful for breaking up longer chains of pandas code like this. Calling the <code>.sample</code> method after grouping takes a simple random sample within each country. Now the proportions of each country in the stratified sample are much closer to those in the population.</p>
</section>
<section id="equal-counts-stratified-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="equal-counts-stratified-sampling">Equal counts stratified sampling</h4>
<p>One variation of stratified sampling is to sample equal counts from each group, rather than an equal proportion. The code only has one change from before. This time, we use the <code>n</code> argument in <code>.sample</code> instead of <code>frac</code> to extract fifteen randomly-selected rows from each country. Here, the resulting sample has equal proportions of one-sixth from each country.</p>
</section>
<section id="weighted-random-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="weighted-random-sampling">Weighted random sampling</h4>
<p>A close relative of stratified sampling that provides even more flexibility is weighted random sampling. In this variant, we create a column of weights that adjust the relative probability of sampling each row. For example, suppose we thought that it was important to have a higher proportion of Taiwanese coffees in the sample than in the population. We create a condition, in this case, rows where the country of origin is Taiwan. Using the <code>where</code> function from NumPy, we can set a weight of two for rows that match the condition and a weight of one for rows that don’t match the condition. This means when each row is randomly sampled, Taiwanese coffees have two times the chance of being picked compared to other coffees. When we call <code>.sample</code>, we pass the column of weights to the weights argument.</p>
</section>
<section id="weighted-random-sampling-results" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="weighted-random-sampling-results">Weighted random sampling results</h4>
<p>Here, we can see that Taiwan now contains seventeen percent of the sampled dataset, compared to eight-point-five percent in the population. This sort of weighted sampling is common in political polling, where we need to correct for under- or over-representation of demographic groups.</p>
</section>
<section id="exercise-2.2.1" class="level4" data-number="2.5.1">
<h4 data-number="2.5.1" class="anchored" data-anchor-id="exercise-2.2.1"><span class="header-section-number">2.5.1</span> Exercise 2.2.1</h4>
</section>
<section id="proportional-stratified-sampling-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="proportional-stratified-sampling-1">Proportional stratified sampling</h4>
<p>If you are interested in subgroups within the population, then you may need to carefully control the counts of each subgroup within the population. <em>Proportional stratified sampling</em> results in subgroup sizes within the sample that are representative of the subgroup sizes within the population. It is equivalent to performing a simple random sample on each subgroup.</p>
</section>
<section id="instructions-8" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-8">Instructions</h4>
<ol type="1">
<li>Get the proportion of employees by Education level from <code>attrition</code>.</li>
<li>Use proportional stratified sampling on <code>attrition_pop</code> to sample 40% of each <code>Education</code> group, setting the seed to <code>2022</code>.</li>
<li>Get the proportion of employees by <code>Education</code> level from <code>attrition_strat</code>.</li>
</ol>
<div id="c41bdae2" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportion of employees by Education level</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>education_counts_pop <span class="op">=</span> attrition[<span class="st">'Education'</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print education_counts_pop</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(education_counts_pop)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportional stratified sampling for 40% of each Education group</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>attrition_strat <span class="op">=</span> attrition.groupby(<span class="st">'Education'</span>)<span class="op">\</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>.sample(frac<span class="op">=</span><span class="fl">0.4</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_strat)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the Education level proportions from attrition_strat</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>education_counts_strat <span class="op">=</span> attrition_strat[<span class="st">'Education'</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Print education_counts_strat</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(education_counts_strat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Education
Bachelor         0.389116
Master           0.270748
College          0.191837
Below_College    0.115646
Doctor           0.032653
Name: proportion, dtype: float64
      Age  Attrition     BusinessTravel  DailyRate            Department  \
1191   53        0.0      Travel_Rarely        238                 Sales   
407    29        0.0  Travel_Frequently        995  Research_Development   
1233   59        0.0  Travel_Frequently       1225                 Sales   
366    37        0.0      Travel_Rarely        571  Research_Development   
702    31        0.0  Travel_Frequently        163  Research_Development   
...   ...        ...                ...        ...                   ...   
733    38        0.0  Travel_Frequently        653  Research_Development   
1061   44        0.0  Travel_Frequently        602       Human_Resources   
1307   41        0.0      Travel_Rarely       1276                 Sales   
1060   33        0.0      Travel_Rarely        516  Research_Development   
177    29        0.0      Travel_Rarely        738  Research_Development   

      DistanceFromHome      Education    EducationField  \
1191                 1  Below_College           Medical   
407                  2  Below_College     Life_Sciences   
1233                 1  Below_College     Life_Sciences   
366                 10  Below_College     Life_Sciences   
702                 24  Below_College  Technical_Degree   
...                ...            ...               ...   
733                 29         Doctor     Life_Sciences   
1061                 1         Doctor   Human_Resources   
1307                 2         Doctor     Life_Sciences   
1060                 8         Doctor     Life_Sciences   
177                  9         Doctor             Other   

     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \
1191               Very_High  Female  ...        Outstanding   
407                      Low    Male  ...          Excellent   
1233                     Low  Female  ...          Excellent   
366                Very_High  Female  ...          Excellent   
702                Very_High  Female  ...        Outstanding   
...                      ...     ...  ...                ...   
733                Very_High  Female  ...          Excellent   
1061                     Low    Male  ...          Excellent   
1307                  Medium  Female  ...          Excellent   
1060               Very_High    Male  ...          Excellent   
177                   Medium    Male  ...          Excellent   

     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \
1191                Very_High                 0                18   
407                 Very_High                 1                 6   
1233                Very_High                 0                20   
366                    Medium                 2                 6   
702                 Very_High                 0                 9   
...                       ...               ...               ...   
733                 Very_High                 0                10   
1061                     High                 0                14   
1307                   Medium                 1                22   
1060                      Low                 0                14   
177                      High                 0                 4   

     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \
1191                     2            Best              14   
407                      0            Best               6   
1233                     2            Good               4   
366                      3            Good               5   
702                      3            Good               5   
...                    ...             ...             ...   
733                      2          Better              10   
1061                     3          Better              10   
1307                     2          Better              18   
1060                     6          Better               0   
177                      2          Better               3   

      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  
1191                   7                        8                   10  
407                    4                        1                    3  
1233                   3                        1                    3  
366                    3                        4                    3  
702                    4                        1                    4  
...                  ...                      ...                  ...  
733                    3                        9                    9  
1061                   7                        0                    2  
1307                  16                       11                    8  
1060                   0                        0                    0  
177                    2                        2                    2  

[588 rows x 31 columns]
Education
Bachelor         0.389456
Master           0.270408
College          0.192177
Below_College    0.115646
Doctor           0.032313
Name: proportion, dtype: float64</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>By grouping then sampling, the size of each group in the sample is representative of the size of the sample in the population</em>.</p>
</div>
</div>
</section>
</section>
<section id="exercise-2.2.2" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="exercise-2.2.2"><span class="header-section-number">2.6</span> Exercise 2.2.2</h3>
<section id="equal-counts-stratified-sampling-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="equal-counts-stratified-sampling-1">Equal counts stratified sampling</h4>
<p>If one subgroup is larger than another subgroup in the population, but you don’t want to reflect that difference in your analysis, then you can use <em>equal counts stratified sampling</em> to generate samples where each subgroup has the same amount of data. For example, if you are analyzing blood types, O is the most common blood type worldwide, but you may wish to have equal amounts of O, A, B, and AB in your sample.</p>
</section>
<section id="instructions-9" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-9">Instructions</h4>
<ol type="1">
<li>Use equal counts stratified sampling on <code>attrition</code> to get 30 employees from each <code>Education</code> group, setting the seed to <code>2022</code>.</li>
<li>Get the proportion of employees by <code>Education</code> level from <code>attrition_eq</code>.</li>
</ol>
<div id="64e9720a" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportion of employees by Education level</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>education_counts_pop <span class="op">=</span> attrition[<span class="st">'Education'</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print education_counts_pop</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(education_counts_pop)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Get 30 employees from each Education group</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>attrition_eq <span class="op">=</span> attrition.groupby(<span class="st">'Education'</span>)<span class="op">\</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>.sample(n<span class="op">=</span><span class="dv">30</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_eq)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the proportions from attrition_eq</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>education_counts_eq <span class="op">=</span> attrition_eq[<span class="st">'Education'</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(education_counts_eq)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Education
Bachelor         0.389116
Master           0.270748
College          0.191837
Below_College    0.115646
Doctor           0.032653
Name: proportion, dtype: float64
      Age  Attrition     BusinessTravel  DailyRate            Department  \
1191   53        0.0      Travel_Rarely        238                 Sales   
407    29        0.0  Travel_Frequently        995  Research_Development   
1233   59        0.0  Travel_Frequently       1225                 Sales   
366    37        0.0      Travel_Rarely        571  Research_Development   
702    31        0.0  Travel_Frequently        163  Research_Development   
...   ...        ...                ...        ...                   ...   
774    33        0.0      Travel_Rarely        922  Research_Development   
869    45        0.0      Travel_Rarely       1015  Research_Development   
530    32        0.0      Travel_Rarely        120  Research_Development   
1049   48        0.0      Travel_Rarely        163                 Sales   
350    29        1.0      Travel_Rarely        408  Research_Development   

      DistanceFromHome      Education    EducationField  \
1191                 1  Below_College           Medical   
407                  2  Below_College     Life_Sciences   
1233                 1  Below_College     Life_Sciences   
366                 10  Below_College     Life_Sciences   
702                 24  Below_College  Technical_Degree   
...                ...            ...               ...   
774                  1         Doctor           Medical   
869                  5         Doctor           Medical   
530                  6         Doctor     Life_Sciences   
1049                 2         Doctor         Marketing   
350                 25         Doctor  Technical_Degree   

     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \
1191               Very_High  Female  ...        Outstanding   
407                      Low    Male  ...          Excellent   
1233                     Low  Female  ...          Excellent   
366                Very_High  Female  ...          Excellent   
702                Very_High  Female  ...        Outstanding   
...                      ...     ...  ...                ...   
774                      Low  Female  ...          Excellent   
869                     High  Female  ...          Excellent   
530                     High    Male  ...        Outstanding   
1049                  Medium  Female  ...          Excellent   
350                     High  Female  ...          Excellent   

     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \
1191                Very_High                 0                18   
407                 Very_High                 1                 6   
1233                Very_High                 0                20   
366                    Medium                 2                 6   
702                 Very_High                 0                 9   
...                       ...               ...               ...   
774                      High                 1                10   
869                       Low                 0                10   
530                       Low                 0                 8   
1049                      Low                 1                14   
350                    Medium                 0                 6   

     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \
1191                     2            Best              14   
407                      0            Best               6   
1233                     2            Good               4   
366                      3            Good               5   
702                      3            Good               5   
...                    ...             ...             ...   
774                      2          Better               6   
869                      3          Better              10   
530                      2          Better               5   
1049                     2          Better               9   
350                      2            Best               2   

      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  
1191                   7                        8                   10  
407                    4                        1                    3  
1233                   3                        1                    3  
366                    3                        4                    3  
702                    4                        1                    4  
...                  ...                      ...                  ...  
774                    1                        0                    5  
869                    7                        1                    4  
530                    4                        1                    4  
1049                   7                        6                    7  
350                    2                        1                    1  

[150 rows x 31 columns]
Education
Below_College    0.2
College          0.2
Bachelor         0.2
Master           0.2
Doctor           0.2
Name: proportion, dtype: float64</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>If you want each subgroup to have equal weight in your analysis, then equal counts stratified sampling is the appropriate technique.</em></p>
</div>
</div>
</section>
</section>
<section id="exercise-2.2.3" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="exercise-2.2.3"><span class="header-section-number">2.7</span> Exercise 2.2.3</h3>
<section id="weighted-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="weighted-sampling">Weighted sampling</h4>
<p>Stratified sampling provides rules about the probability of picking rows from your dataset at the subgroup level. A generalization of this is weighted sampling, which lets you specify rules about the probability of picking rows at the row level. The probability of picking any given row is proportional to the weight value for that row.</p>
</section>
<section id="instructions-10" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-10">Instructions</h4>
<ol type="1">
<li>Plot <code>YearsAtCompany</code> from <code>attrition</code> as a histogram with bins of width <code>1</code> from <code>0</code> to <code>40</code>.</li>
<li>Sample 400 employees from <code>attrition</code> weighted by <code>YearsAtCompany</code>.</li>
<li>Plot <code>YearsAtCompany</code> from <code>attrition_weight</code> as a histogram with bins of width <code>1</code> from <code>0</code> to <code>40</code>.</li>
<li>Which is higher? The mean <code>YearsAtCompany</code> from <code>attrition</code> or the mean <code>YearsAtCompany</code> from <code>attrition_weight</code>? <strong>Answer</strong>: <em>The weighted sample mean is around 11, which is higher than the population mean of around 7. The fact that the two numbers are different means that the weighted simple random sample is biased.</em></li>
</ol>
<div id="2b4641b4" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot YearsAtCompany from attrition_pop as a histogram</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>attrition[<span class="st">'YearsAtCompany'</span>].hist(bins<span class="op">=</span>np.arange(<span class="dv">0</span>,<span class="dv">41</span>,<span class="dv">1</span>))</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 400 employees weighted by YearsAtCompany</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>attrition_weight <span class="op">=</span> attrition.sample(n<span class="op">=</span><span class="dv">400</span>, weights<span class="op">=</span><span class="st">'YearsAtCompany'</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_weight)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot YearsAtCompany from attrition_weight as a histogram</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>attrition_weight[<span class="st">'YearsAtCompany'</span>].hist(bins<span class="op">=</span>np.arange(<span class="dv">0</span>, <span class="dv">41</span>, <span class="dv">1</span>))</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="co"># The mean YearsAtCompany from attrition dataset </span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition[<span class="st">'YearsAtCompany'</span>].mean())</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="co"># The mean YearsAtCompany from attrition_weight </span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_weight[<span class="st">'YearsAtCompany'</span>].mean())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>      Age  Attrition     BusinessTravel  DailyRate            Department  \
1415   48        0.0      Travel_Rarely       1224  Research_Development   
1295   41        0.0      Travel_Rarely        582  Research_Development   
1176   36        0.0      Travel_Rarely        363  Research_Development   
1381   45        1.0      Travel_Rarely       1449                 Sales   
1359   43        0.0  Travel_Frequently        394                 Sales   
...   ...        ...                ...        ...                   ...   
1234   40        0.0  Travel_Frequently       1395  Research_Development   
894    43        0.0      Travel_Rarely        244       Human_Resources   
1020   31        0.0      Travel_Rarely        480  Research_Development   
180    42        0.0      Travel_Rarely        544       Human_Resources   
1231   37        0.0      Travel_Rarely       1239       Human_Resources   

      DistanceFromHome      Education    EducationField  \
1415                10       Bachelor     Life_Sciences   
1295                28         Master     Life_Sciences   
1176                 1       Bachelor  Technical_Degree   
1381                 2       Bachelor         Marketing   
1359                26        College     Life_Sciences   
...                ...            ...               ...   
1234                26       Bachelor           Medical   
894                  2       Bachelor     Life_Sciences   
1020                 7        College           Medical   
180                  2  Below_College  Technical_Degree   
1231                 8        College             Other   

     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \
1415               Very_High    Male  ...          Excellent   
1295                     Low  Female  ...        Outstanding   
1176                    High  Female  ...        Outstanding   
1381                     Low  Female  ...          Excellent   
1359                    High    Male  ...          Excellent   
...                      ...     ...  ...                ...   
1234                  Medium  Female  ...          Excellent   
894                   Medium    Male  ...          Excellent   
1020                  Medium  Female  ...          Excellent   
180                     High    Male  ...          Excellent   
1231                    High    Male  ...          Excellent   

     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \
1415                Very_High                 0                29   
1295                     High                 1                21   
1176                     High                 1                17   
1381                      Low                 0                26   
1359                Very_High                 2                25   
...                       ...               ...               ...   
1234                      Low                 1                20   
894                    Medium                 0                10   
1020                   Medium                 1                13   
180                      High                 1                 4   
1231                     High                 0                19   

     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \
1415                     3          Better              22   
1295                     3          Better              20   
1176                     2          Better               7   
1381                     2          Better              24   
1359                     3            Best              25   
...                    ...             ...             ...   
1234                     2          Better              20   
894                      5          Better               9   
1020                     5             Bad              13   
180                      5          Better               3   
1231                     4            Good              10   

      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  
1415                  10                       12                    9  
1295                   7                        0                   10  
1176                   7                        7                    7  
1381                  10                        1                   11  
1359                  12                        4                   12  
...                  ...                      ...                  ...  
1234                   7                        2                   13  
894                    7                        1                    8  
1020                  10                        3                   12  
180                    2                        1                    0  
1231                   0                        4                    7  

[400 rows x 31 columns]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-12-output-2.png" width="575" height="411" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>7.0081632653061225
11.22</code></pre>
</div>
</div>
</section>
</section>
<section id="chapter-2.3-cluster-sampling" class="level3" data-number="2.8">
<h3 data-number="2.8" class="anchored" data-anchor-id="chapter-2.3-cluster-sampling"><span class="header-section-number">2.8</span> Chapter 2.3: Cluster sampling</h3>
<p>One problem with stratified sampling is that we need to collect data from every subgroup. In cases where collecting data is expensive, for example, when we have to physically travel to a location to collect it, it can make our analysis prohibitively expensive. There’s a cheaper alternative called cluster sampling.</p>
<section id="stratified-sampling-vs.-cluster-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="stratified-sampling-vs.-cluster-sampling">Stratified sampling vs.&nbsp;cluster sampling</h4>
<p>The stratified sampling approach was to split the population into subgroups, then use simple random sampling on each of them. Cluster sampling means that we limit the number of subgroups in the analysis by picking a few of them with simple random sampling. We then perform simple random sampling on each subgroup as before.</p>
</section>
<section id="varieties-of-coffee" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="varieties-of-coffee">Varieties of coffee</h4>
<p>Let’s return to the coffee dataset and look at the varieties of coffee. In this image, each bean represents the whole subgroup rather than an individual coffee, and there are twenty-eight of them. To extract unique varieties, we use the <code>.unique</code> method. This returns an array, so wrapping it in the list function creates a list of unique varieties. Let’s suppose that it’s expensive to work with all of the different varieties. Enter cluster sampling.</p>
</section>
<section id="stage-1-sampling-for-subgroups" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="stage-1-sampling-for-subgroups">Stage 1: sampling for subgroups</h4>
<p>The first stage of cluster sampling is to randomly cut down the number of varieties, and we do this by randomly selecting them. Here, we’ve used the <code>random.sample</code> function from the random package to get three varieties, specified using the argument k.</p>
</section>
<section id="stage-2-sampling-each-group" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="stage-2-sampling-each-group">Stage 2: sampling each group</h4>
<p>The second stage of cluster sampling is to perform simple random sampling on each of the three varieties we randomly selected. We first filter the dataset for rows where the variety is one of the three selected, using the <code>.isin</code> method. To ensure that the <code>isin</code> filtering removes levels with zero rows, we apply the <code>cat.remove_unused_categories</code> method on the Series of focus, which is variety here. If we exclude this method, we might receive an error when sampling by variety level. The pandas code is the same as for stratified sampling. Here, we’ve opted for equal counts sampling, with five rows from each remaining variety.</p>
</section>
<section id="stage-2-output" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="stage-2-output">Stage 2 output</h4>
<p>Here’s the first few columns of the result. Notice that there are the fifteen rows, which we’d expect from sampling five rows from three varieties.</p>
</section>
<section id="multistage-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="multistage-sampling">Multistage sampling</h4>
<p>Note that we had two stages in the cluster sampling. We randomly sampled the subgroups to include, then we randomly sampled rows from those subgroups. Cluster sampling is a special case of multistage sampling. It’s possible to use more than two stages. A common example is national surveys, which can include several levels of administrative regions, like states, counties, cities, and neighborhoods.</p>
</section>
</section>
<section id="exercise-2.3.1" class="level3" data-number="2.9">
<h3 data-number="2.9" class="anchored" data-anchor-id="exercise-2.3.1"><span class="header-section-number">2.9</span> Exercise 2.3.1</h3>
<section id="performing-cluster-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="performing-cluster-sampling">Performing cluster sampling</h4>
<p>Now that you know when to use cluster sampling, it’s time to put it into action. In this exercise, you’ll explore the JobRole column of the attrition dataset. You can think of each job role as a subgroup of the whole population of employees.</p>
<p>Use a seed of 19790801 to set the seed with <code>random.seed()</code>.</p>
</section>
<section id="instructions-11" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-11">Instructions</h4>
<ol type="1">
<li></li>
</ol>
<ul>
<li>Create a list of unique <code>JobRole</code> values from <code>attrition</code>, and assign to <code>job_roles_pop</code>.</li>
<li>Randomly sample four <code>JobRole</code> values from <code>job_roles_pop</code>.</li>
</ul>
<ol start="2" type="1">
<li>Subset attrition_pop for the sampled job roles by filtering for rows where <code>JobRole</code> is in <code>job_roles_samp</code>.</li>
<li></li>
</ol>
<ul>
<li>Remove any unused categories from <code>JobRole</code>.</li>
<li>For each job role in the filtered dataset, take a random sample of ten rows, setting the seed to <code>2022</code>.</li>
</ul>
<div id="6b836366" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the seed</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">19790801</span>)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of unique JobRole values</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>job_roles_pop <span class="op">=</span> <span class="bu">list</span>(attrition[<span class="st">'JobRole'</span>].unique())</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly sample four JobRole values</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>job_roles_samp <span class="op">=</span> random.sample(job_roles_pop, k<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(job_roles_samp)</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter for rows where JobRole is in job_roles_samp</span></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>jobrole_condition <span class="op">=</span> attrition[<span class="st">'JobRole'</span>].isin(job_roles_samp)</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>attrition_filtered <span class="op">=</span> attrition[jobrole_condition]</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_filtered)</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove categories with no rows</span></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>attrition_filtered[<span class="st">'JobRole'</span>] <span class="op">=</span> attrition_filtered[<span class="st">'JobRole'</span>].cat.remove_unused_categories()</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly sample 10 employees from each sampled job role</span></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>attrition_clust <span class="op">=</span> attrition_filtered.groupby(<span class="st">'JobRole'</span>)<span class="op">\</span></span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>.sample(n<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_clust)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>['Research_Director', 'Research_Scientist', 'Human_Resources', 'Manager']
      Age  Attrition BusinessTravel  DailyRate            Department  \
0      21        0.0  Travel_Rarely        391  Research_Development   
5      27        0.0     Non-Travel        443  Research_Development   
6      18        0.0     Non-Travel        287  Research_Development   
10     18        0.0     Non-Travel       1431  Research_Development   
17     31        0.0  Travel_Rarely       1082  Research_Development   
...   ...        ...            ...        ...                   ...   
1462   54        0.0  Travel_Rarely        584  Research_Development   
1464   55        0.0  Travel_Rarely        452  Research_Development   
1465   55        0.0  Travel_Rarely       1117                 Sales   
1466   58        0.0     Non-Travel        350                 Sales   
1469   58        1.0  Travel_Rarely        286  Research_Development   

      DistanceFromHome Education EducationField EnvironmentSatisfaction  \
0                   15   College  Life_Sciences                    High   
5                    3  Bachelor        Medical               Very_High   
6                    5   College  Life_Sciences                  Medium   
10                  14  Bachelor        Medical                  Medium   
17                   1    Master        Medical                    High   
...                ...       ...            ...                     ...   
1462                22    Doctor        Medical                  Medium   
1464                 1  Bachelor        Medical               Very_High   
1465                18    Doctor  Life_Sciences                     Low   
1466                 2  Bachelor        Medical                  Medium   
1469                 2    Master  Life_Sciences               Very_High   

      Gender  ...  PerformanceRating RelationshipSatisfaction  \
0       Male  ...          Excellent                Very_High   
5       Male  ...          Excellent                     High   
6       Male  ...          Excellent                Very_High   
10    Female  ...          Excellent                     High   
17      Male  ...          Excellent                   Medium   
...      ...  ...                ...                      ...   
1462  Female  ...        Outstanding                     High   
1464    Male  ...          Excellent                     High   
1465  Female  ...        Outstanding                Very_High   
1466    Male  ...        Outstanding                Very_High   
1469    Male  ...          Excellent                Very_High   

      StockOptionLevel TotalWorkingYears TrainingTimesLastYear  \
0                    0                 0                     6   
5                    3                 0                     6   
6                    0                 0                     2   
10                   0                 0                     4   
17                   0                 1                     4   
...                ...               ...                   ...   
1462                 1                36                     6   
1464                 0                37                     2   
1465                 0                37                     2   
1466                 1                37                     0   
1469                 0                40                     2   

     WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \
0             Better               0                   0   
5               Good               0                   0   
6             Better               0                   0   
10               Bad               0                   0   
17            Better               1                   1   
...              ...             ...                 ...   
1462          Better              10                   8   
1464          Better              36                  10   
1465          Better              10                   9   
1466            Good              16                   9   
1469          Better              31                  15   

      YearsSinceLastPromotion YearsWithCurrManager  
0                           0                    0  
5                           0                    0  
6                           0                    0  
10                          0                    0  
17                          1                    0  
...                       ...                  ...  
1462                        4                    7  
1464                        4                   13  
1465                        7                    7  
1466                       14                   14  
1469                       13                    8  

[526 rows x 31 columns]
      Age  Attrition     BusinessTravel  DailyRate            Department  \
1348   44        1.0      Travel_Rarely       1376       Human_Resources   
886    41        0.0         Non-Travel        552       Human_Resources   
983    39        0.0      Travel_Rarely        141       Human_Resources   
88     27        1.0  Travel_Frequently       1337       Human_Resources   
189    34        0.0      Travel_Rarely        829       Human_Resources   
160    24        0.0  Travel_Frequently        897       Human_Resources   
839    46        0.0      Travel_Rarely        991       Human_Resources   
966    30        0.0      Travel_Rarely       1240       Human_Resources   
162    28        0.0         Non-Travel        280       Human_Resources   
1231   37        0.0      Travel_Rarely       1239       Human_Resources   
1375   44        0.0      Travel_Rarely       1315  Research_Development   
1462   54        0.0      Travel_Rarely        584  Research_Development   
1316   45        0.0  Travel_Frequently        364  Research_Development   
1356   48        0.0  Travel_Frequently        117  Research_Development   
1387   48        0.0         Non-Travel       1262  Research_Development   
1321   54        0.0         Non-Travel        142       Human_Resources   
1266   50        0.0      Travel_Rarely       1452  Research_Development   
1330   46        0.0      Travel_Rarely        406                 Sales   
1052   59        0.0      Travel_Rarely       1089                 Sales   
1449   52        0.0      Travel_Rarely        699  Research_Development   
1439   58        0.0      Travel_Rarely       1055  Research_Development   
1339   58        0.0  Travel_Frequently       1216  Research_Development   
1426   49        0.0      Travel_Rarely       1245  Research_Development   
1415   48        0.0      Travel_Rarely       1224  Research_Development   
1322   51        0.0      Travel_Rarely        684  Research_Development   
1284   40        0.0      Travel_Rarely       1308  Research_Development   
1149   37        0.0      Travel_Rarely        161  Research_Development   
1126   42        0.0      Travel_Rarely        810  Research_Development   
1374   46        0.0      Travel_Rarely       1009  Research_Development   
1050   33        0.0      Travel_Rarely        213  Research_Development   
86     26        0.0      Travel_Rarely        482  Research_Development   
930    52        1.0      Travel_Rarely        723  Research_Development   
860    37        0.0      Travel_Rarely        674  Research_Development   
36     20        1.0      Travel_Rarely       1362  Research_Development   
997    32        0.0      Travel_Rarely        824  Research_Development   
1358   45        0.0      Travel_Rarely       1339  Research_Development   
993    41        0.0  Travel_Frequently       1200  Research_Development   
421    34        0.0      Travel_Rarely        181  Research_Development   
789    28        1.0      Travel_Rarely        654  Research_Development   
94     36        1.0      Travel_Rarely        318  Research_Development   

      DistanceFromHome      Education    EducationField  \
1348                 1        College           Medical   
886                  4       Bachelor   Human_Resources   
983                  3       Bachelor   Human_Resources   
88                  22       Bachelor   Human_Resources   
189                  3        College   Human_Resources   
160                 10       Bachelor           Medical   
839                  1        College     Life_Sciences   
966                  9       Bachelor   Human_Resources   
162                  1        College     Life_Sciences   
1231                 8        College             Other   
1375                 3         Master             Other   
1462                22         Doctor           Medical   
1316                25       Bachelor           Medical   
1356                22       Bachelor           Medical   
1387                 1         Master           Medical   
1321                26       Bachelor   Human_Resources   
1266                11       Bachelor     Life_Sciences   
1330                 3  Below_College         Marketing   
1052                 1        College  Technical_Degree   
1449                 1         Master     Life_Sciences   
1439                 1       Bachelor           Medical   
1339                15         Master     Life_Sciences   
1426                18         Master     Life_Sciences   
1415                10       Bachelor     Life_Sciences   
1322                 6       Bachelor     Life_Sciences   
1284                14       Bachelor           Medical   
1149                10       Bachelor     Life_Sciences   
1126                23         Doctor     Life_Sciences   
1374                 2       Bachelor     Life_Sciences   
1050                 7       Bachelor           Medical   
86                   1        College     Life_Sciences   
930                  8         Master           Medical   
860                 13       Bachelor           Medical   
36                  10  Below_College           Medical   
997                  5        College     Life_Sciences   
1358                 7       Bachelor     Life_Sciences   
993                 22       Bachelor     Life_Sciences   
421                  2         Master           Medical   
789                  1        College     Life_Sciences   
94                   9       Bachelor           Medical   

     EnvironmentSatisfaction  Gender  ...  PerformanceRating  \
1348                  Medium    Male  ...          Excellent   
886                     High    Male  ...          Excellent   
983                     High  Female  ...          Excellent   
88                       Low  Female  ...          Excellent   
189                     High    Male  ...          Excellent   
160                      Low    Male  ...          Excellent   
839                Very_High  Female  ...          Excellent   
966                     High    Male  ...          Excellent   
162                     High    Male  ...          Excellent   
1231                    High    Male  ...          Excellent   
1375               Very_High    Male  ...          Excellent   
1462                  Medium  Female  ...        Outstanding   
1316                  Medium  Female  ...        Outstanding   
1356               Very_High  Female  ...          Excellent   
1387                     Low    Male  ...        Outstanding   
1321               Very_High  Female  ...          Excellent   
1266                    High  Female  ...          Excellent   
1330                     Low    Male  ...          Excellent   
1052                  Medium    Male  ...          Excellent   
1449                    High    Male  ...          Excellent   
1439               Very_High  Female  ...        Outstanding   
1339                     Low    Male  ...          Excellent   
1426               Very_High    Male  ...          Excellent   
1415               Very_High    Male  ...          Excellent   
1322                     Low    Male  ...          Excellent   
1284                    High    Male  ...          Excellent   
1149                    High  Female  ...        Outstanding   
1126                     Low  Female  ...          Excellent   
1374                     Low    Male  ...          Excellent   
1050                    High    Male  ...          Excellent   
86                    Medium  Female  ...          Excellent   
930                     High    Male  ...          Excellent   
860                      Low    Male  ...          Excellent   
36                 Very_High    Male  ...          Excellent   
997                Very_High  Female  ...          Excellent   
1358                  Medium    Male  ...          Excellent   
993                Very_High  Female  ...          Excellent   
421                Very_High    Male  ...          Excellent   
789                      Low  Female  ...          Excellent   
94                 Very_High    Male  ...          Excellent   

     RelationshipSatisfaction  StockOptionLevel TotalWorkingYears  \
1348                Very_High                 1                24   
886                    Medium                 1                10   
983                      High                 1                12   
88                        Low                 0                 1   
189                      High                 1                 4   
160                 Very_High                 1                 3   
839                      High                 0                10   
966                 Very_High                 0                12   
162                    Medium                 1                 3   
1231                     High                 0                19   
1375                      Low                 1                26   
1462                     High                 1                36   
1316                     High                 0                22   
1356                   Medium                 1                24   
1387                     High                 0                27   
1321                     High                 0                23   
1266                   Medium                 0                21   
1330                Very_High                 1                23   
1052                     High                 1                14   
1449                      Low                 1                34   
1439                     High                 1                32   
1339                   Medium                 0                23   
1426                     High                 1                31   
1415                Very_High                 0                29   
1322                     High                 0                23   
1284                      Low                 0                21   
1149                      Low                 1                16   
1126                   Medium                 0                16   
1374                     High                 0                26   
1050                Very_High                 0                14   
86                       High                 1                 1   
930                       Low                 0                11   
860                       Low                 0                10   
36                  Very_High                 0                 1   
997                       Low                 1                12   
1358                     High                 1                25   
993                       Low                 2                12   
421                       Low                 3                 6   
789                 Very_High                 0                10   
94                        Low                 1                 2   

     TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \
1348                     1          Better              20   
886                      4          Better               3   
983                      3             Bad               8   
88                       2          Better               1   
189                      1             Bad               3   
160                      2          Better               2   
839                      3            Best               7   
966                      2             Bad              11   
162                      2          Better               3   
1231                     4            Good              10   
1375                     2            Best               2   
1462                     6          Better              10   
1316                     4          Better               0   
1356                     3          Better              22   
1387                     3            Good               5   
1321                     3          Better               5   
1266                     5          Better               5   
1330                     3          Better              12   
1052                     1             Bad               6   
1449                     5          Better              33   
1439                     3          Better               9   
1339                     3          Better               2   
1426                     5          Better              31   
1415                     3          Better              22   
1322                     5          Better              20   
1284                     2            Best              20   
1149                     2          Better              16   
1126                     2          Better               1   
1374                     2             Bad               3   
1050                     3            Best              13   
86                       3            Good               1   
930                      3            Good               8   
860                      2          Better              10   
36                       5          Better               1   
997                      2          Better               7   
1358                     2          Better               1   
993                      4            Good               6   
421                      3          Better               5   
789                      4          Better               7   
94                       0            Good               1   

      YearsInCurrentRole  YearsSinceLastPromotion YearsWithCurrManager  
1348                   6                        3                    6  
886                    2                        1                    2  
983                    3                        3                    6  
88                     0                        0                    0  
189                    2                        0                    2  
160                    2                        2                    1  
839                    6                        5                    7  
966                    9                        4                    7  
162                    2                        2                    2  
1231                   0                        4                    7  
1375                   2                        0                    1  
1462                   8                        4                    7  
1316                   0                        0                    0  
1356                  17                        4                    7  
1387                   4                        2                    1  
1321                   3                        4                    4  
1266                   4                        4                    4  
1330                   9                        4                    9  
1052                   4                        0                    4  
1449                  18                       11                    9  
1439                   8                        1                    5  
1339                   2                        2                    2  
1426                   9                        0                    9  
1415                  10                       12                    9  
1322                  18                       15                   15  
1284                   7                        4                    9  
1149                  11                        6                    8  
1126                   0                        0                    0  
1374                   2                        0                    1  
1050                   9                        3                    7  
86                     0                        1                    0  
930                    2                        7                    7  
860                    8                        3                    7  
36                     0                        1                    1  
997                    1                        2                    5  
1358                   0                        0                    0  
993                    2                        3                    3  
421                    0                        1                    2  
789                    7                        3                    7  
94                     0                        0                    0  

[40 rows x 31 columns]</code></pre>
</div>
</div>
</section>
</section>
<section id="chapter-2.4-comparing-sampling-methods" class="level3" data-number="2.10">
<h3 data-number="2.10" class="anchored" data-anchor-id="chapter-2.4-comparing-sampling-methods"><span class="header-section-number">2.10</span> Chapter 2.4: Comparing sampling methods</h3>
<p>Let’s review the various sampling techniques we learned about.</p>
<section id="review-of-sampling-techniques---setup" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="review-of-sampling-techniques---setup">Review of sampling techniques - setup</h4>
<p>For convenience, we’ll stick to the six countries with the most coffee varieties that we used before. This corresponds to eight hundred and eighty rows and eight columns, retrieved using the <code>.shape</code> attribute.</p>
</section>
<section id="review-of-simple-random-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="review-of-simple-random-sampling">Review of simple random sampling</h4>
<p>Simple random sampling uses <code>.sample</code> with either <code>n</code> or <code>frac</code> set to determine how many rows to pseudo-randomly choose, given a seed value set with <code>random_state</code>. The simple random sample returns two hundred and ninety-three rows because we specified <code>frac</code> as one-third, and one-third of eight hundred and eighty is just over two hundred and ninety-three.</p>
</section>
<section id="review-of-stratified-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="review-of-stratified-sampling">Review of stratified sampling</h4>
<p>Stratified sampling groups by the country subgroup before performing simple random sampling on each subgroup. Given each of these top countries have quite a few rows, stratifying produces the same number of rows as the simple random sample.</p>
</section>
<section id="review-of-cluster-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="review-of-cluster-sampling">Review of cluster sampling</h4>
<p>In the cluster sample, we’ve used two out of six countries to roughly mimic frac equals one-third from the other sample types. Setting n equal to one-sixth of the total number of rows gives roughly equal sample sizes in each of the two subgroups. Using <code>.shape</code> again, we see that this cluster sample has close to the same number of rows: two-hundred-ninety-two compared to two-hundred-ninety-three for the other sample types.</p>
</section>
<section id="calculating-mean-cup-points" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="calculating-mean-cup-points">Calculating mean cup points</h4>
<p>Let’s calculate a population parameter, the mean of the total cup points. When the population parameter is the mean of a field, it’s often called the population mean. Remember that in real-life scenarios, we typically wouldn’t know what the population mean is. Since we have it here, though, we can use this value of eighty-one-point-nine as a gold standard to measure against. Now we’ll calculate the same value using each of the sampling techniques we’ve discussed. These are point estimates of the mean, often called sample means. The simple and stratified sample means are really close to the population mean. Cluster sampling isn’t quite as close, but that’s typical. Cluster sampling is designed to give us an answer that’s almost as good while using less data.</p>
</section>
<section id="mean-cup-points-by-country-simple-random" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="mean-cup-points-by-country-simple-random">Mean cup points by country: simple random</h4>
<p>Here’s a slightly more complicated calculation of the mean total cup points for each country. We group by country before calculating the mean to return six numbers. So how do the numbers from the simple random sample compare? The sample means are pretty close to the population means.</p>
</section>
<section id="mean-cup-points-by-country-stratified" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="mean-cup-points-by-country-stratified">Mean cup points by country: stratified</h4>
<p>The same is true of the sample means from the stratified technique. Each sample mean is relatively close to the population mean.</p>
</section>
<section id="mean-cup-points-by-country-cluster" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="mean-cup-points-by-country-cluster">Mean cup points by country: cluster</h4>
<p>With cluster sampling, while the sample means are pretty close to the population means, the obvious limitation is that we only get values for the two countries that were included in the sample. If the mean cup points for each country is an important metric in our analysis, cluster sampling would be a bad idea.</p>
</section>
</section>
<section id="exercise-2.4.1" class="level3" data-number="2.11">
<h3 data-number="2.11" class="anchored" data-anchor-id="exercise-2.4.1"><span class="header-section-number">2.11</span> Exercise 2.4.1</h3>
<section id="kinds-of-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="kinds-of-sampling">3 kinds of sampling</h4>
<p>You’re going to compare the performance of point estimates using simple, stratified, and cluster sampling. Before doing that, you’ll have to set up the samples.</p>
<p>You’ll use the <code>RelationshipSatisfaction</code> column of the <code>attrition</code> dataset, which categorizes the employee’s relationship with the company. It has four levels: <code>Low</code>, <code>Medium</code>, <code>High</code>, and <code>Very_High</code>.</p>
</section>
<section id="instructions-12" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-12">Instructions</h4>
<ol type="1">
<li>Perform simple random sampling on <code>attrition</code> to get one-quarter of the population, setting the seed to <code>2022</code>.</li>
<li>Perform stratified sampling on <code>attrition</code> to sample one-quarter of each <code>RelationshipSatisfaction</code> group, setting the seed to <code>2022</code>.</li>
<li>Create a list of unique values from <code>attrition</code>’s <code>RelationshipSatisfaction</code> column. Randomly sample <code>satisfaction_unique</code> to get two values. Subset the population for rows where <code>RelationshipSatisfaction</code> is in <code>satisfaction_samp</code> and clear any unused categories from <code>RelationshipSatisfaction</code>; assign to <code>attrition_clust_prep</code>. Perform cluster sampling on the selected satisfaction groups, sampling one quarter of the <em>population</em> and setting the seed to <code>2022</code>.</li>
</ol>
<div id="f4fb038e" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform simple random sampling to get 0.25 of the population</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>attrition_srs <span class="op">=</span> attrition.sample(frac<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform stratified sampling to get 0.25 of each relationship group</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>attrition_strat <span class="op">=</span> attrition.groupby(<span class="st">'RelationshipSatisfaction'</span>)<span class="op">\</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>.sample(frac<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of unique RelationshipSatisfaction values</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>satisfaction_unique <span class="op">=</span> <span class="bu">list</span>(attrition[<span class="st">'RelationshipSatisfaction'</span>].unique())</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly sample 2 unique satisfaction values</span></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>satisfaction_samp <span class="op">=</span> random.sample(satisfaction_unique, k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter for satisfaction_samp and clear unused categories from RelationshipSatisfaction</span></span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>satis_condition <span class="op">=</span> attrition[<span class="st">'RelationshipSatisfaction'</span>].isin(satisfaction_samp)</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>attrition_clust_prep <span class="op">=</span> attrition[satis_condition]</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>attrition_clust_prep[<span class="st">'RelationshipSatisfaction'</span>] <span class="op">=</span> attrition_clust_prep[<span class="st">'RelationshipSatisfaction'</span>].cat.remove_unused_categories()</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform cluster sampling on the selected group, getting 0.25 of attrition_clust_prep</span></span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>attrition_clust <span class="op">=</span> attrition_clust_prep.groupby(<span class="st">"RelationshipSatisfaction"</span>)<span class="op">\</span></span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>.sample(n<span class="op">=</span><span class="bu">len</span>(attrition) <span class="op">//</span> <span class="dv">6</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_clust)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>      Age  Attrition     BusinessTravel  DailyRate            Department  \
1381   45        1.0      Travel_Rarely       1449                 Sales   
1357   42        0.0      Travel_Rarely        300  Research_Development   
924    30        0.0      Travel_Rarely        288  Research_Development   
1224   46        0.0      Travel_Rarely       1003  Research_Development   
1277   48        0.0      Travel_Rarely       1236  Research_Development   
...   ...        ...                ...        ...                   ...   
357    27        0.0      Travel_Rarely        798  Research_Development   
424    44        1.0  Travel_Frequently        429  Research_Development   
1182   36        0.0  Travel_Frequently        884  Research_Development   
1055   34        0.0  Travel_Frequently        669  Research_Development   
962    34        0.0      Travel_Rarely       1031  Research_Development   

      DistanceFromHome Education EducationField EnvironmentSatisfaction  \
1381                 2  Bachelor      Marketing                     Low   
1357                 2  Bachelor  Life_Sciences                     Low   
924                  2  Bachelor  Life_Sciences                    High   
1224                 8    Master  Life_Sciences               Very_High   
1277                 1    Master  Life_Sciences               Very_High   
...                ...       ...            ...                     ...   
357                  6    Master        Medical                     Low   
424                  1   College        Medical                    High   
1182                23   College        Medical                    High   
1055                 1  Bachelor        Medical               Very_High   
962                  6    Master  Life_Sciences                    High   

      Gender  ...  PerformanceRating RelationshipSatisfaction  \
1381  Female  ...          Excellent                      Low   
1357    Male  ...          Excellent                      Low   
924     Male  ...          Excellent                      Low   
1224  Female  ...        Outstanding                      Low   
1277  Female  ...          Excellent                      Low   
...      ...  ...                ...                      ...   
357   Female  ...          Excellent                     High   
424     Male  ...          Excellent                     High   
1182    Male  ...          Excellent                     High   
1055    Male  ...        Outstanding                     High   
962   Female  ...          Excellent                     High   

      StockOptionLevel TotalWorkingYears TrainingTimesLastYear  \
1381                 0                26                     2   
1357                 0                24                     2   
924                  3                11                     3   
1224                 3                19                     2   
1277                 1                21                     3   
...                ...               ...                   ...   
357                  2                 6                     5   
424                  3                 6                     2   
1182                 1                17                     3   
1055                 0                14                     3   
962                  1                12                     3   

     WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \
1381          Better              24                  10   
1357            Good              22                   6   
924           Better              11                  10   
1224          Better              16                  13   
1277             Bad               3                   2   
...              ...             ...                 ...   
357             Good               5                   3   
424             Good               5                   3   
1182          Better               5                   2   
1055          Better              13                   9   
962           Better               1                   0   

      YearsSinceLastPromotion YearsWithCurrManager  
1381                        1                   11  
1357                        4                   14  
924                        10                    8  
1224                        1                    7  
1277                        0                    2  
...                       ...                  ...  
357                         0                    3  
424                         2                    3  
1182                        0                    3  
1055                        4                    9  
962                         0                    0  

[490 rows x 31 columns]</code></pre>
</div>
</div>
</section>
</section>
<section id="exercise-2.4.4" class="level3" data-number="2.12">
<h3 data-number="2.12" class="anchored" data-anchor-id="exercise-2.4.4"><span class="header-section-number">2.12</span> Exercise 2.4.4</h3>
<section id="comparing-point-estimates" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="comparing-point-estimates">Comparing point estimates</h4>
<p>Now that you have three types of sample (simple, stratified, and cluster), you can compare point estimates from each sample to the population parameter. That is, you can calculate the same summary statistic on each sample and see how it compares to the summary statistic for the population.</p>
<p>Here, we’ll look at how satisfaction with the company affects whether or not the employee leaves the company. That is, you’ll calculate the proportion of employees who left the company (they have an Attrition value of <code>1</code>) for each value of <code>RelationshipSatisfaction</code>.</p>
</section>
<section id="instructions-13" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-13">Instructions</h4>
<ol type="1">
<li>Group <code>attrition</code> by <code>RelationshipSatisfaction</code> levels and calculate the mean of Attrition for each level.</li>
<li>Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the simple random sample, <code>attrition_srs</code>.</li>
<li>Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the stratified sample, <code>attrition_strat</code>.</li>
<li>Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the cluster sample, <code>attrition_clust</code>.</li>
</ol>
<div id="76143c0e" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform simple random sampling to get 0.25 of the population</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>attrition_srs <span class="op">=</span> attrition.sample(frac<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform stratified sampling to get 0.25 of each relationship group</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>attrition_strat <span class="op">=</span> attrition.groupby(<span class="st">'RelationshipSatisfaction'</span>)<span class="op">\</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>.sample(frac<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Mean Attrition by RelationshipSatisfaction group</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>mean_attrition_pop <span class="op">=</span> attrition.groupby(<span class="st">'RelationshipSatisfaction'</span>)<span class="op">\</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_attrition_pop)</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the same thing for the simple random sample </span></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>mean_attrition_srs <span class="op">=</span> attrition_srs.groupby(<span class="st">'RelationshipSatisfaction'</span>)<span class="op">\</span></span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_attrition_srs)</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the same thing for the stratified sample </span></span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>mean_attrition_strat <span class="op">=</span> attrition_strat.groupby(<span class="st">'RelationshipSatisfaction'</span>)<span class="op">\</span></span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_attrition_strat)</span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the same thing for the cluster sample </span></span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>mean_attrition_clust <span class="op">=</span> attrition_clust.groupby(<span class="st">'RelationshipSatisfaction'</span>)<span class="op">\</span></span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_attrition_clust)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>RelationshipSatisfaction
Low          0.206522
Medium       0.148515
High         0.154684
Very_High    0.148148
Name: Attrition, dtype: float64</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
RelationshipSatisfaction
Low          0.134328
Medium       0.164179
High         0.160000
Very_High    0.155963
Name: Attrition, dtype: float64
RelationshipSatisfaction
Low          0.144928
Medium       0.078947
High         0.165217
Very_High    0.129630
Name: Attrition, dtype: float64
RelationshipSatisfaction
Low     0.191837
High    0.134694
Name: Attrition, dtype: float64</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="chapter-3-sampling-distributions" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="chapter-3-sampling-distributions"><span class="header-section-number">3</span> CHAPTER 3: Sampling Distributions</h2>
<p>Let’s test your sampling. In this chapter, you’ll discover how to quantify the accuracy of sample statistics using relative errors, and measure variation in your estimates by generating sampling distributions.</p>
<section id="chapter-3.1-relative-error-of-point-estimates" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="chapter-3.1-relative-error-of-point-estimates"><span class="header-section-number">3.1</span> Chapter 3.1: Relative error of point estimates</h3>
<p>Let’s see how the size of the sample affects the accuracy of the point estimates we calculate.</p>
<section id="sample-size-is-number-of-rows" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sample-size-is-number-of-rows">Sample size is number of rows</h4>
<p>The sample size, calculated here with the <code>len</code> function, is the number of observations, that is, the number of rows in the sample. That’s true whichever method we use to create the sample. We’ll stick to looking at simple random sampling since it works well in most cases and it’s easier to reason about.</p>
</section>
<section id="various-sample-sizes" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="various-sample-sizes">Various sample sizes</h4>
<p>Let’s calculate a population parameter, the mean cup points of the coffees. It’s around eighty-two-point-one-five. This is our gold standard to compare against. If we take a sample size of ten, the point estimate of this parameter is wrong by about point-eight-eight. Increasing the sample size to one hundred gets us closer; the estimate is only wrong by about point-three-four. Increasing the sample size further to one thousand brings the estimate to about point-zero-three away from the population parameter. In general, larger sample sizes will give us more accurate results.</p>
</section>
<section id="relative-errors" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="relative-errors">Relative errors</h4>
<p>For any of these sample sizes, we want to compare the population mean to the sample mean. This is the same code we just saw, but with the numerical sample size replaced with a variable named <code>sample_size</code>. The most common metric for assessing the difference between the population and a sample mean is the relative error. The relative error is the absolute difference between the two numbers; that is, we ignore any minus signs, divided by the population mean. Here, we also multiply by one hundred to make it a percentage.</p>
</section>
<section id="relative-error-vs.-sample-size" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="relative-error-vs.-sample-size">Relative error vs.&nbsp;sample size</h4>
<p>Here’s a line plot of relative error versus sample size. We see that the relative error decreases as the sample size increases, and beyond that, the plot has other important properties. Firstly, the blue line is really noisy, particularly for small sample sizes. If our sample size is small, the sample mean we calculate can be wildly different by adding one or two more random rows to the sample. Secondly, the amplitude of the line is quite steep, to begin with. When we have a small sample size, adding just a few more samples can give us much better accuracy. Further to the right of the plot, the line is less steep. If we already have a large sample size, adding a few more rows to the sample doesn’t bring as much benefit. Finally, at the far right of the plot, where the sample size is the whole population, the relative error decreases to zero.</p>
</section>
</section>
<section id="exercise-3.1.1" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="exercise-3.1.1"><span class="header-section-number">3.2</span> Exercise 3.1.1</h3>
<section id="calculating-relative-errors" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="calculating-relative-errors">Calculating relative errors</h4>
<p>The size of the sample you take affects how accurately the point estimates reflect the corresponding population parameter. For example, when you calculate a sample mean, you want it to be close to the population mean. However, if your sample is too small, this might not be the case.</p>
<p>The most common metric for assessing accuracy is <em>relative error</em>. This is the absolute difference between the population parameter and the point estimate, all divided by the population parameter. It is sometimes expressed as a percentage.</p>
</section>
<section id="instructions-14" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-14">Instructions</h4>
<ol type="1">
<li>Generate a simple random sample from attrition_pop of fifty rows, setting the seed to 2022.</li>
</ol>
<ul>
<li>Calculate the mean employee <code>Attrition</code> in the sample.</li>
<li>Calculate the relative error between <code>mean_attrition_srs50</code> and <code>mean_attrition_pop</code> as a <em>percentage</em>.</li>
</ul>
<ol start="2" type="1">
<li>Calculate the <em>relative error percentage</em> again. This time, use a simple random sample of one hundred rows of <code>attrition</code>.</li>
</ol>
<div id="1ff3fd52" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Population Attrtion mean </span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>mean_attrition_pop <span class="op">=</span> attrition[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_attrition_pop)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a simple random sample of 50 rows, with seed 2022</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>attrition_srs50 <span class="op">=</span> attrition.sample(n<span class="op">=</span><span class="dv">50</span>, random_state <span class="op">=</span> <span class="dv">2022</span>)</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean employee attrition in the sample</span></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>mean_attrition_srs50 <span class="op">=</span> attrition_srs50[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the relative error percentage</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>rel_error_pct50 <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> <span class="bu">abs</span>(mean_attrition_pop <span class="op">-</span> mean_attrition_srs50)<span class="op">/</span>mean_attrition_pop</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Print rel_error_pct50</span></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rel_error_pct50)</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a simple random sample of 100 rows, with seed 2022</span></span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>attrition_srs100 <span class="op">=</span> attrition.sample(n<span class="op">=</span><span class="dv">100</span>, random_state <span class="op">=</span> <span class="dv">2022</span>)</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean employee attrition in the sample</span></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>mean_attrition_srs100 <span class="op">=</span> attrition_srs100[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the relative error percentage</span></span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>rel_error_pct100 <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> <span class="bu">abs</span>(mean_attrition_pop <span class="op">-</span> mean_attrition_srs100)<span class="op">/</span>mean_attrition_pop</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Print rel_error_pct100</span></span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rel_error_pct100)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>0.16122448979591836
62.78481012658227
6.962025316455695</code></pre>
</div>
</div>
</section>
</section>
<section id="chapter-3.2-creating-a-sampling-distribution" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="chapter-3.2-creating-a-sampling-distribution"><span class="header-section-number">3.3</span> Chapter 3.2: Creating a sampling distribution</h3>
<p>We just saw how point estimates like the sample mean will vary depending on which rows end up in the sample.</p>
<section id="same-code-different-answer" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="same-code-different-answer">Same code, different answer</h4>
<p>For example, this same code to calculate the mean cup points from a simple random sample of thirty coffees gives a slightly different answer each time. Let’s try to visualize and quantify this variation.</p>
</section>
<section id="same-code-1000-times" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="same-code-1000-times">Same code, 1000 times</h4>
<p>A for loop lets us run the same code many times. It’s especially useful for situations like this where the result contains some randomness. We start by creating an empty list to store the means. Then, we set up the for loop to repeatedly sample 30 coffees from coffee_ratings a total of 1000 times, calculating the mean cup points each time. After each calculation, we append the result, also called a replicate, to the list. Each time the code is run, we get one sample mean, so running the code a thousand times generates a list of one thousand sample means.</p>
</section>
<section id="distribution-of-sample-means-for-size-30" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="distribution-of-sample-means-for-size-30">Distribution of sample means for size 30</h4>
<p>The one thousand sample means form a distribution of sample means. To visualize a distribution, the best plot is often a histogram. Here we can see that most of the results lie between eighty-one and eighty-three, and they roughly follow a bell-shaped curve, like a normal distribution. There’s an important piece of jargon we need to know here. A distribution of replicates of sample means, or other point estimates, is known as a sampling distribution.</p>
</section>
<section id="different-sample-sizes" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="different-sample-sizes">Different sample sizes</h4>
<p>Here are histograms from running the same code again with different sample sizes. When we decrease the original sample size of thirty to six, we can see from the x-values that the range of the results is broader. The bulk of the results now lie between eighty and eighty-four. On the other hand, increasing the sample size to one hundred and fifty results in a much narrower range. Now most of the results are between eighty-one-point-eight and eighty-two-point-six. As we saw previously, bigger sample sizes give us more accurate results. By replicating the sampling many times, as we’ve done here, we can quantify that accuracy.</p>
</section>
</section>
<section id="exercise-3.2.1" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="exercise-3.2.1"><span class="header-section-number">3.4</span> Exercise 3.2.1</h3>
<section id="replicating-samples" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="replicating-samples">Replicating samples</h4>
<p>When you calculate a point estimate such as a sample mean, the value you calculate depends on the rows that were included in the sample. That means that there is some randomness in the answer. In order to quantify the variation caused by this randomness, you can create many samples and calculate the sample mean (or another statistic) for each sample.</p>
</section>
<section id="instructions-15" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-15">Instructions</h4>
<ol type="1">
<li>Replicate the provided code so that it runs 500 times. Assign the resulting list of sample means to <code>mean_attritions</code>.</li>
<li>Draw a histogram of the <code>mean_attritions</code> list with 16 bins.</li>
</ol>
<div id="096fd290" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an empty list</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>mean_attritions <span class="op">=</span> []</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop 500 times to create 500 sample means</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    mean_attritions.append(</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">60</span>)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the first few entries of the list</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_attritions[<span class="dv">0</span>:<span class="dv">5</span>])</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a histogram of the 500 sample means</span></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>plt.hist(mean_attritions, bins<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[0.16666666666666666, 0.18333333333333332, 0.1, 0.1, 0.1]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-17-output-2.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="chapter-3.3-approximate-sampling-distributions" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="chapter-3.3-approximate-sampling-distributions"><span class="header-section-number">3.5</span> Chapter 3.3: Approximate sampling distributions</h3>
<p>In the last exercise, we saw that while increasing the number of replicates didn’t affect the relative error of the sample means; it did result in a more consistent shape to the distribution.</p>
<section id="dice" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="dice">4 dice</h4>
<p>Let’s consider the case of four six-sided dice rolls. We can generate all possible combinations of rolls using the <code>expand_grid</code> function, which is defined in the pandas documentation, and uses the <code>itertools</code> package. There are six to the power four, or one-thousand-two-hundred-ninety-six possible dice roll combinations.</p>
</section>
<section id="mean-roll" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="mean-roll">Mean roll</h4>
<p>Let’s consider the mean of the four rolls by adding a column to our DataFrame called <code>mean_roll</code>. <code>mean_roll</code> ranges from 1, when four ones are rolled, to 6, when four sixes are rolled.</p>
</section>
<section id="exact-sampling-distribution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exact-sampling-distribution">Exact sampling distribution</h4>
<p>Since the mean roll takes discrete values instead of continuous values, the best way to see the distribution of mean_roll is to draw a bar plot. First, we convert mean_roll to a categorical by setting its type to category. We are interested in the counts of each value, so we use dot-<code>value_counts</code>, passing the sort equals False argument. This ensures the x-axis ranges from one to six instead of sorting the bars by frequency. Chaining <code>.plot</code> to <code>value_counts</code>, and setting kind to <code>"bar"</code>, produces a bar plot of the mean roll distribution. This is the exact sampling distribution of the mean roll because it contains every single combination of die rolls.</p>
</section>
<section id="the-number-of-outcomes-increases-fast" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-number-of-outcomes-increases-fast">The number of outcomes increases fast</h4>
<p>If we increase the number of dice in our scenario, the number of possible outcomes increases by a factor of six each time. These values can be shown by creating a DataFrame with two columns: <code>n_dice</code>, ranging from 1 to 100, and <code>n_outcomes</code>, which is the number of possible outcomes, calculated using six to the power of the number of dice. With just one hundred dice, the number of outcomes is about the same as the number of atoms in the universe: six-point-five times ten to the seventy-seventh power. Long before you start dealing with big datasets, it becomes computationally impossible to calculate the exact sampling distribution. That means we need to rely on approximations.</p>
</section>
<section id="simulating-the-mean-of-four-dice-rolls" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="simulating-the-mean-of-four-dice-rolls">Simulating the mean of four dice rolls</h4>
<p>We can generate a sample mean of four dice rolls using NumPy’s <code>random.choice</code> method, specifying size as four. This will randomly choose values from a specified list, in this case, four values from the numbers one to six, which is created using a range from one to seven wrapped in the list function. Notice that we set <code>replace</code> equals <code>True</code> because we can roll the same number several times.</p>
</section>
<section id="simulating-the-mean-of-four-dice-rolls-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="simulating-the-mean-of-four-dice-rolls-1">Simulating the mean of four dice rolls</h4>
<p>Then we use a for loop to generate lots of sample means, in this case, one thousand. We again use the <code>.append</code> method to populate the sample means list with our simulated sample means. The output contains a sampling of many of the same values we saw with the exact sampling distribution.</p>
</section>
<section id="approximate-sampling-distribution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="approximate-sampling-distribution">Approximate sampling distribution</h4>
<p>Here’s a histogram of the approximate sampling distribution of mean rolls. This time, it uses the simulated rather than the exact values. It’s known as an approximate sampling distribution. Notice that although it isn’t perfect, it’s pretty close to the exact sampling distribution. Usually, we don’t have access to the whole population, so we can’t calculate the exact sampling distribution. However, we can feel relatively confident that using an approximation will provide a good guess as to how the sampling distribution will behave.</p>
</section>
</section>
<section id="exercise-3.3.1" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="exercise-3.3.1"><span class="header-section-number">3.6</span> Exercise 3.3.1</h3>
<section id="exact-sampling-distribution-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exact-sampling-distribution-1">Exact sampling distribution</h4>
<p>To quantify how the point estimate (sample statistic) you are interested in varies, you need to know all the possible values it can take and how often. That is, you need to know its distribution.</p>
<p>The distribution of a sample statistic is called the <em>sampling distribution</em>. When we can calculate this exactly, rather than using an approximation, it is known as the <em>exact sampling distribution</em>.</p>
<p>Let’s take another look at the sampling distribution of dice rolls. This time, we’ll look at five eight-sided dice. (These have the numbers one to eight.)</p>
</section>
<section id="instructions-16" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-16">Instructions</h4>
<ol type="1">
<li>Expand a grid representing 5 8-sided dice. That is, create a DataFrame with five columns from a dictionary, named <code>die1</code> to <code>die5</code>. The rows should contain all possibilities for throwing five dice, each numbered <code>1</code> to <code>8</code>.</li>
<li>Add a column, <code>mean_roll</code>, to dice, that contains the mean of the five rolls as a categorical.</li>
<li>Create a bar plot of the <code>mean_roll</code> categorical column, so it displays the count of each <code>mean_roll</code> in increasing order from <code>1.0</code> to <code>8.0</code>.</li>
</ol>
<div id="7d429804" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to create a grid of all possible combinations</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> expand_grid(dictionary):</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> itertools <span class="im">import</span> product</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame([row <span class="cf">for</span> row <span class="kw">in</span> product(<span class="op">*</span>dictionary.values())], columns<span class="op">=</span>dictionary.keys())</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Expand a grid representing 5 8-sided dice</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>dice <span class="op">=</span> expand_grid(</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'die1'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>),</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>     <span class="st">'die2'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>),</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>     <span class="st">'die3'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>),</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>     <span class="st">'die4'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>),</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>     <span class="st">'die5'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>)}</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dice)</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a column of mean rolls and convert to a categorical</span></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>dice[<span class="st">'mean_roll'</span>] <span class="op">=</span> (dice[<span class="st">'die1'</span>]<span class="op">+</span> dice[<span class="st">'die2'</span>]<span class="op">+</span> dice[<span class="st">'die3'</span>]<span class="op">+</span> dice[<span class="st">'die4'</span>]<span class="op">+</span> dice[<span class="st">'die5'</span>])<span class="op">/</span><span class="dv">5</span></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>                     </span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>dice[<span class="st">'mean_roll'</span>] <span class="op">=</span> dice[<span class="st">'mean_roll'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Print result</span></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dice)</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a bar plot of mean_roll</span></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>dice[<span class="st">'mean_roll'</span>].value_counts(sort<span class="op">=</span><span class="va">False</span>).plot(kind<span class="op">=</span><span class="st">'bar'</span>)</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>       die1  die2  die3  die4  die5
0         1     1     1     1     1
1         1     1     1     1     2
2         1     1     1     1     3
3         1     1     1     1     4
4         1     1     1     1     5
...     ...   ...   ...   ...   ...
32763     8     8     8     8     4
32764     8     8     8     8     5
32765     8     8     8     8     6
32766     8     8     8     8     7
32767     8     8     8     8     8

[32768 rows x 5 columns]
       die1  die2  die3  die4  die5 mean_roll
0         1     1     1     1     1       1.0
1         1     1     1     1     2       1.2
2         1     1     1     1     3       1.4
3         1     1     1     1     4       1.6
4         1     1     1     1     5       1.8
...     ...   ...   ...   ...   ...       ...
32763     8     8     8     8     4       7.2
32764     8     8     8     8     5       7.4
32765     8     8     8     8     6       7.6
32766     8     8     8     8     7       7.8
32767     8     8     8     8     8       8.0

[32768 rows x 6 columns]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-18-output-2.png" width="583" height="438" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-3.3.2" class="level3" data-number="3.7">
<h3 data-number="3.7" class="anchored" data-anchor-id="exercise-3.3.2"><span class="header-section-number">3.7</span> Exercise 3.3.2</h3>
<section id="generating-an-approximate-sampling-distribution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="generating-an-approximate-sampling-distribution">Generating an approximate sampling distribution</h4>
<p>Calculating the exact sampling distribution is only possible in very simple situations. With just five eight-sided dice, the number of possible rolls is <code>8**5</code>, which is over thirty thousand. When the dataset is more complicated, for example, where a variable has hundreds or thousands of categories, the number of possible outcomes becomes too difficult to compute exactly.</p>
<p>In this situation, you can calculate an <em>approximate sampling distribution</em> by simulating the exact sampling distribution. That is, you can repeat a procedure over and over again to simulate both the sampling process and the sample statistic calculation process.</p>
</section>
<section id="instructions-17" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-17">Instructions</h4>
<ol type="1">
<li>Sample one to eight, five times, with replacement. Assign to <code>five_rolls</code>.</li>
</ol>
<ul>
<li>Calculate the mean of <code>five_rolls</code>.</li>
</ul>
<ol start="2" type="1">
<li>Replicate the sampling code 1000 times, assigning each result to the list <code>sample_means_1000</code>.</li>
<li>Plot <code>sample_means_1000</code> as a histogram with 20 bins.</li>
</ol>
<div id="cd46b0c1" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample one to eight, five times, with replacement</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>five_rolls <span class="op">=</span> np.random.choice(<span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>)), size<span class="op">=</span><span class="dv">5</span>, replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the mean of five_rolls</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(five_rolls.mean())</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Replicate the sampling code 1000 times</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>sample_means_1000 <span class="op">=</span> []</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>    sample_means_1000.append(</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>        np.random.choice(<span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>)), size<span class="op">=</span><span class="dv">5</span>, replace<span class="op">=</span><span class="va">True</span>).mean()</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the first 10 entries of the result</span></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample_means_1000[<span class="dv">0</span>:<span class="dv">10</span>])</span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a histogram of sample_means_1000 with 20 bins</span></span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a>plt.hist(sample_means_1000, bins<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>3.4
[3.8, 3.6, 4.4, 3.6, 2.8, 4.6, 4.6, 4.6, 3.6, 5.4]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-19-output-2.png" width="575" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="chapter-3.4-standard-errors-and-the-central-limit-theorem" class="level3" data-number="3.8">
<h3 data-number="3.8" class="anchored" data-anchor-id="chapter-3.4-standard-errors-and-the-central-limit-theorem"><span class="header-section-number">3.8</span> Chapter 3.4: Standard errors and the Central Limit Theorem</h3>
<p>The Gaussian distribution (also known as the normal distribution) plays an important role in statistics. Its distinctive bell-shaped curve has been cropping up throughout this course.</p>
<section id="sampling-distribution-of-mean-cup-points" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sampling-distribution-of-mean-cup-points">Sampling distribution of mean cup points</h4>
<p>Here are approximate sampling distributions of the mean cup points from the coffee dataset. Each histogram shows five thousand replicates, with different sample sizes in each case. Look at the x-axis labels. We already saw how increasing the sample size results in greater accuracy in our estimates of the population parameter, so the width of the distribution shrinks as the sample size increases. When the sample size is five, the x-axis ranges from seventy-six to eighty-six, whereas, for a sample size of three hundred and twenty, the range is from eighty-one-point-six to eighty-two-point-six. Now, look at the shape of each distribution. As the sample size increases, we can see that the shape of the curve gets closer and closer to being a normal distribution. At sample size five, the curve is only a very loose approximation since it isn’t very symmetric. By sample size eighty, it is a very reasonable approximation.</p>
</section>
<section id="consequences-of-the-central-limit-theorem" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="consequences-of-the-central-limit-theorem">Consequences of the central limit theorem</h4>
<p>What we just saw is, in essence, what the central limit theorem tells us. The means of independent samples have normal distributions. Then, as the sample size increases, we see two things. The distribution of these averages gets closer to being normal, and the width of this sampling distribution gets narrower.</p>
</section>
<section id="population-sampling-distribution-means" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="population-sampling-distribution-means">Population &amp; sampling distribution means</h4>
<p>Recall the population parameter of the mean cup points. We’ve seen this calculation before, and its value is eighty-two-point-one-five. We can also calculate summary statistics on our sampling distributions to see how they compare. For each of our four sampling distributions, if we take the mean of our sample means, we can see that we get values that are pretty close to the population parameter that the sampling distributions are trying to estimate.</p>
</section>
<section id="population-sampling-distribution-standard-deviations" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="population-sampling-distribution-standard-deviations">Population &amp; sampling distribution standard deviations</h4>
<p>Now let’s consider the standard deviation of the population cup points. It’s about two-point-seven. By comparison, if we take the standard deviation of the sample means from each of the sampling distributions using NumPy, we get much smaller numbers, and they decrease as the sample size increases. Note that when we are calculating a population standard deviation with pandas <code>.std</code>, we must specify <code>ddof</code> equals zero, as <code>.std</code> calculates a sample standard deviation by default. When we are calculating a standard deviation on a sample of the population using NumPy’s std function, like in these calculations on the sampling distribution, we must specify a <code>ddof</code> of one. So what are these smaller standard deviation values?</p>
</section>
<section id="population-mean-over-square-root-sample-size" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="population-mean-over-square-root-sample-size">Population mean over square root sample size</h4>
<p>One other consequence of the central limit theorem is that if we divide the population standard deviation, in this case around 2.7, by the square root of the sample size, we get an estimate of the standard deviation of the sampling distribution for that sample size. It isn’t exact because of the randomness involved in the sampling process, but it’s pretty close.</p>
</section>
<section id="standard-error" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="standard-error">Standard error</h4>
<p>We just saw the impact of the sample size on the standard deviation of the sampling distribution. This standard deviation of the sampling distribution has a special name: the standard error. It is useful in a variety of contexts, from estimating population standard deviation to setting expectations on what level of variability we would expect from the sampling process.</p>
</section>
</section>
<section id="exercise-3.4.1" class="level3" data-number="3.9">
<h3 data-number="3.9" class="anchored" data-anchor-id="exercise-3.4.1"><span class="header-section-number">3.9</span> Exercise 3.4.1</h3>
<section id="population-sampling-distribution-means-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="population-sampling-distribution-means-1">Population &amp; sampling distribution means</h4>
<p>One of the useful features of sampling distributions is that you can quantify them. Specifically, you can calculate summary statistics on them. Here, you’ll look at the relationship between the mean of the sampling distribution and the population parameter’s mean.</p>
<p>Three sampling distributions are provided. For each, the employee attrition dataset was sampled using simple random sampling, then the mean attrition was calculated. This was done 1000 times to get a sampling distribution of mean attritions. One sampling distribution used a sample size of 5 for each replicate, one used 50, and one used 500.</p>
</section>
<section id="instructions-18" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-18">Instructions</h4>
<ol type="1">
<li>Calculate the mean of <code>sampling_distribution_5</code>, <code>sampling_distribution_50</code>, and <code>sampling_distribution_500</code> (a mean of sample means).</li>
</ol>
<div id="4eae3732" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Set a seed for reproducibility</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>random_seed <span class="op">=</span> <span class="dv">2021</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create three empty lists to hold the sampling distributions</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>sampling_distribution_5 <span class="op">=</span> []   <span class="co"># Sample size of 5</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>sampling_distribution_50 <span class="op">=</span> []  <span class="co"># Sample size of 50</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>sampling_distribution_500 <span class="op">=</span> [] <span class="co"># Sample size of 500</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform biased sampling and calculate mean attrition 1000 times for each sample size</span></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample size = 5 (heavier weights toward high attrition)</span></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>    sampling_distribution_5.append(</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span>random_seed <span class="op">+</span> i)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample size = 50 (bias reduces as sample size increases)</span></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>    sampling_distribution_50.append(</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">50</span>, random_state<span class="op">=</span>random_seed <span class="op">+</span> i)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample size = 500 (approaching unbiased mean)</span></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>    sampling_distribution_500.append(</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">500</span>, random_state<span class="op">=</span>random_seed <span class="op">+</span> i)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional: Convert the sampling distributions to DataFrame for analysis</span></span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>sampling_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sample_Size_5'</span>: sampling_distribution_5,</span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sample_Size_50'</span>: sampling_distribution_50,</span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sample_Size_500'</span>: sampling_distribution_500</span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean of the mean attritions for each sampling distribution</span></span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a>mean_of_means_5 <span class="op">=</span> np.mean(sampling_distribution_5)</span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a>mean_of_means_50 <span class="op">=</span> np.mean(sampling_distribution_50)</span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a>mean_of_means_500 <span class="op">=</span> np.mean(sampling_distribution_500)</span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_of_means_5)</span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_of_means_50)</span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_of_means_500)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>0.155
0.15998
0.160622</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>Even for small sample sizes, the mean of the sampling distribution is a good approximation of the population mean.</em></p>
</div>
</div>
</section>
</section>
<section id="exercise-3.4.2" class="level3" data-number="3.10">
<h3 data-number="3.10" class="anchored" data-anchor-id="exercise-3.4.2"><span class="header-section-number">3.10</span> Exercise 3.4.2</h3>
<section id="population-sampling-distribution-variation" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="population-sampling-distribution-variation">Population &amp; sampling distribution variation</h4>
<p>You just calculated the mean of the sampling distribution and saw how it is an estimate of the corresponding population parameter. Similarly, as a result of the central limit theorem, the standard deviation of the sampling distribution has an interesting relationship with the population parameter’s standard deviation and the sample size.</p>
</section>
<section id="instructions-19" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-19">Instructions</h4>
<ol type="1">
<li>Calculate the standard deviation of <code>sampling_distribution_5</code>, <code>sampling_distribution_50</code>, and <code>sampling_distribution_500</code> (a standard deviation of sample means).</li>
</ol>
<div id="b959679f" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Set a seed for reproducibility</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>random_seed <span class="op">=</span> <span class="dv">2021</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create three empty lists to hold the sampling distributions</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>sampling_distribution_5 <span class="op">=</span> []   <span class="co"># Sample size of 5</span></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>sampling_distribution_50 <span class="op">=</span> []  <span class="co"># Sample size of 50</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>sampling_distribution_500 <span class="op">=</span> [] <span class="co"># Sample size of 500</span></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform biased sampling and calculate mean attrition 1000 times for each sample size</span></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample size = 5 (heavier weights toward high attrition)</span></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>    sampling_distribution_5.append(</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span>random_seed <span class="op">+</span> i)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample size = 50 (bias reduces as sample size increases)</span></span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>    sampling_distribution_50.append(</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">50</span>, random_state<span class="op">=</span>random_seed <span class="op">+</span> i)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample size = 500 (approaching unbiased mean)</span></span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>    sampling_distribution_500.append(</span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">500</span>, random_state<span class="op">=</span>random_seed <span class="op">+</span> i)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional: Convert the sampling distributions to DataFrame for analysis</span></span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>sampling_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sample_Size_5'</span>: sampling_distribution_5,</span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sample_Size_50'</span>: sampling_distribution_50,</span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sample_Size_500'</span>: sampling_distribution_500</span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the std. dev. of the mean attritions for each sampling distribution</span></span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a>sd_of_means_5 <span class="op">=</span> np.std(sampling_distribution_5, ddof <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a>sd_of_means_50 <span class="op">=</span> np.std(sampling_distribution_50, ddof <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a>sd_of_means_500 <span class="op">=</span> np.std(sampling_distribution_500, ddof <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sd_of_means_5)</span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sd_of_means_50)</span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sd_of_means_500)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>0.15244093360458746
0.04970785119546479
0.014243454356018837</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>The amount of variation in the sampling distribution is related to the amount of variation in the population and the sample size. This is another consequence of the Central Limit Theorem.</em></p>
</div>
</div>
</section>
</section>
</section>
<section id="chapter-4-bootstrap-distributions" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="chapter-4-bootstrap-distributions"><span class="header-section-number">4</span> CHAPTER 4: Bootstrap Distributions</h2>
<p>You’ll get to grips with resampling to perform bootstrapping and estimate variation in an unknown population. You’ll learn the difference between sampling distributions and bootstrap distributions using resampling.</p>
<section id="chapter-4.1-introduction-to-bootstrapping" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="chapter-4.1-introduction-to-bootstrapping"><span class="header-section-number">4.1</span> Chapter 4.1: Introduction to bootstrapping</h3>
<p>So far, we’ve mostly focused on the idea of sampling without replacement.</p>
<section id="with-or-without" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="with-or-without">With or without</h4>
<p>Sampling without replacement is like dealing a pack of cards. When we deal the ace of spades to one player, we can’t then deal the ace of spades to another player. Sampling with replacement is like rolling dice. If we roll a six, we can still get a six on the next roll. Sampling with replacement is sometimes called resampling. We’ll use the terms interchangeably.</p>
</section>
<section id="simple-random-sampling-without-replacement" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="simple-random-sampling-without-replacement">Simple random sampling without replacement</h4>
<p>If we take a simple random sample without replacement, each row of the dataset, or each type of coffee, can only appear once in the sample.</p>
</section>
<section id="simple-random-sampling-with-replacement" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="simple-random-sampling-with-replacement">Simple random sampling with replacement</h4>
<p>If we sample with replacement, it means that each row of the dataset, or each coffee, can be sampled multiple times.</p>
</section>
<section id="why-sample-with-replacement" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="why-sample-with-replacement">Why sample with replacement?</h4>
<p>So far, we’ve been treating the <code>coffee_ratings</code> dataset as the population of all coffees. Of course, it doesn’t include every coffee in the world, so we could treat the coffee dataset as just being a big sample of coffees. To imagine what the whole population is like, we need to approximate the other coffees that aren’t in the dataset. Each of the coffees in the sample dataset will have properties that are representative of the coffees that we don’t have. Resampling lets us use the existing coffees to approximate those other theoretical coffees.</p>
</section>
<section id="coffee-data-preparation" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="coffee-data-preparation">Coffee data preparation</h4>
<p>To keep it simple, let’s focus on three columns of the coffee dataset. To make it easier to see which rows ended up in the sample, we’ll add a row index column called index using the <code>reset_index</code> method.</p>
</section>
<section id="resampling-with-.sample" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="resampling-with-.sample">Resampling with <code>.sample()</code></h4>
<p>To sample with replacement, we call sample as usual but set the <code>replace</code> argument to <code>True</code>. Setting <code>frac</code> to <code>1</code> produces a sample of the same size as the original dataset.</p>
</section>
<section id="repeated-coffees" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="repeated-coffees">Repeated coffees</h4>
<p>Counting the values of the index column shows how many times each coffee ended up in the resampled dataset. Some coffees were sampled four or five times.</p>
</section>
<section id="missing-coffees" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="missing-coffees">Missing coffees</h4>
<p>That means that some coffees didn’t end up in the resample. By taking the number of distinct index values in the resampled dataset, using <code>len</code> on <code>drop_duplicates</code>, we see that eight hundred and sixty-eight different coffees were included. By comparing this number with the total number of coffees, we can see that four hundred and seventy coffees weren’t included in the resample.</p>
</section>
<section id="bootstrapping" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bootstrapping">Bootstrapping</h4>
<p>We’re going to use resampling for a technique called bootstrapping. In some sense, bootstrapping is the opposite of sampling from a population. With sampling, we treat the dataset as the population and move to a smaller sample. With bootstrapping, we treat the dataset as a sample and use it to build up a theoretical population. A use case of bootstrapping is to try to understand the variability due to sampling. This is important in cases where we aren’t able to sample the population multiple times to create a sampling distribution.</p>
</section>
<section id="bootstrapping-process" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bootstrapping-process">Bootstrapping process</h4>
<p>The bootstrapping process has three steps. First, randomly sample with replacement to get a resample the same size as the original dataset. Then, calculate a statistic, such as a mean of one of the columns. Note that the mean isn’t always the choice here and bootstrapping allows for complex statistics to be computed, too. Then, replicate this many times to get lots of these bootstrap statistics. Earlier in the course, we did something similar. We took a simple random sample, then calculated a summary statistic, then repeated those two steps to form a sampling distribution. This time, when we’ve used resampling instead of sampling, we get a bootstrap distribution.</p>
</section>
<section id="bootstrapping-coffee-mean-flavor" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bootstrapping-coffee-mean-flavor">Bootstrapping coffee mean flavor</h4>
<p>The resampling step uses the code we just saw: calling sample with <code>frac</code> set to <code>one</code> and <code>replace</code> set to <code>True</code>. Calculating a bootstrap statistic can be done with mean from NumPy. In this case, we’re calculating the mean flavor score. To repeat steps one and two one thousand times, we can wrap the code in a for loop and append the statistics to a list.</p>
</section>
<section id="bootstrap-distribution-histogram" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bootstrap-distribution-histogram">Bootstrap distribution histogram</h4>
<p>Here’s a histogram of the bootstrap distribution of the sample mean. Notice that it is close to following a normal distribution.</p>
</section>
</section>
<section id="exercise-4.1.1" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="exercise-4.1.1"><span class="header-section-number">4.2</span> Exercise 4.1.1</h3>
<section id="generating-a-bootstrap-distribution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="generating-a-bootstrap-distribution">Generating a bootstrap distribution</h4>
<p>The process for generating a bootstrap distribution is similar to the process for generating a sampling distribution; only the first step is different.</p>
<p>To make a sampling distribution, you start with the population and sample without replacement. To make a bootstrap distribution, you start with a sample and sample that with replacement. After that, the steps are the same: calculate the summary statistic that you are interested in on that sample/resample, then replicate the process many times. In each case, you can visualize the distribution with a histogram.</p>
<p>Here, <code>spotify_sample</code> is a subset of the <code>spotify</code> dataset. To make it easier to see how resampling works, a row index column called <code>'index'</code> has been added, and only the artist name, song name, and <code>danceability</code> columns have been included.</p>
</section>
<section id="instructions-20" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-20">Instructions</h4>
<ol type="1">
<li>Generate a single bootstrap resample from <code>spotify_sample</code>.</li>
<li>Calculate the mean of the <code>danceability</code> column of <code>spotify_1_resample</code> using <code>numpy</code>.</li>
<li>Replicate the expression provided 1000 times.</li>
<li>Create a bootstrap distribution by drawing a histogram of <code>mean_danceability_1000</code>.</li>
</ol>
<div id="2378d482" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course array</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Subset of spotify sample to use</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify.sample(n<span class="op">=</span><span class="dv">41656</span>)[[<span class="st">'artists'</span>, <span class="st">'name'</span>, <span class="st">'danceability'</span>]]</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>spotify_sample[<span class="st">'index'</span>] <span class="op">=</span> spotify_sample.index</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Reorder columns to make 'index' the first column</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify_sample[[<span class="st">'index'</span>, <span class="st">'artists'</span>, <span class="st">'name'</span>, <span class="st">'danceability'</span>]]</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 1 bootstrap resample</span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>spotify_1_resample <span class="op">=</span> spotify_sample.sample(frac<span class="op">=</span><span class="dv">1</span>, replace <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the resample</span></span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(spotify_1_resample)</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate of the danceability column of spotify_1_resample</span></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>mean_danceability_1 <span class="op">=</span> np.mean(spotify_1_resample[<span class="st">'danceability'</span>])</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_danceability_1)</span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Replicate this 1000 times</span></span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>mean_danceability_1000 <span class="op">=</span> []</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>    mean_danceability_1000.append(</span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>        np.mean(spotify_sample.sample(frac<span class="op">=</span><span class="dv">1</span>, replace<span class="op">=</span><span class="va">True</span>)[<span class="st">'danceability'</span>])</span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_danceability_1000)</span>
<span id="cb43-38"><a href="#cb43-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-39"><a href="#cb43-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a histogram of the resample means</span></span>
<span id="cb43-40"><a href="#cb43-40" aria-hidden="true" tabindex="-1"></a>plt.hist(mean_danceability_1000)</span>
<span id="cb43-41"><a href="#cb43-41" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>       index                                   artists  \
40112  40112                  ['Peter, Paul and Mary']   
13844  13844                             ['Cat Power']   
19675  19675                                ['Thalía']   
7595    7595                              ['¡MAYDAY!']   
29531  29531                               ['50 Cent']   
...      ...                                       ...   
1695    1695  ['Nevada', 'Mark Morrison', 'Fetty Wap']   
24818  24818                        ['Eli Young Band']   
30099  30099                     ['Portugal. The Man']   
14328  14328                            ['Dan + Shay']   
3933    3933                           ['beabadoobee']   

                                    name  danceability  
40112  If I Had a Hammer - 2004 Remaster         0.628  
13844                          Manhattan         0.711  
19675  El Próximo Viernes - Live Version         0.755  
7595                            Badlands         0.730  
29531                               Heat         0.519  
...                                  ...           ...  
1695                            The Mack         0.711  
24818                         Love Ain't         0.600  
30099                         People Say         0.543  
14328                When I Pray for You         0.382  
3933                      If You Want To         0.666  

[41656 rows x 4 columns]
0.591741393796812
[0.5926616429806031, 0.5912292370846938, 0.5909229594776263, 0.5927271509506433, 0.5903040522373727, 0.5904833997503361, 0.5917418835221816, 0.5922994814672555, 0.5914843479930862, 0.5911015628000768, 0.5910246807182639, 0.5908936575763394, 0.5916269060879585, 0.5900065416746687, 0.5917592447666603, 0.5908108027655079, 0.5914857355483004, 0.5907100729786825, 0.5920515892068371, 0.5908976978106396, 0.5915547940272711, 0.5913858267716536, 0.5902494694641828, 0.5917825787401575, 0.5907649030151719, 0.5898005377376608, 0.5917094008066064, 0.591363736316497, 0.5930069281736124, 0.5914717639715767, 0.5915227866333782, 0.5912849193393509, 0.5916315728826579, 0.5912630929517957, 0.5917036537353564, 0.5913172484155945, 0.5920614677357404, 0.5918894973113117, 0.5914229018628769, 0.5909182566737086, 0.5912621135010563, 0.5932023886114846, 0.5913446466295372, 0.5914125432110621, 0.5915523838102553, 0.5906654263491453, 0.5916980434991357, 0.5903723353178414, 0.59163305406184, 0.5910800532936432, 0.5913561815824852, 0.5914952539850202, 0.5909793667178798, 0.5905758066064912, 0.5919920131553678, 0.5912781039946227, 0.5909638107355483, 0.5910683022853851, 0.5915094992318033, 0.5922963318609564, 0.591612788073747, 0.5917137915306319, 0.5907620150758595, 0.5920314144420973, 0.5923453164009986, 0.5909815032648358, 0.5905275854618782, 0.592712125504129, 0.5911578740157479, 0.5902561911849433, 0.591076615613597, 0.5912477698290762, 0.5928863140964086, 0.5911237876896486, 0.5917720736508546, 0.5907518172652199, 0.59126746207029, 0.5922028231227194, 0.5923195097945074, 0.5893636618974458, 0.5906233147685808, 0.589327114941425, 0.5907251656424046, 0.5908303245630883, 0.5918481035144997, 0.5904468071826388, 0.5913788313808335, 0.5913491717879777, 0.5928264739773381, 0.5911507898021894, 0.5917474337430383, 0.5907791674668716, 0.592098578836182, 0.5923731371231036, 0.5905206956980987, 0.5922919579412329, 0.5906497695410025, 0.592824565488765, 0.5894132273862108, 0.5910890892068369, 0.589665411945458, 0.590928648934127, 0.5908854858843864, 0.5906148814096408, 0.5911096168619167, 0.5914759530439793, 0.591913573074707, 0.5915902895141156, 0.5919703692145188, 0.5921623943729595, 0.5893560759554445, 0.5918974289418091, 0.5922629489149223, 0.5911910841175341, 0.5905550965047052, 0.5914129369118495, 0.5908735068177454, 0.5919226618014212, 0.5896804277895141, 0.5918910745150758, 0.5909926445169964, 0.5911779095448436, 0.5917400518532745, 0.5915469128096793, 0.5913175364893414, 0.5905761234876128, 0.5907693513539466, 0.5906233627808719, 0.5912526574803149, 0.5927125000000001, 0.5911204172268102, 0.591540640003841, 0.5917459309583253, 0.5929342711734203, 0.5907872023237949, 0.5913667538889956, 0.5913613477050124, 0.591543861628577, 0.5911535024966391, 0.5916594920299596, 0.5917597753024774, 0.5915537425580949, 0.5913673348377184, 0.5902052669483386, 0.5921307902823123, 0.5922133234107931, 0.59176450691377, 0.5908498079508354, 0.5921899246207029, 0.5902212502400616, 0.591075211254081, 0.5908238285000962, 0.5911779167466872, 0.591597575379297, 0.5911855266948339, 0.591486897445746, 0.5907305646245438, 0.5904625072018437, 0.5910472945073939, 0.5932251968503938, 0.5922508738236989, 0.5926040690416747, 0.5928815320722105, 0.592704957269061, 0.5919456188784329, 0.5913617774150182, 0.5920723593239869, 0.5910119646629537, 0.5911351858075667, 0.5908025206452852, 0.5905718023814096, 0.5913644805070098, 0.5918884698482811, 0.5920818753600922, 0.5896607163433839, 0.5931012171115806, 0.5906362204724409, 0.5907316857115422, 0.5917356923372383, 0.589505547820242, 0.5899278327251776, 0.590222049644709, 0.5902936095640483, 0.5908735500288073, 0.590986719800269, 0.5914564504513155, 0.5898611220472441, 0.5917383714230843, 0.5906654359516036, 0.5919472536969463, 0.5912775494526599, 0.5910284760898791, 0.5934322474553485, 0.5896524822354522, 0.5910561695794123, 0.5908920875744191, 0.5902741117726137, 0.5905362276742847, 0.590882072210486, 0.590599875168043, 0.5908478994622623, 0.5913667442865375, 0.5918546980026886, 0.5904053701747647, 0.5917759626464375, 0.5895187800076819, 0.5908222512963319, 0.5918495222777032, 0.5900889595736508, 0.5915296956020741, 0.5913352434223161, 0.5898937079892452, 0.5923311407720376, 0.5915862036681391, 0.5940695506049549, 0.5917825115229497, 0.5930165882465911, 0.5913268460725946, 0.5910937223929326, 0.5900466463414634, 0.5914370534856923, 0.5902467711734204, 0.5910614149222201, 0.5923320698098714, 0.5910843311887843, 0.5907899102170155, 0.5909062920107548, 0.592194761859036, 0.5913332365085462, 0.591112581620895, 0.5909766900326484, 0.5915309703284041, 0.590623329172268, 0.5901133426157096, 0.5921993206260803, 0.5908828812175917, 0.5899316689072402, 0.5901478538505858, 0.5905907672364126, 0.5902499351834068, 0.5930716007297867, 0.5911384122335318, 0.5904675052813521, 0.5917777270981371, 0.5917923180334165, 0.5915852050124832, 0.5902159376800461, 0.590632266660265, 0.5921240565584789, 0.5914197426541195, 0.5921987540810447, 0.5910413049740735, 0.5910402102938352, 0.5912252400614557, 0.592017966199347, 0.5932416074515076, 0.5914617077971961, 0.5900291458613406, 0.5921380521413483, 0.5920603274438255, 0.591674582293067, 0.5902335293835222, 0.5910828188016133, 0.5902600609756098, 0.5917250552141349, 0.59129137699251, 0.5906637987324755, 0.5911635226618014, 0.5929964830996735, 0.5919752664682159, 0.5907731539274055, 0.5931615661609372, 0.5916405919915498, 0.5921774246207028, 0.5912177573458806, 0.5899589518916841, 0.5922920563664298, 0.5903987444785865, 0.5900557206644902, 0.5916777799116574, 0.5904387555214134, 0.591779863645093, 0.5909780295755712, 0.5911936599769541, 0.5912351906087958, 0.5913116093719992, 0.5907024846360668, 0.59152518964855, 0.5913170131553677, 0.5928027318993663, 0.5910584045515651, 0.5906748223545227, 0.5914975537737661, 0.5910603538505858, 0.5910368038217784, 0.5906125840215095, 0.590829880449395, 0.5898112324755137, 0.5915295947762628, 0.5911917658920683, 0.5905858747839446, 0.5899634506433646, 0.5914477866333782, 0.59188657816401, 0.5911678725753793, 0.5908139523718072, 0.5912910673132321, 0.5905924116573842, 0.5919264043595159, 0.5914105771077396, 0.5909259602458229, 0.5907988453043979, 0.5905330948722872, 0.59271008738237, 0.591034023910121, 0.5906882922028039, 0.5912970016324179, 0.5905414826195505, 0.5907892524486268, 0.5912277415018246, 0.5926230146917612, 0.5910144973113118, 0.5903959333589398, 0.5893279575571346, 0.5915921884002304, 0.5923037113501056, 0.590284820914154, 0.5921825979450739, 0.5918227986364508, 0.5920301829268293, 0.5909208637411179, 0.5921829940464759, 0.5918935639523717, 0.5918183550989052, 0.5911464686959861, 0.5909657384290378, 0.5914849025350489, 0.5918623727674285, 0.5912250480122913, 0.5908601930094104, 0.5905958829460342, 0.5919878240829652, 0.5903600825811407, 0.5906465767236413, 0.5911581284808911, 0.5908419531400039, 0.5926030535817169, 0.592132223449203, 0.5909092639715767, 0.5912387387171115, 0.591160910793163, 0.5905455972729019, 0.5913859876128289, 0.5898515772037642, 0.5914172652198963, 0.5921838222584981, 0.5907164346072594, 0.5919652559055117, 0.5926609516036105, 0.5915412833685423, 0.5915394156904167, 0.5920688088150565, 0.5915686503744958, 0.5892230723065105, 0.590730298156328, 0.591971274246207, 0.5912631577683887, 0.5903619286537355, 0.5921993158248512, 0.5902068225465719, 0.5924656111964663, 0.5905889667754946, 0.5911808935087381, 0.592788443441521, 0.5904766564240446, 0.5916002664682158, 0.5903410024966391, 0.5911174692721337, 0.5915942409256769, 0.5931965839254849, 0.5914087310351451, 0.5903466655463798, 0.592073655655848, 0.5911923564432494, 0.5902879849241406, 0.5907995486844633, 0.5908140579988478, 0.5886947066449011, 0.5899172988285001, 0.5924709597657001, 0.5896644829076244, 0.5909232235452276, 0.5915582629153064, 0.5907816785096984, 0.5916415522373727, 0.5916577971960822, 0.5901446754369118, 0.5908445482043404, 0.5906149030151719, 0.59106637699251, 0.5916466607451507, 0.5913610476281927, 0.5910194905895909, 0.5902812367966198, 0.5906482667562896, 0.5905427309391204, 0.5909240685615518, 0.5900334357595544, 0.5922640603994622, 0.5923900470520453, 0.5896494166506625, 0.5919552045323603, 0.5905567817361245, 0.5910871062992125, 0.5910113549068562, 0.5905677045323603, 0.5903230939120415, 0.591066084117534, 0.5911983075667371, 0.5902467423660457, 0.5925808815056655, 0.5926247215287114, 0.591081844152103, 0.5904435183406952, 0.5909309271173421, 0.5910111628576916, 0.591658959093528, 0.5913717087574419, 0.5893621975225658, 0.5919720304397926, 0.5911797340119072, 0.5917493854426733, 0.591742798156328, 0.592457098617246, 0.5909810663529864, 0.5907371543115038, 0.5915628480891109, 0.5909732787593625, 0.5899263899558288, 0.5906783080468601, 0.5898577995966968, 0.5905892812560015, 0.5913739485308239, 0.5897054181870559, 0.5900391420203572, 0.5918797436143652, 0.5905917202803919, 0.5914559175148839, 0.5914782360284233, 0.5918666002496639, 0.5913876776454772, 0.5911210821970425, 0.5926675244862685, 0.591414446898406, 0.5904670899750336, 0.5895828620126752, 0.5911828164009987, 0.592114430094104, 0.5916178653735357, 0.5905210005761475, 0.5905137051085078, 0.5918047100057615, 0.5900443921643941, 0.5887437680046091, 0.590961599769541, 0.5896375528135203, 0.5915577083733435, 0.590345834933743, 0.5908644613020934, 0.5911053725753793, 0.5920369550604955, 0.5919685303437681, 0.5916705276550798, 0.5923448482811599, 0.5912085581908968, 0.5910077083733435, 0.5927318969656232, 0.5903771509506434, 0.5904429950067216, 0.5922689720568466, 0.5908619958709429, 0.5903033056462454, 0.5912089975033609, 0.5905264451699636, 0.5922565752832726, 0.5922172436143652, 0.589337456788938, 0.5918172244094487, 0.5918977098137124, 0.5896244766660265, 0.5896562752064528, 0.591152391012099, 0.5900982979642788, 0.5913992342039563, 0.5900886090839256, 0.5899317961398118, 0.5908641900326483, 0.5903901310735548, 0.5911480194929902, 0.5931159568849625, 0.5911266228154408, 0.5906838774726331, 0.5916445890147879, 0.5915871999231804, 0.5910617414057998, 0.5921722512963318, 0.5914262747263299, 0.5895554613981179, 0.5908564768580757, 0.5906258090071058, 0.5925458613405032, 0.5915739197234492, 0.5906524486268484, 0.5903650302477434, 0.5904123823698867, 0.5917875840215095, 0.5911104810831572, 0.5915711710197811, 0.5935077731899365, 0.590618026214711, 0.5917633666218552, 0.5915322258498176, 0.5907491717879777, 0.5919432950835414, 0.5927266204148262, 0.5917016924332629, 0.5914483819857884, 0.5914492630113309, 0.5924000096024582, 0.5913834309583254, 0.5897857427501442, 0.5905894348953332, 0.591994555406184, 0.5917281712118301, 0.591571567121183, 0.5921743182254657, 0.5901465575187248, 0.5909690104666796, 0.5907872695410025, 0.5909780343768005, 0.5916456740925676, 0.5917466583445362, 0.590724289418091, 0.5896283032456309, 0.5906449419051277, 0.5918511162857693, 0.5897077323794891, 0.5920285865181486, 0.5909270789322066, 0.5910364557326675, 0.5909983219704243, 0.5918327059727291, 0.5910312704052237, 0.5907272733819857, 0.5929490061455732, 0.5912110596312656, 0.5925778567313232, 0.5907013395429229, 0.5913772229690801, 0.5901988429037833, 0.5913894541002496, 0.5913386402919147, 0.5910812680046094, 0.5913870678893796, 0.5917711950259266, 0.5917978274438256, 0.5921003432878817, 0.5925698458805454, 0.5905424884770502, 0.5911803869790667, 0.5910578764163626, 0.5913813736316498, 0.5913344152102938, 0.5920792898982139, 0.590987202323795, 0.5912458925484926, 0.5920510562704051, 0.5924995510850778, 0.5907250864221241, 0.5905292922988286, 0.5912207773189936, 0.5913054782024199, 0.5925251632417899, 0.5905603394468985, 0.5899143172652199, 0.5923736700595352, 0.5917944377760708, 0.5916957893220665, 0.5909236508546188, 0.5926740637603227, 0.5915126344344152, 0.5901454988477051, 0.5908931174380642, 0.5915016348185136, 0.5915574587094296, 0.5906221336662186, 0.5904921067793355, 0.5923366573842903, 0.5914849409448819, 0.593137228730555, 0.5931890387939313, 0.5914667826963702, 0.5908760010562705, 0.5911016492222009, 0.5910045995774919, 0.591552304589975, 0.5914536825427309, 0.5923033200499328, 0.5917603058382945, 0.5901439840599194, 0.5918508018052622, 0.5924167490877664, 0.591587982523526, 0.5922674548684463, 0.5922550845016323, 0.5929316064912619, 0.5919387819281736, 0.5919274558286921, 0.5901350633762243, 0.5903831236796621, 0.5908995630881505, 0.5912228082389092, 0.5912214758978299, 0.5918459141540234, 0.5894999951987709, 0.591283704628385, 0.5910272613789129, 0.589813465047052, 0.5903519180910313, 0.590896980026887, 0.5917108843864031, 0.5907662401574804, 0.5910690536777414, 0.5914731443249472, 0.5894100345688496, 0.5905653183214904, 0.5909030607835606, 0.5896165234299982, 0.5918423252352601, 0.5900764571730364, 0.5912617125984252, 0.5924135418667179, 0.5907782408296524, 0.5921932734780103, 0.5925110260226617, 0.5903616381793738, 0.590628346456693, 0.5913857451507587, 0.5911528063184175, 0.5902470352410217, 0.5894106707317073, 0.5909741165738429, 0.5907371807182639, 0.5908601738044941, 0.5918326579604378, 0.5914354642788555, 0.5902412641636259, 0.5931608147685807, 0.5910897805838294, 0.5903462214326868, 0.5920667010754753, 0.5915275590551181, 0.5902948122719416, 0.5902076267524486, 0.5923443945650086, 0.5912719128096793, 0.590834794507394, 0.5920066857115421, 0.5914438688304205, 0.5917880761474938, 0.5918856323218744, 0.589066230555022, 0.5912582413097754, 0.5920773405991934, 0.5903263011330901, 0.5913766372191281, 0.592531219992318, 0.5918475753792971, 0.5914500384098329, 0.5911696274246206, 0.5935998847705012, 0.5926084957749184, 0.5915739365277511, 0.5918752880737469, 0.5904889451699634, 0.5912049764739773, 0.5904391780295755, 0.5927635202611868, 0.5907452299788746, 0.5916699923180334, 0.5920747335317841, 0.5915456980987133, 0.5896649750336086, 0.5914672268100634, 0.5900466895525255, 0.5933143484732091, 0.5910936599769541, 0.5922225537737661, 0.5927912401574803, 0.5894851329940465, 0.5908105867101978, 0.5919073026694833, 0.5908564480507009, 0.592417337238333, 0.5911806654503552, 0.5909675028807374, 0.5907690656808143, 0.5907764331668908, 0.592030238140964, 0.5921125192049165, 0.5900756097560975, 0.5904101762051086, 0.5910435111388516, 0.5908461230074898, 0.5920875432110619, 0.590618888035337, 0.5909370150758594, 0.5911077467831765, 0.5931547220088342, 0.5909817097176878, 0.5913940296715959, 0.5886069041674669, 0.5914294051277127, 0.5906930190128672, 0.5915716511426925, 0.5901891204148262, 0.5931667322834646, 0.5924378240829653, 0.5907444185711541, 0.5901002256577683, 0.591022028039178, 0.5901844272133666, 0.5911939000384099, 0.5908411297292107, 0.5922902895141157, 0.5920179565968888, 0.5914320001920491, 0.592058562992126, 0.5912873175532937, 0.5913952083733437, 0.5917587238333013, 0.5919691425004802, 0.5922314216439408, 0.5903801397157673, 0.5904707677165355, 0.5911409496831189, 0.5901993014211638, 0.5915161633378145, 0.5922194161705397, 0.5909040354330709, 0.5916686575763396, 0.5920118446322258, 0.5905843191857115, 0.5911155535817169, 0.5918460797964279, 0.5914337238333014, 0.5915415354330709, 0.5906859732091415, 0.5915891180142117, 0.5911964566929134, 0.5915119022469751, 0.5905008306126368, 0.5922994118494336, 0.5895777102938352, 0.5906694233723834, 0.590757960437872, 0.5904870750912233, 0.5917914298060303, 0.5903014307662762, 0.5909345616477819, 0.5919178077587863, 0.5920319929902055, 0.5910793595160361, 0.5895955348569233, 0.5907411081236796, 0.5901596432686766, 0.5919464230843097, 0.5905677333397351, 0.5929830972729019, 0.5919192577299789, 0.5911707437103898, 0.5908752400614558, 0.5909162137507202, 0.5914774582293066, 0.592510048972537, 0.5913340215095064, 0.5908185063376225, 0.5910790762435183, 0.5919565248703669, 0.5907861316497024, 0.5914483603802574, 0.5903393676781256, 0.5917439816593048, 0.5916352770309199, 0.5918654527559055, 0.5920566881121567, 0.591219853082389, 0.5915324683118879, 0.5919101449971192, 0.5926820794123296, 0.591250753792971, 0.5919444257729979, 0.589140471960822, 0.5904931270405224, 0.5905940512771269, 0.5919322210485883, 0.5908368422316113, 0.591454587574419, 0.5907810183406953, 0.5919966631457653, 0.591575878624928, 0.5908360980410985, 0.5910640147877857, 0.5913534472825043, 0.5915799380641446, 0.5908325691376992, 0.5902554469944306, 0.591631361628577, 0.5922005449395046, 0.5912278807374688, 0.5916780727866333, 0.5905427525446514, 0.5912972657000192, 0.59099926781256, 0.5913474865565584, 0.5910899846360669, 0.5936998391588246, 0.5908733435759554, 0.5906226497983483, 0.59051488621087, 0.5920130329364317, 0.5888686767812561, 0.591043866429806, 0.5910991165738428, 0.5922102482235453, 0.5914233219704244, 0.5916931198386787, 0.5917946394276936, 0.5913848569233724, 0.5902227602266181, 0.591087814480507, 0.5904899150182447, 0.5903164178029576, 0.5928820842135586, 0.5891299716727483, 0.591887348761283, 0.5917103250432111, 0.5915796307854811, 0.5922009914538122, 0.5917082797196082, 0.5917062608027656, 0.5922855074899174, 0.5918847873055503, 0.592393340695218, 0.5923265603994622, 0.591542565296716, 0.5907780511811024, 0.590535867582101, 0.5894348425196851, 0.5913348305166123, 0.5930217111580566, 0.590582516324179, 0.5904781616093719, 0.5919792586902247, 0.5907385250624161, 0.5907233651814865, 0.5923417490877665, 0.5904190272709814, 0.592173845304398, 0.590197467351642, 0.5909474361436527, 0.5910543427117342, 0.5902070962166315, 0.5907220736508547, 0.5921677909544844, 0.5930224601497984, 0.5914549812752063, 0.59139707125024, 0.5923126728442482, 0.5902207797196083, 0.590962651238717, 0.5904989437295947, 0.5918422364125216, 0.591248622047244, 0.5891576651622816, 0.5905306678509699, 0.5905225993854427, 0.5906712934511235, 0.5912723041098522, 0.5910382009794508, 0.5903470904551565, 0.5914191521029384, 0.590758313328212, 0.5911435783560592, 0.5924365661609372, 0.5911546139811792, 0.5908434415210294, 0.5915033896677551, 0.5918223953332052, 0.5913173900518532, 0.5910597729018628, 0.5914045299596696, 0.59166672988285, 0.5919528879393124, 0.5914732379489149, 0.5902820001920491, 0.5910820265988093, 0.5922341559439217, 0.5913152150950645, 0.5905537281544077, 0.5912655031688113, 0.5914478730555023, 0.592068215863261, 0.5909442385250624, 0.5914872863453045, 0.5907959477626273, 0.5911647349721528, 0.5919397805838295, 0.5923229378720953, 0.5918574010946802, 0.5931670299596697, 0.5920471840791242, 0.5915165666410601, 0.5913484852122143, 0.5934646965623199, 0.5929283440560783, 0.5901001200307279, 0.5907375192049165, 0.5908793307086615, 0.5913857307470712, 0.5896550748991741, 0.5912437896101402, 0.5920363140964088, 0.5901828644132898, 0.5901846504705205, 0.5913092975801805, 0.5902259842519685, 0.592349577491838, 0.5905967351642021, 0.5903985164202036, 0.5898652487036681, 0.5920668667178797, 0.5904112732859612, 0.5909481851353947, 0.5909578164009986, 0.5906385706740925, 0.5915187487996927, 0.5911055694257729, 0.5900728370462838, 0.5911598737276744, 0.58958109996159, 0.5920260394661033, 0.5912480531015939, 0.5907358699827155, 0.5918171283848664, 0.591998449202996, 0.5909410241021702, 0.5910068849625505, 0.5913388347416939, 0.5915792754945265, 0.5912604354714807, 0.5911615325523334, 0.5915866669867487, 0.5914991117726138, 0.5895093888035338, 0.5908362348761282, 0.5901038961974265, 0.5917853346456694, 0.5895735980410987, 0.590800021605531, 0.5918660721144613, 0.5917803221624736, 0.5912428173612445, 0.5911495270789322, 0.5912164082004993, 0.5933099169387364, 0.5915245414826197, 0.5908806558478971, 0.5910030847897061, 0.5922708973497215, 0.5917013371423085, 0.5906103514499713, 0.592029105050893, 0.5910212646437488, 0.5930227626272326, 0.5915585773958134, 0.5931748751680429, 0.5891325019204917, 0.5911125720184367, 0.5910315128672939, 0.590771127808719, 0.5913357907624351, 0.591508627808719, 0.5920597729018628, 0.5896400158440561, 0.5907519132898023, 0.5910475153639332, 0.591450835413866, 0.5915562920107547, 0.5908322906664105]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-22-output-2.png" width="575" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="chapter-4.2-comparing-sampling-and-bootstrap-distributions" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="chapter-4.2-comparing-sampling-and-bootstrap-distributions"><span class="header-section-number">4.3</span> Chapter 4.2: Comparing sampling and bootstrap distributions</h3>
<section id="coffee-focused-subset" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="coffee-focused-subset">Coffee focused subset</h4>
<p>we took a focused subset of the coffee dataset. Here’s a five hundred row sample from it.</p>
</section>
<section id="the-bootstrap-of-mean-coffee-flavors" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-bootstrap-of-mean-coffee-flavors">The bootstrap of mean coffee flavors</h4>
<p>Here, we generate a bootstrap distribution of the mean coffee flavor scores from that sample. <code>.sample</code> generates a resample, <code>np.mean</code> calculates the statistic, and the for loop with <code>.append</code> repeats these steps to produce a distribution of bootstrap statistics.</p>
</section>
<section id="mean-flavor-bootstrap-distribution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="mean-flavor-bootstrap-distribution">Mean flavor bootstrap distribution</h4>
<p>Observing the histogram of the bootstrap distribution, which is close to a normal distribution.</p>
</section>
<section id="sample-bootstrap-distribution-population-means" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sample-bootstrap-distribution-population-means">Sample, bootstrap distribution, population means</h4>
<p>Here’s the mean flavor score from the original sample. In the bootstrap distribution, each value is an estimate of the mean flavor score. Recall that each of these values corresponds to one potential sample mean from the theoretical population. If we take the mean of those means, we get our best guess of the population mean. The two values are really close. However, there’s a problem. The true population mean is actually a little different.</p>
</section>
<section id="interpreting-the-means" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="interpreting-the-means">Interpreting the means</h4>
<p>The behavior that you just saw is typical. The bootstrap distribution mean is usually almost identical to the original sample mean. However, that is not often a good thing. If the original sample wasn’t closely representative of the population, then the bootstrap distribution mean won’t be a good estimate of the population mean. Bootstrapping cannot correct any potential biases due to differences between the sample and the population.</p>
</section>
<section id="sample-sd-vs.-bootstrap-distribution-sd" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sample-sd-vs.-bootstrap-distribution-sd">Sample <code>sd</code> vs.&nbsp;bootstrap distribution <code>sd</code></h4>
<p>While we do have that limitation in estimating the population mean, one great thing about distributions is that we can also quantify variation. The standard deviation of the sample flavors is around <code>0.354</code>. Recall that pandas <code>.std</code> calculates a sample standard deviation by default. If we calculate the standard deviation of the bootstrap distribution, specifying a <code>ddof</code> of one, then we get a completely different number. So what’s going on here?</p>
</section>
<section id="sample-bootstrap-distn-popn-standard-deviations" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sample-bootstrap-distn-popn-standard-deviations">Sample, bootstrap dist’n, pop’n standard deviations</h4>
<p>Remember that one goal of bootstrapping is to quantify what variability we might expect in our sample statistic as we go from one sample to another. Recall that this quantity is called the standard error as measured by the standard deviation of the sampling distribution of that statistic. The standard deviation of the bootstrap means can be used as a way to estimate this measure of uncertainty. If we multiply that standard error by the square root of the sample size, we get an estimate of the standard deviation in the original population. Our estimate of the standard deviation is around point-three-five-three. The true standard deviation is around point-three-four-one, so our estimate is pretty close. In fact, it is closer than just using the sample standard deviation alone.</p>
</section>
<section id="interpreting-the-standard-errors" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="interpreting-the-standard-errors">Interpreting the standard errors</h4>
<p>To recap, the estimated standard error is the standard deviation of the bootstrap distribution values for our statistic of interest. This estimated standard error times the square root of the sample size gives a really good estimate of the standard deviation of the population. That is, although bootstrapping was poor at estimating the population mean, it is generally great for estimating the population standard deviation.</p>
</section>
</section>
<section id="exercise-4.2.1" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="exercise-4.2.1"><span class="header-section-number">4.4</span> Exercise 4.2.1</h3>
<section id="sampling-distribution-vs.-bootstrap-distribution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sampling-distribution-vs.-bootstrap-distribution">Sampling distribution vs.&nbsp;bootstrap distribution</h4>
<p>The sampling distribution and bootstrap distribution are closely linked. In situations where you can repeatedly sample from a population (these occasions are rare), it’s helpful to generate both the sampling distribution and the bootstrap distribution, one after the other, to see how they are related.</p>
<p>Here, the statistic you are interested in is the mean <code>popularity</code> score of the songs.</p>
</section>
<section id="instructions-21" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-21">Instructions</h4>
<ol type="1">
<li>Generate a sampling distribution of 2000 replicates.</li>
</ol>
<ul>
<li>Sample 500 rows of the population without replacement and calculate the mean <code>popularity</code>.</li>
</ul>
<ol start="2" type="1">
<li>Generate a bootstrap distribution of 2000 replicates.</li>
</ol>
<ul>
<li>Sample 500 rows of the sample with replacement and calculate the mean <code>popularity</code>.</li>
</ul>
<div id="19626f85" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course array</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify.sample(n<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_samp <span class="op">=</span> []</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a sampling distribution of 2000 replicates</span></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_samp.append(</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample 500 rows and calculate the mean popularity </span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>        spotify.sample(n<span class="op">=</span><span class="dv">500</span>)[<span class="st">'popularity'</span>].mean()</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sampling distribution results</span></span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_popularity_2000_samp)</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_boot <span class="op">=</span> []</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a bootstrap distribution of 2000 replicates</span></span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_boot.append(</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resample 500 rows and calculate the mean popularity     </span></span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>        np.mean(spotify_sample.sample(frac<span class="op">=</span><span class="dv">1</span>, replace<span class="op">=</span><span class="va">True</span>)[<span class="st">'popularity'</span>])</span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the bootstrap distribution results</span></span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_popularity_2000_boot)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[55.06, 55.014, 54.164, 54.772, 55.028, 55.338, 55.792, 54.814, 54.58, 53.66, 54.884, 54.228, 55.012, 53.928, 54.882, 55.45, 54.892, 54.936, 54.28, 55.444, 54.26, 54.758, 55.436, 54.688, 54.528, 54.804, 56.13, 54.866, 54.642, 54.534, 53.96, 55.278, 54.802, 55.006, 55.15, 55.05, 54.908, 54.186, 54.888, 55.812, 53.944, 55.078, 54.702, 55.686, 53.992, 54.578, 54.376, 54.326, 54.974, 54.672, 56.162, 54.782, 54.902, 54.28, 54.492, 54.426, 54.332, 55.018, 54.594, 54.766, 54.958, 54.278, 55.396, 54.058, 55.426, 54.196, 53.42, 55.43, 54.874, 55.26, 54.44, 54.742, 55.344, 53.808, 55.294, 54.636, 54.416, 54.508, 53.778, 54.162, 54.33, 55.608, 54.86, 55.152, 55.21, 55.354, 55.416, 55.074, 55.24, 54.89, 54.76, 54.162, 55.264, 55.576, 55.344, 54.434, 54.53, 54.29, 54.096, 55.182, 54.93, 55.21, 54.838, 54.26, 54.504, 55.43, 55.484, 55.294, 55.028, 55.208, 54.282, 56.132, 55.036, 54.68, 55.076, 55.684, 55.114, 55.02, 54.538, 54.382, 54.05, 54.138, 55.17, 55.652, 54.568, 55.148, 54.728, 53.886, 55.624, 55.258, 55.038, 55.03, 54.738, 55.346, 55.588, 55.492, 54.47, 54.944, 54.614, 54.806, 54.508, 54.206, 55.22, 55.342, 55.152, 54.794, 55.058, 54.362, 55.52, 55.448, 54.902, 54.562, 55.008, 54.168, 54.796, 55.148, 55.368, 55.098, 53.936, 55.95, 54.346, 54.878, 55.102, 54.936, 54.602, 54.59, 54.682, 55.526, 54.844, 53.688, 54.886, 54.524, 54.59, 55.142, 54.532, 54.45, 54.338, 54.292, 54.82, 53.948, 55.06, 54.354, 54.31, 54.686, 55.686, 54.868, 55.65, 54.866, 55.092, 55.31, 54.622, 54.196, 54.396, 54.582, 54.494, 54.426, 54.766, 55.67, 54.536, 54.602, 55.046, 54.958, 55.532, 54.034, 55.026, 55.558, 55.296, 53.986, 55.108, 55.272, 54.742, 53.464, 55.448, 54.888, 54.774, 54.768, 54.224, 54.976, 54.686, 54.614, 55.18, 54.402, 54.378, 54.12, 55.096, 55.16, 54.852, 53.958, 55.158, 55.39, 54.318, 55.296, 54.844, 54.982, 54.476, 55.034, 54.44, 54.36, 54.28, 54.046, 55.448, 55.774, 55.574, 54.366, 54.632, 55.166, 54.928, 55.352, 54.364, 54.328, 55.182, 54.616, 55.242, 54.83, 55.382, 54.768, 55.278, 55.408, 54.486, 55.22, 54.906, 55.256, 55.238, 54.38, 54.676, 55.226, 55.446, 54.576, 54.168, 54.634, 54.762, 54.612, 55.25, 54.782, 54.618, 55.104, 54.646, 54.562, 54.064, 54.826, 54.37, 54.168, 54.864, 54.514, 55.448, 54.596, 54.812, 54.826, 55.532, 54.774, 55.7, 54.976, 53.498, 54.602, 54.104, 54.316, 54.616, 54.138, 54.66, 54.782, 54.572, 55.248, 54.924, 54.98, 54.37, 55.188, 54.794, 54.412, 54.872, 55.138, 56.0, 55.924, 55.146, 54.222, 54.776, 55.372, 55.084, 54.992, 54.474, 54.612, 54.882, 54.572, 56.112, 55.524, 54.514, 55.186, 54.328, 55.658, 55.316, 54.816, 54.876, 54.532, 54.792, 54.808, 54.432, 54.146, 55.356, 53.942, 54.674, 55.004, 54.394, 55.392, 54.756, 55.03, 54.142, 55.91, 54.966, 54.404, 54.976, 55.062, 55.096, 54.128, 54.91, 54.912, 54.702, 55.336, 54.914, 55.042, 54.824, 55.702, 55.228, 54.574, 54.848, 54.48, 54.594, 54.406, 55.17, 54.84, 55.222, 53.774, 54.772, 54.666, 55.38, 55.084, 54.546, 54.86, 54.972, 54.72, 54.648, 55.0, 54.766, 55.944, 55.51, 54.34, 54.716, 54.714, 55.382, 54.988, 55.674, 53.912, 54.834, 54.594, 55.752, 55.698, 54.102, 54.298, 54.514, 53.248, 54.654, 55.256, 55.898, 54.586, 55.272, 54.808, 54.71, 55.534, 54.712, 54.206, 55.014, 54.714, 55.656, 55.74, 54.926, 54.514, 55.714, 54.414, 54.468, 54.846, 54.844, 54.068, 54.712, 56.228, 55.612, 54.548, 55.412, 54.646, 53.918, 54.422, 54.484, 55.304, 56.544, 54.824, 54.898, 54.304, 54.74, 54.528, 54.426, 54.212, 54.842, 55.2, 54.576, 54.578, 53.88, 55.072, 54.876, 54.698, 54.78, 54.1, 54.89, 55.48, 53.956, 54.712, 54.864, 54.232, 54.156, 55.798, 55.118, 54.066, 54.802, 54.7, 54.294, 54.936, 54.986, 54.448, 54.858, 54.414, 55.662, 55.362, 54.174, 54.544, 55.004, 55.822, 55.024, 54.9, 54.816, 54.716, 54.892, 55.686, 55.968, 55.212, 55.174, 55.276, 54.764, 54.994, 54.708, 55.02, 54.638, 55.824, 54.288, 54.744, 54.558, 55.034, 54.504, 54.072, 56.076, 54.788, 54.762, 53.77, 54.754, 54.874, 54.574, 55.364, 55.29, 55.316, 54.958, 54.588, 55.42, 54.846, 54.966, 54.492, 55.09, 54.216, 54.144, 55.088, 54.758, 56.362, 55.104, 55.122, 54.346, 55.3, 54.852, 54.518, 54.3, 54.59, 54.308, 54.836, 55.092, 54.742, 55.1, 54.634, 55.118, 54.244, 53.992, 55.462, 54.556, 54.768, 54.64, 53.944, 55.172, 55.004, 55.394, 54.464, 55.014, 55.27, 54.22, 54.704, 54.788, 54.814, 54.402, 54.806, 53.85, 54.834, 54.83, 54.674, 54.342, 54.972, 54.758, 55.192, 54.84, 54.424, 54.82, 54.6, 54.576, 54.716, 54.18, 54.958, 54.632, 53.858, 54.494, 54.956, 55.332, 54.888, 54.694, 55.674, 55.304, 54.62, 55.76, 54.148, 55.596, 54.05, 54.836, 54.728, 54.588, 54.876, 54.694, 56.002, 54.812, 55.014, 54.066, 53.81, 54.836, 55.564, 54.146, 53.996, 54.062, 54.458, 54.344, 54.488, 55.076, 54.976, 54.604, 54.648, 54.086, 55.292, 54.86, 55.438, 54.024, 53.808, 55.692, 54.632, 54.82, 55.36, 54.146, 54.422, 55.424, 55.61, 55.554, 54.608, 54.268, 53.828, 54.824, 54.562, 55.154, 55.38, 54.98, 54.828, 54.88, 54.568, 54.538, 55.088, 55.132, 54.468, 55.086, 54.172, 54.202, 55.656, 55.978, 54.66, 55.356, 55.07, 54.842, 54.692, 55.796, 55.218, 54.492, 54.994, 55.438, 54.582, 55.242, 53.454, 53.902, 55.416, 55.244, 55.432, 54.402, 54.628, 55.206, 55.306, 54.948, 55.014, 55.262, 54.73, 55.238, 54.168, 54.946, 55.438, 54.85, 54.86, 55.988, 55.426, 54.654, 54.354, 54.562, 56.326, 54.474, 54.85, 54.718, 55.396, 54.67, 54.244, 54.924, 55.238, 54.748, 54.106, 54.796, 54.756, 54.51, 54.502, 55.02, 54.98, 54.24, 55.21, 55.534, 55.16, 54.952, 54.982, 54.646, 54.614, 55.33, 54.722, 54.464, 54.69, 55.886, 54.772, 55.006, 54.876, 54.536, 54.994, 55.278, 54.782, 54.886, 55.074, 54.662, 55.66, 55.3, 54.08, 54.134, 54.766, 55.344, 55.03, 54.814, 54.924, 54.746, 54.698, 55.544, 55.022, 55.156, 53.608, 54.44, 55.45, 54.504, 54.912, 54.21, 54.306, 55.276, 55.01, 54.422, 55.898, 53.79, 54.942, 54.388, 55.132, 54.578, 54.196, 55.324, 54.742, 55.092, 54.912, 54.496, 55.65, 54.968, 54.174, 54.656, 55.126, 55.246, 54.98, 55.116, 55.13, 54.482, 54.17, 55.382, 54.064, 54.288, 54.216, 55.286, 54.54, 55.0, 55.404, 54.98, 54.82, 54.776, 54.782, 54.952, 54.958, 54.324, 55.292, 55.434, 54.834, 55.156, 55.434, 54.072, 54.572, 54.616, 55.382, 54.424, 54.92, 55.12, 54.056, 54.81, 55.146, 55.042, 55.132, 55.128, 54.634, 54.59, 55.404, 54.606, 55.424, 54.926, 55.002, 54.238, 54.644, 55.156, 53.612, 54.766, 55.236, 55.096, 55.326, 55.056, 55.044, 55.142, 54.952, 54.818, 54.28, 55.256, 55.168, 54.484, 55.52, 54.278, 54.51, 54.888, 54.918, 55.41, 55.016, 54.972, 55.378, 55.638, 54.396, 54.862, 54.752, 54.192, 54.648, 55.662, 54.396, 54.46, 54.976, 54.84, 55.162, 54.402, 54.756, 54.474, 55.034, 54.662, 55.064, 55.918, 55.346, 55.012, 54.608, 55.35, 54.924, 55.1, 54.86, 54.784, 54.562, 54.592, 54.784, 55.288, 53.952, 54.146, 54.468, 55.486, 55.296, 54.318, 55.126, 55.144, 54.394, 55.784, 54.188, 53.952, 54.838, 53.638, 55.2, 54.706, 54.79, 54.338, 54.954, 54.62, 55.048, 55.35, 54.914, 54.824, 54.092, 55.182, 55.228, 55.074, 55.918, 55.252, 54.906, 55.704, 54.782, 54.914, 55.336, 54.514, 53.88, 54.9, 53.83, 55.116, 55.03, 55.66, 55.028, 54.708, 55.152, 54.372, 55.21, 54.868, 54.758, 54.344, 54.8, 54.718, 54.694, 54.968, 55.322, 54.186, 54.918, 55.112, 55.994, 54.792, 55.308, 54.48, 54.648, 54.452, 54.524, 55.644, 54.724, 54.396, 54.51, 54.736, 54.544, 54.794, 54.676, 55.404, 55.438, 55.346, 54.59, 54.984, 54.822, 54.75, 54.66, 55.032, 55.54, 53.726, 53.854, 54.282, 54.832, 53.938, 55.432, 55.098, 54.724, 55.032, 54.448, 54.844, 54.56, 55.372, 55.392, 54.632, 54.882, 55.196, 54.24, 55.336, 54.836, 55.184, 55.668, 54.836, 54.832, 55.028, 54.932, 54.646, 54.946, 54.344, 55.128, 55.23, 55.884, 55.738, 54.942, 54.778, 54.558, 55.538, 54.564, 53.768, 54.922, 54.06, 54.888, 55.176, 54.86, 55.702, 55.606, 55.348, 54.784, 54.52, 54.986, 54.776, 54.022, 54.834, 54.924, 54.69, 54.89, 54.108, 55.222, 54.07, 54.92, 54.464, 54.458, 54.856, 55.866, 54.874, 55.324, 53.948, 54.372, 55.606, 55.414, 55.05, 54.562, 54.14, 55.688, 54.286, 54.786, 54.722, 54.344, 55.084, 54.87, 54.632, 55.364, 54.566, 55.312, 54.33, 54.94, 54.056, 54.762, 54.502, 55.63, 54.072, 54.68, 54.802, 54.642, 54.6, 54.924, 54.402, 55.606, 54.364, 54.712, 55.432, 54.674, 55.184, 54.67, 54.758, 54.618, 55.056, 54.456, 55.208, 55.208, 55.55, 54.156, 54.906, 54.774, 54.776, 55.236, 55.062, 56.424, 54.564, 54.35, 55.054, 54.814, 53.724, 55.106, 54.41, 55.282, 54.342, 54.836, 54.882, 55.708, 54.368, 54.434, 54.61, 55.824, 53.946, 55.182, 53.9, 55.4, 54.734, 54.832, 55.426, 54.208, 54.834, 54.46, 53.97, 55.174, 55.752, 55.202, 54.924, 55.108, 53.77, 54.722, 54.952, 55.216, 55.038, 54.836, 55.59, 55.7, 53.756, 54.936, 55.484, 55.324, 55.88, 54.838, 55.16, 54.94, 55.192, 54.666, 54.5, 55.43, 53.844, 54.436, 55.068, 55.382, 54.84, 54.492, 54.286, 55.328, 55.242, 54.622, 55.708, 55.672, 54.014, 53.988, 54.834, 54.98, 54.548, 55.078, 54.344, 55.744, 54.674, 54.524, 54.934, 54.744, 56.026, 55.578, 54.754, 54.59, 54.376, 55.042, 54.604, 55.078, 55.382, 55.144, 54.774, 55.142, 54.366, 53.936, 55.166, 54.85, 54.1, 54.144, 54.872, 54.47, 54.0, 53.616, 55.04, 54.422, 54.586, 54.86, 54.614, 55.362, 54.874, 54.37, 54.428, 54.912, 55.132, 54.544, 55.152, 55.128, 54.25, 54.574, 54.346, 55.484, 55.022, 55.098, 54.652, 54.488, 55.638, 55.296, 55.306, 54.998, 54.422, 54.608, 54.638, 55.37, 55.298, 54.856, 55.194, 54.88, 54.71, 54.908, 55.02, 55.338, 55.364, 55.176, 54.97, 54.53, 54.702, 55.116, 55.024, 54.856, 55.376, 54.698, 54.406, 55.614, 54.476, 55.594, 54.526, 54.864, 55.348, 54.83, 54.996, 54.422, 54.652, 55.356, 55.388, 54.378, 54.466, 55.132, 54.75, 55.008, 54.304, 54.734, 54.978, 54.41, 54.974, 54.816, 55.314, 54.854, 54.764, 54.228, 55.9, 55.762, 55.494, 53.688, 54.836, 54.646, 54.72, 56.05, 54.518, 53.998, 55.258, 54.324, 54.77, 54.978, 55.178, 54.898, 54.37, 54.79, 54.78, 55.542, 54.094, 54.992, 54.814, 54.946, 54.776, 56.12, 54.716, 54.87, 55.182, 54.112, 55.08, 54.716, 54.564, 55.018, 54.868, 55.532, 53.982, 54.766, 54.634, 55.874, 55.322, 54.26, 54.722, 54.664, 55.454, 54.576, 55.128, 55.408, 54.538, 54.674, 55.302, 54.16, 55.34, 54.81, 54.956, 55.372, 55.518, 54.824, 54.61, 55.06, 55.096, 54.452, 54.73, 54.836, 55.17, 54.766, 54.284, 55.314, 54.442, 55.66, 54.682, 54.868, 54.778, 55.394, 55.088, 55.554, 54.676, 54.594, 54.846, 54.318, 53.62, 55.01, 54.546, 54.614, 55.584, 54.804, 55.562, 55.682, 54.082, 54.878, 54.968, 55.322, 55.136, 55.202, 53.974, 54.744, 54.2, 54.716, 54.49, 54.734, 54.508, 54.832, 53.782, 55.658, 54.698, 55.244, 54.962, 54.86, 55.436, 54.774, 54.762, 54.88, 54.704, 55.022, 54.53, 54.698, 55.922, 54.642, 55.096, 55.086, 54.442, 54.516, 55.54, 54.146, 55.066, 55.064, 55.622, 54.482, 54.694, 55.046, 54.382, 53.736, 54.162, 54.422, 54.76, 55.73, 55.494, 54.86, 55.616, 54.736, 55.432, 54.97, 54.676, 54.47, 56.108, 55.082, 55.016, 54.304, 55.16, 55.476, 55.036, 56.0, 55.47, 55.056, 55.326, 54.142, 54.596, 55.36, 54.768, 55.49, 54.986, 55.11, 54.334, 54.53, 56.0, 55.248, 55.186, 53.978, 53.538, 54.974, 54.988, 55.06, 54.984, 54.744, 54.258, 54.758, 54.996, 54.302, 55.094, 55.41, 54.878, 54.242, 55.744, 55.274, 55.316, 55.018, 54.772, 54.514, 54.922, 55.316, 55.122, 54.17, 55.034, 53.988, 55.298, 56.298, 54.566, 55.112, 55.318, 55.82, 56.036, 55.186, 55.262, 54.462, 54.356, 54.872, 54.972, 55.556, 55.31, 54.24, 54.522, 54.97, 54.884, 55.278, 55.04, 55.09, 54.862, 55.168, 54.906, 54.62, 54.908, 55.508, 54.452, 55.266, 55.004, 54.918, 54.536, 54.936, 55.064, 53.852, 54.626, 55.08, 54.884, 55.662, 54.246, 55.566, 54.454, 54.772, 54.61, 55.264, 54.288, 54.846, 54.464, 54.914, 54.454, 54.91, 55.602, 55.15, 54.45, 54.072, 54.446, 55.012, 55.628, 54.552, 53.92, 55.296, 55.446, 54.432, 54.942, 54.882, 54.894, 55.452, 55.238, 54.758, 54.346, 54.49, 54.6, 54.336, 54.424, 54.656, 54.508, 54.6, 55.866, 54.534, 54.432, 54.4, 55.286, 55.97, 55.086, 54.548, 54.074, 54.462, 54.716, 54.726, 54.84, 54.984, 54.91, 56.056, 55.074, 54.172, 54.054, 54.286, 54.262, 54.63, 54.598, 55.522, 55.542, 55.162, 54.772, 54.566, 54.432, 54.886, 55.318, 53.066, 54.704, 54.328, 53.93, 53.946, 54.57, 54.346, 54.148, 54.47, 54.32, 55.24, 54.654, 54.33, 55.528, 54.932, 54.88, 54.124, 54.924, 54.97, 54.85, 54.836, 55.338, 55.496, 53.858, 54.446, 53.916, 54.55, 54.82, 54.234, 55.35, 54.544, 54.348, 54.118, 54.504, 55.072, 53.88, 55.162, 54.778, 55.364, 54.61, 55.306, 54.682, 54.608, 54.198, 54.624, 54.764, 54.492, 54.806, 55.118, 54.638, 54.992, 54.908, 55.31, 55.122, 53.71, 55.41, 55.378, 54.924, 54.934, 55.078, 55.324, 54.858, 54.942, 54.16, 54.354, 54.978, 54.878, 55.566, 54.77, 55.212, 54.932, 54.938, 54.374, 54.942, 55.154, 55.384, 55.204, 55.06, 55.586, 54.962, 54.052, 54.61, 54.16, 55.05, 54.572, 54.086, 55.77, 54.964, 54.162, 55.41, 54.646, 54.328, 54.522, 55.654, 55.33, 54.902, 54.744, 54.188, 55.3, 54.796, 55.112, 54.664, 54.6, 54.478, 54.58, 54.914, 54.468, 54.506, 53.732, 54.838, 54.308, 55.244, 55.442, 55.366, 54.614, 55.058, 55.55, 54.226, 54.468, 55.538, 54.652, 53.736, 54.808, 54.318, 54.458, 55.67, 55.176, 55.38, 54.016, 54.724, 54.506, 54.606, 54.584, 55.868, 54.63, 54.09, 54.85, 54.656, 54.136, 55.328, 54.642, 55.704, 54.774, 54.832, 55.232, 54.476, 54.966, 55.244, 55.318, 54.27, 54.778, 54.818, 54.25, 55.242, 54.918, 54.51, 54.712, 55.26, 54.746, 55.182, 54.252, 54.728, 54.874, 54.962, 54.644, 55.34, 54.43, 54.02, 54.426, 55.42, 55.566, 55.506, 54.81, 55.412, 55.298, 54.708, 55.67, 53.988, 55.146, 53.856, 54.192, 55.18, 54.534, 55.038, 54.228, 54.176, 53.978, 54.644, 56.016, 54.236, 55.246, 54.044, 54.966, 54.102, 55.084, 54.828, 54.198, 54.31, 54.076, 54.678, 55.22, 55.326, 54.986, 54.706, 55.054, 54.906, 55.622, 54.012, 54.888, 54.658, 55.154, 55.296, 54.942, 54.262, 55.312, 54.776, 54.39, 53.956, 55.676, 54.884, 54.858, 55.492, 54.494, 55.006, 55.256, 55.868, 55.232, 55.024, 54.986, 55.032, 55.15, 54.846, 54.882, 55.178, 54.274, 54.946, 54.898, 54.914, 55.426, 54.57, 54.462, 54.798, 55.122, 54.806, 55.368, 54.552, 54.578, 55.168, 55.082, 54.698, 54.932, 55.16, 55.038, 55.252, 54.742, 54.878, 54.806, 54.838, 54.88, 54.99, 55.242, 54.284, 54.588, 55.998, 54.584, 55.274, 55.384, 54.998, 53.696, 55.012, 54.19, 55.282, 54.964, 54.368, 55.096, 53.986, 55.4, 55.2, 54.366, 55.19, 55.032, 53.588, 54.75, 55.36, 54.966, 54.602, 55.894, 54.796, 54.782, 54.692, 54.866, 55.222, 54.086, 53.882, 54.286, 54.778, 55.752, 55.056, 55.354, 55.346, 54.812, 54.676, 53.572, 53.844, 54.444, 54.088, 54.418, 54.32, 54.054, 54.656, 55.11, 55.718, 54.644, 54.52, 54.66, 54.714, 54.718, 54.456, 54.664, 55.236, 54.352, 54.828, 54.972, 55.4, 54.66, 54.606, 55.332, 54.464, 54.624, 55.934, 53.606, 54.754, 54.326, 55.554, 54.994, 55.06, 54.674, 55.082, 54.752, 55.452, 54.262, 55.186, 54.08, 55.546, 54.402, 54.454, 53.786, 54.438, 55.706, 55.6, 54.08, 54.27, 54.4, 54.002, 54.854, 54.698, 54.74, 54.704, 54.556, 54.792, 55.436, 54.742, 55.498, 54.612, 55.728, 54.484, 55.236, 55.336, 54.186, 55.186, 55.19, 55.83, 54.718, 55.246, 55.11, 55.044, 54.518, 53.978, 55.064, 54.754, 55.274, 54.726, 55.19, 54.566, 54.56, 54.87, 54.244, 55.038, 54.112, 54.932, 55.164, 55.166, 54.0, 54.526, 55.622, 54.692, 54.242, 54.92, 54.388, 54.96, 54.416, 54.85, 54.934, 55.696, 54.806, 55.22, 54.624, 55.482, 55.88, 54.312, 54.64, 55.062, 54.322, 55.59, 54.804, 54.776, 54.68, 54.682, 54.536, 55.822, 54.642, 56.1, 54.628, 55.288, 54.68, 54.648, 55.408, 55.218, 54.794, 54.444, 54.418, 55.062, 55.562, 55.616, 54.526, 55.226, 54.796, 55.222, 54.858, 54.87, 55.092, 55.534, 55.018, 54.686, 54.26, 54.398, 55.462, 55.51, 55.08, 54.02, 55.076, 54.608, 55.71, 55.136, 54.734, 55.036, 54.884, 55.736, 54.32, 54.674, 54.714, 54.52, 54.496, 55.258, 54.486]
[55.08, 54.902, 55.524, 55.756, 55.474, 55.592, 55.024, 55.196, 55.318, 55.234, 56.428, 54.954, 55.414, 55.602, 55.034, 54.146, 55.664, 54.69, 54.88, 55.318, 55.47, 55.018, 55.068, 55.238, 55.076, 55.108, 55.944, 54.57, 54.91, 55.288, 55.316, 55.864, 55.226, 55.148, 54.164, 55.566, 55.214, 54.498, 54.646, 55.126, 55.984, 55.278, 55.802, 54.438, 55.47, 56.028, 55.114, 55.188, 55.48, 55.712, 55.506, 54.66, 55.08, 55.224, 55.628, 55.646, 55.118, 54.912, 53.924, 55.516, 55.162, 53.852, 54.906, 55.23, 54.754, 54.828, 54.512, 55.258, 55.218, 54.944, 55.65, 55.532, 54.904, 55.06, 55.408, 55.566, 54.578, 55.37, 55.52, 54.236, 54.802, 55.686, 55.354, 54.682, 55.648, 55.318, 55.204, 55.32, 55.464, 55.02, 55.556, 54.382, 55.53, 55.454, 54.734, 55.34, 55.3, 54.818, 54.812, 55.562, 54.87, 53.942, 54.784, 55.32, 54.92, 55.706, 55.092, 55.148, 54.956, 56.062, 55.666, 55.18, 55.182, 55.216, 55.418, 55.798, 54.308, 55.918, 55.402, 54.81, 54.534, 54.43, 55.254, 55.172, 54.598, 56.102, 55.546, 54.91, 55.164, 55.77, 54.632, 55.288, 55.47, 56.17, 55.57, 54.348, 55.02, 55.112, 55.626, 55.504, 54.884, 55.014, 54.01, 55.316, 54.5, 55.086, 55.084, 55.28, 55.116, 56.026, 54.872, 54.534, 55.278, 55.356, 53.76, 54.76, 55.99, 55.138, 54.886, 54.872, 54.898, 55.15, 54.232, 55.014, 54.65, 55.698, 54.99, 54.844, 55.408, 54.958, 55.276, 55.174, 55.194, 54.254, 54.964, 55.358, 55.156, 55.08, 55.504, 55.034, 54.474, 55.308, 55.932, 54.968, 55.534, 55.646, 55.178, 54.72, 54.57, 55.762, 54.67, 55.064, 54.828, 55.762, 54.62, 56.442, 55.398, 56.71, 55.164, 55.284, 55.218, 54.546, 54.788, 54.602, 54.55, 55.194, 55.014, 54.106, 54.26, 55.252, 55.242, 54.842, 54.874, 55.718, 55.292, 54.932, 55.612, 55.61, 55.3, 55.29, 55.538, 54.946, 56.002, 54.86, 54.446, 55.074, 55.01, 55.534, 55.04, 54.69, 55.316, 54.73, 54.75, 54.692, 54.956, 56.456, 56.172, 54.978, 56.084, 54.418, 55.294, 54.934, 55.014, 55.438, 54.944, 54.652, 53.912, 54.822, 55.122, 55.426, 54.692, 55.062, 54.894, 55.51, 55.598, 55.042, 54.442, 55.0, 56.17, 54.806, 55.914, 55.574, 56.158, 54.936, 54.008, 55.004, 55.496, 55.636, 54.878, 55.598, 56.452, 55.414, 55.12, 55.47, 54.536, 55.042, 55.902, 55.502, 56.074, 54.6, 55.124, 54.986, 55.852, 55.542, 54.932, 55.172, 55.47, 55.314, 54.45, 55.072, 54.452, 55.21, 55.058, 56.066, 55.018, 55.282, 54.988, 55.514, 55.218, 55.756, 54.716, 55.052, 55.316, 54.832, 55.284, 56.008, 55.358, 55.32, 55.538, 54.99, 54.64, 55.426, 55.284, 55.044, 55.552, 55.51, 54.616, 56.252, 54.634, 54.646, 54.856, 54.948, 54.928, 55.304, 55.506, 55.36, 54.292, 55.42, 54.948, 54.904, 55.866, 54.558, 54.17, 55.492, 56.148, 55.09, 55.416, 55.296, 56.27, 55.758, 55.554, 55.93, 54.976, 54.638, 55.692, 54.888, 54.936, 53.652, 54.232, 55.052, 55.64, 54.02, 55.756, 55.142, 55.562, 54.858, 55.3, 54.914, 54.992, 54.85, 54.294, 54.698, 54.584, 54.896, 54.204, 54.376, 55.272, 55.024, 55.412, 55.534, 54.51, 55.108, 55.32, 55.572, 55.334, 55.398, 55.77, 55.044, 55.42, 55.066, 55.902, 56.116, 55.706, 55.54, 54.64, 55.47, 54.942, 56.152, 55.578, 54.274, 55.144, 54.756, 54.856, 55.174, 54.948, 55.212, 54.894, 55.782, 55.06, 54.538, 54.828, 55.36, 54.602, 55.4, 54.688, 55.05, 54.494, 55.396, 55.122, 55.22, 55.19, 54.564, 54.976, 55.48, 55.696, 55.128, 55.334, 54.87, 54.58, 55.852, 55.72, 55.382, 55.224, 55.634, 54.858, 54.938, 56.512, 55.494, 54.748, 55.806, 54.166, 54.806, 56.134, 54.504, 55.112, 54.292, 56.79, 54.554, 55.156, 55.63, 56.18, 55.304, 55.74, 55.112, 55.102, 55.55, 55.24, 54.86, 54.982, 54.954, 55.248, 55.38, 55.13, 54.63, 54.954, 54.616, 55.224, 54.966, 54.328, 54.276, 55.33, 55.086, 55.176, 55.588, 54.944, 54.504, 54.448, 55.302, 54.646, 54.694, 55.728, 55.844, 54.972, 54.866, 54.988, 54.482, 55.434, 55.18, 55.888, 53.784, 54.954, 54.914, 55.762, 56.084, 53.952, 55.148, 54.262, 54.318, 55.09, 56.738, 55.654, 53.942, 54.85, 54.892, 54.424, 56.076, 54.982, 54.586, 54.944, 54.798, 54.576, 54.538, 54.726, 55.402, 55.016, 55.4, 55.116, 53.666, 55.366, 53.922, 55.664, 55.2, 55.852, 55.218, 54.98, 54.914, 55.5, 54.948, 54.892, 55.194, 55.906, 55.306, 54.532, 54.904, 55.204, 55.572, 55.482, 54.524, 55.34, 54.21, 55.852, 54.438, 55.04, 54.298, 55.338, 54.834, 55.416, 55.234, 54.75, 54.234, 54.932, 55.972, 55.438, 55.72, 55.13, 54.48, 54.732, 56.164, 55.004, 54.79, 55.414, 54.95, 54.314, 55.446, 54.722, 55.55, 55.57, 55.228, 54.724, 54.73, 55.52, 54.838, 55.78, 55.662, 55.552, 55.142, 55.714, 55.28, 55.224, 54.452, 55.196, 54.588, 55.48, 54.274, 54.988, 56.314, 54.564, 55.17, 55.338, 56.124, 55.358, 55.43, 55.092, 55.552, 55.5, 55.58, 54.72, 55.27, 55.442, 55.026, 54.664, 54.87, 54.72, 55.042, 54.722, 54.292, 55.204, 55.87, 55.074, 56.224, 55.228, 55.242, 55.622, 54.908, 54.778, 55.616, 54.408, 55.272, 54.926, 54.774, 55.076, 56.288, 55.08, 55.986, 54.798, 55.162, 54.862, 55.186, 55.346, 54.616, 55.484, 54.486, 55.054, 55.014, 54.64, 56.146, 54.91, 54.826, 55.384, 55.278, 55.628, 54.624, 53.33, 55.108, 55.244, 55.248, 56.03, 55.414, 54.528, 55.342, 55.332, 55.236, 55.43, 54.688, 55.92, 54.9, 55.112, 55.076, 54.418, 54.672, 55.066, 54.928, 55.61, 55.27, 54.836, 54.884, 55.474, 55.124, 55.836, 55.616, 55.236, 55.914, 54.93, 55.196, 55.204, 56.13, 54.854, 54.786, 55.298, 55.142, 55.018, 55.338, 55.472, 55.488, 55.364, 55.576, 55.608, 54.752, 54.316, 55.58, 55.632, 55.912, 55.684, 55.104, 54.97, 56.058, 55.268, 54.888, 54.576, 55.57, 55.494, 54.888, 55.33, 55.652, 54.394, 54.832, 54.75, 54.844, 55.23, 54.624, 54.29, 55.144, 55.418, 55.122, 56.11, 54.872, 55.284, 54.77, 54.156, 54.846, 55.312, 54.58, 56.416, 55.318, 54.284, 55.518, 54.332, 55.742, 54.396, 55.194, 55.178, 55.034, 54.872, 55.87, 55.498, 54.718, 54.9, 55.14, 55.318, 54.908, 56.258, 55.758, 56.012, 55.596, 54.458, 54.48, 54.704, 54.69, 55.688, 55.776, 55.664, 54.74, 55.69, 55.56, 55.112, 55.714, 54.88, 55.266, 54.652, 55.572, 54.654, 54.892, 55.284, 54.956, 54.878, 54.84, 54.632, 54.658, 55.44, 54.648, 55.586, 54.846, 55.334, 56.046, 54.318, 55.638, 55.408, 54.698, 56.38, 55.47, 54.268, 55.806, 55.654, 55.424, 54.232, 54.558, 55.462, 55.402, 55.676, 54.778, 54.078, 54.454, 56.012, 56.262, 55.288, 55.044, 55.618, 55.268, 54.52, 55.622, 54.65, 54.996, 55.64, 55.804, 55.362, 55.182, 55.198, 54.766, 55.45, 55.394, 55.768, 54.946, 54.272, 55.25, 53.978, 55.444, 55.64, 54.57, 55.166, 56.368, 55.226, 54.904, 55.524, 54.82, 54.226, 54.79, 55.46, 54.702, 55.838, 55.526, 54.986, 54.638, 56.392, 54.73, 55.996, 54.618, 55.228, 54.434, 55.642, 55.614, 55.08, 55.042, 56.212, 54.668, 54.878, 55.21, 55.172, 54.852, 56.39, 54.898, 55.254, 55.314, 55.834, 55.814, 54.992, 54.444, 55.928, 55.208, 54.902, 55.668, 55.088, 55.198, 55.35, 55.83, 55.224, 55.552, 55.788, 55.09, 55.318, 54.96, 55.46, 55.37, 55.836, 55.02, 54.92, 55.212, 55.194, 54.762, 55.634, 54.97, 54.808, 55.552, 54.814, 54.986, 54.804, 55.244, 55.558, 55.56, 55.018, 55.142, 55.096, 54.16, 56.24, 54.794, 55.204, 55.362, 55.118, 54.782, 55.372, 56.22, 55.514, 55.43, 54.712, 55.664, 55.666, 54.984, 55.006, 55.086, 54.94, 54.798, 55.102, 55.114, 54.598, 55.376, 55.19, 54.546, 56.446, 54.434, 54.13, 54.424, 55.312, 53.626, 55.066, 54.968, 55.448, 55.588, 55.01, 54.666, 56.134, 54.674, 55.058, 55.562, 55.292, 55.502, 54.738, 55.328, 54.172, 54.908, 54.806, 54.99, 54.582, 55.274, 54.93, 54.54, 55.572, 54.972, 55.452, 55.294, 55.256, 54.186, 55.29, 55.028, 54.768, 55.264, 55.252, 55.696, 54.574, 55.13, 55.258, 54.456, 54.502, 55.098, 55.768, 54.878, 54.878, 55.228, 54.514, 55.876, 55.312, 55.49, 55.16, 55.49, 55.146, 55.848, 55.346, 55.316, 56.206, 55.756, 55.348, 54.994, 54.792, 54.996, 54.492, 54.334, 55.024, 54.996, 55.424, 54.774, 55.544, 55.734, 55.94, 55.244, 55.838, 56.252, 54.538, 55.334, 54.678, 55.126, 55.926, 55.618, 54.508, 54.906, 55.992, 54.95, 54.502, 55.064, 55.134, 55.258, 55.824, 55.03, 54.584, 54.95, 55.472, 54.428, 55.572, 55.362, 54.528, 55.026, 55.118, 55.354, 54.458, 55.592, 55.604, 54.672, 55.714, 53.94, 54.944, 55.02, 54.008, 54.658, 55.294, 55.31, 55.32, 55.54, 55.486, 54.856, 55.072, 54.948, 54.982, 55.512, 55.044, 54.406, 54.504, 54.982, 55.762, 54.57, 55.138, 54.622, 55.274, 55.544, 55.178, 55.786, 55.066, 55.04, 54.946, 55.434, 54.316, 55.568, 55.554, 54.692, 55.576, 54.792, 55.144, 54.866, 54.464, 54.684, 54.832, 54.326, 54.864, 55.692, 55.474, 56.382, 55.422, 54.398, 54.99, 55.462, 54.418, 54.8, 55.202, 54.862, 56.016, 54.666, 54.602, 55.442, 54.802, 54.718, 54.776, 55.58, 55.626, 55.042, 54.888, 56.066, 55.372, 56.604, 55.684, 55.716, 55.576, 55.49, 53.32, 56.288, 55.63, 54.232, 55.388, 55.69, 55.466, 55.708, 54.772, 55.454, 55.252, 55.938, 54.532, 55.128, 55.224, 56.106, 55.778, 54.4, 55.314, 55.636, 55.078, 54.828, 55.958, 55.8, 55.338, 54.818, 55.2, 56.394, 54.706, 55.174, 54.828, 55.438, 54.906, 54.444, 55.704, 55.488, 54.96, 54.436, 54.434, 54.884, 54.756, 54.372, 54.876, 55.628, 55.592, 54.954, 54.902, 55.186, 55.044, 54.498, 55.426, 55.198, 55.514, 55.13, 54.498, 55.13, 55.054, 55.27, 54.6, 55.902, 55.364, 55.332, 55.042, 55.52, 55.07, 55.644, 54.358, 54.99, 56.252, 54.742, 55.492, 55.644, 55.326, 55.316, 55.0, 55.132, 56.188, 54.95, 55.918, 54.692, 54.86, 55.268, 55.404, 54.636, 54.898, 54.884, 54.772, 54.762, 54.804, 55.434, 55.356, 54.362, 54.642, 54.82, 54.49, 54.914, 55.036, 54.784, 54.576, 54.782, 54.838, 55.012, 54.79, 55.008, 55.97, 54.992, 54.996, 55.52, 55.25, 54.984, 55.254, 55.018, 55.524, 55.184, 55.862, 54.514, 54.94, 55.758, 55.838, 55.386, 55.47, 55.166, 56.364, 55.746, 55.068, 54.616, 55.596, 54.834, 54.764, 54.45, 54.792, 55.33, 55.034, 54.888, 54.498, 54.628, 55.56, 55.32, 54.454, 55.004, 55.17, 55.34, 54.934, 55.332, 54.568, 55.162, 55.078, 55.188, 55.372, 55.382, 55.272, 55.69, 55.508, 55.342, 55.306, 55.226, 54.76, 55.042, 54.018, 55.746, 55.034, 55.548, 55.916, 54.746, 55.338, 54.488, 55.498, 54.766, 55.138, 55.934, 55.466, 55.296, 55.522, 55.246, 55.178, 53.932, 55.05, 54.712, 55.092, 54.472, 54.816, 54.878, 55.482, 54.966, 54.66, 54.51, 55.236, 54.9, 55.738, 55.404, 55.22, 55.36, 55.19, 55.198, 55.768, 54.656, 54.214, 55.878, 54.97, 54.776, 55.116, 55.906, 55.508, 54.364, 53.93, 55.296, 54.978, 55.156, 55.064, 55.016, 54.576, 54.616, 55.306, 54.686, 54.858, 55.608, 55.07, 54.358, 55.078, 55.298, 54.726, 56.02, 54.684, 54.944, 55.386, 54.804, 55.544, 55.064, 55.5, 55.02, 55.964, 54.758, 54.85, 54.668, 55.61, 55.264, 54.996, 55.584, 54.812, 54.86, 55.516, 54.378, 56.168, 54.44, 54.572, 55.23, 55.65, 55.112, 55.506, 54.674, 55.716, 54.924, 55.386, 55.174, 54.87, 55.676, 55.75, 55.386, 55.102, 55.708, 54.528, 55.87, 55.79, 54.346, 54.852, 54.812, 54.844, 55.058, 55.114, 54.602, 55.832, 54.712, 54.842, 54.342, 55.052, 55.294, 55.822, 55.256, 55.492, 54.942, 55.232, 55.044, 55.938, 55.386, 54.934, 56.472, 55.41, 54.464, 55.744, 54.6, 56.17, 55.454, 55.232, 55.264, 54.66, 55.366, 55.394, 55.066, 55.52, 55.09, 55.342, 54.48, 54.546, 54.918, 55.69, 54.426, 55.144, 54.94, 55.806, 54.768, 55.444, 54.774, 55.186, 55.718, 55.584, 55.128, 55.334, 55.588, 54.008, 55.258, 55.098, 55.532, 54.848, 55.552, 54.18, 55.478, 55.9, 55.266, 55.218, 55.286, 55.086, 54.636, 54.84, 55.334, 55.54, 55.228, 55.25, 55.302, 55.698, 55.602, 55.268, 54.872, 55.022, 54.354, 55.138, 55.222, 54.272, 55.174, 54.868, 55.286, 55.216, 54.678, 56.008, 55.28, 54.84, 55.484, 54.924, 54.654, 54.47, 56.72, 55.532, 55.688, 55.982, 55.318, 54.912, 55.786, 54.986, 54.798, 55.65, 55.304, 54.874, 55.69, 55.38, 55.354, 55.93, 54.722, 53.888, 55.192, 55.584, 55.07, 54.212, 55.154, 55.778, 55.008, 54.806, 55.962, 54.948, 55.224, 55.268, 54.748, 54.458, 55.472, 55.694, 54.528, 55.038, 55.448, 55.122, 55.168, 54.956, 53.722, 55.988, 55.668, 54.518, 55.518, 54.942, 55.088, 55.68, 56.254, 54.84, 55.154, 56.152, 55.62, 53.928, 54.99, 55.552, 55.6, 54.626, 54.846, 55.252, 55.654, 55.202, 55.494, 55.87, 54.876, 54.988, 54.916, 55.438, 55.892, 54.894, 55.094, 55.018, 54.77, 55.14, 54.358, 55.238, 54.9, 55.558, 55.526, 55.188, 55.292, 54.954, 55.818, 55.864, 55.176, 54.55, 54.76, 55.69, 54.956, 55.418, 55.166, 54.964, 54.922, 54.866, 54.682, 55.416, 55.034, 55.232, 54.67, 54.526, 55.486, 55.422, 55.304, 55.412, 54.15, 55.536, 54.936, 55.47, 54.738, 54.942, 54.586, 54.872, 55.08, 55.036, 55.236, 54.836, 54.562, 55.112, 54.944, 55.672, 54.546, 54.68, 55.662, 54.882, 54.676, 55.0, 55.984, 54.876, 54.92, 55.45, 55.158, 55.018, 55.284, 54.592, 54.632, 55.288, 55.708, 54.252, 54.96, 55.83, 55.434, 55.35, 55.586, 55.866, 54.598, 54.752, 54.892, 55.26, 55.724, 55.714, 55.354, 54.838, 55.51, 55.184, 55.42, 55.636, 54.378, 55.828, 55.63, 55.668, 55.274, 55.53, 55.294, 55.658, 54.624, 54.268, 55.646, 55.22, 55.052, 56.1, 55.224, 54.944, 54.688, 54.832, 55.314, 55.254, 54.874, 54.758, 55.28, 54.138, 54.604, 54.674, 54.702, 56.25, 55.294, 54.884, 55.534, 55.138, 55.15, 53.902, 54.782, 54.314, 55.474, 55.14, 55.096, 54.718, 54.664, 54.844, 54.44, 54.418, 55.906, 56.014, 55.946, 54.818, 55.43, 54.642, 55.136, 54.144, 55.07, 55.464, 54.934, 54.876, 55.308, 55.732, 55.008, 55.564, 55.884, 54.304, 54.876, 54.7, 55.186, 55.298, 55.02, 55.672, 55.03, 54.554, 54.282, 55.764, 54.886, 54.998, 55.492, 55.45, 55.3, 54.53, 54.004, 54.554, 56.004, 54.96, 55.696, 55.504, 54.998, 55.102, 54.74, 54.886, 55.388, 55.494, 54.868, 54.888, 55.036, 55.708, 55.1, 54.642, 53.534, 54.736, 55.414, 54.848, 55.336, 55.032, 54.094, 55.706, 54.41, 55.032, 55.888, 55.2, 54.812, 55.372, 55.524, 54.868, 54.702, 54.914, 55.202, 54.802, 54.79, 55.31, 55.312, 55.286, 55.036, 55.292, 54.514, 54.33, 55.478, 55.772, 55.438, 55.232, 55.742, 55.394, 54.55, 55.602, 55.02, 53.87, 54.588, 54.84, 55.602, 54.816, 54.936, 55.882, 55.72, 55.494, 55.054, 55.476, 56.304, 55.986, 55.29, 54.984, 55.612, 54.714, 54.682, 55.534, 55.548, 54.394, 54.762, 54.384, 54.814, 54.884, 54.808, 55.41, 55.08, 54.782, 55.436, 55.086, 54.836, 55.62, 55.268, 54.6, 55.294, 55.892, 53.966, 54.732, 53.514, 54.012, 55.308, 55.61, 55.198, 55.47, 54.438, 55.53, 55.094, 54.348, 54.81, 55.142, 54.93, 55.564, 54.74, 54.02, 55.334, 55.394, 54.792, 55.754, 56.41, 54.902, 55.2, 56.056, 54.546, 55.496, 53.712, 55.322, 54.798, 55.864, 55.214, 55.214, 55.552, 54.974, 55.004, 54.886, 54.964, 55.372, 56.002, 55.592, 55.184, 55.032, 55.6, 54.954, 55.238, 54.884, 56.168, 55.714, 54.942, 55.578, 56.11, 55.294, 54.156, 55.168, 54.684, 55.012, 55.244, 55.016, 54.602, 54.552, 55.504, 53.794, 54.266, 55.998, 54.972, 55.128, 54.658, 55.528, 55.058, 55.518, 55.416, 54.986, 55.096, 55.114, 56.522, 54.482, 54.8, 55.39, 55.272, 54.822, 54.604, 54.658, 54.932, 56.008, 55.096, 55.454, 55.598, 54.782, 55.006, 54.422, 55.484, 54.78, 55.408, 54.804, 55.658, 55.026, 54.712, 56.246, 54.534, 55.032, 55.602, 54.76, 55.554, 54.716, 54.678, 54.994, 54.698, 54.456, 54.158, 54.206, 55.404, 55.674, 55.45, 54.714, 54.778, 55.242, 55.542, 55.074, 55.108, 54.664, 54.162, 55.718, 55.592, 55.278, 54.974, 56.054, 55.36, 55.704, 54.032, 55.474, 55.13, 54.244, 54.716, 54.434, 54.112, 55.596, 54.922, 55.178, 54.2, 55.058, 55.23, 53.666, 55.138, 54.65, 54.958, 55.242, 55.162, 54.44, 54.878, 54.038, 55.272, 54.58, 55.016, 55.27, 55.316, 54.814, 55.298, 54.934, 54.226, 54.496, 55.064, 55.228, 55.318, 54.982, 54.978, 55.318, 54.512, 55.916, 54.606, 55.184, 54.846, 54.582, 56.034, 55.568, 54.87, 54.676, 55.596, 54.83, 54.008, 55.034, 54.082, 55.918, 55.636, 55.214, 55.334, 55.068, 55.388, 55.536, 55.112, 55.192, 54.958, 55.24, 54.782, 55.674, 54.61, 55.318, 55.208, 56.012, 55.04, 55.71, 55.528, 55.584, 55.334, 54.494, 55.478, 54.776, 54.448, 55.98, 55.302, 54.576, 54.53, 55.528, 54.824, 56.358, 55.164, 54.65, 55.438, 54.996, 54.368, 54.518, 54.888, 55.07, 55.834, 54.56, 54.878]</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>The sampling distribution and bootstrap distribution are closely related, and so is the code to generate them.</em></p>
</div>
</div>
</section>
</section>
<section id="exercise-4.2.2" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="exercise-4.2.2"><span class="header-section-number">4.5</span> Exercise 4.2.2</h3>
<section id="compare-sampling-and-bootstrap-means" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="compare-sampling-and-bootstrap-means">Compare sampling and bootstrap means</h4>
<p>To make calculation easier, distributions similar to those calculated from the previous exercise have been included, this time using a sample size of 5000.</p>
<p>spotify_population, spotify_sample, sampling_distribution, and bootstrap_distribution are available; pandas and numpy are loaded with their usual aliases.</p>
</section>
<section id="instructions-22" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-22">Instructions</h4>
<ol type="1">
<li>Calculate the mean <code>popularity</code> in 4 ways:</li>
</ol>
<ul>
<li>Population: from <code>spotify</code>, take the mean of <code>popularity</code>.</li>
<li>Sample: from <code>spotify_sample</code>, take the mean of <code>popularity</code>.</li>
<li>Sampling distribution: from <code>sampling_distribution</code>, take its mean.</li>
<li>Bootstrap distribution: from `<code>bootstrap_distribution</code>, take its mean.</li>
</ul>
<div id="baa5082a" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course array</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify.sample(n<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_samp <span class="op">=</span> []</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a sampling distribution of 2000 replicates</span></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_samp.append(</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample 500 rows and calculate the mean popularity </span></span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>        spotify.sample(n<span class="op">=</span><span class="dv">500</span>)[<span class="st">'popularity'</span>].mean()</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a><span class="co"># The sampling distribution results</span></span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>sampling_distribution <span class="op">=</span> mean_popularity_2000_samp </span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_boot <span class="op">=</span> []</span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a bootstrap distribution of 2000 replicates</span></span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_boot.append(</span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resample 500 rows and calculate the mean popularity     </span></span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a>        np.mean(spotify_sample.sample(frac<span class="op">=</span><span class="dv">1</span>, replace<span class="op">=</span><span class="va">True</span>)[<span class="st">'popularity'</span>])</span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a><span class="co"># The bootstrap distribution results</span></span>
<span id="cb47-34"><a href="#cb47-34" aria-hidden="true" tabindex="-1"></a>bootstrap_distribution <span class="op">=</span> mean_popularity_2000_boot</span>
<span id="cb47-35"><a href="#cb47-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-36"><a href="#cb47-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the population mean popularity</span></span>
<span id="cb47-37"><a href="#cb47-37" aria-hidden="true" tabindex="-1"></a>pop_mean <span class="op">=</span> spotify[<span class="st">'popularity'</span>].mean()</span>
<span id="cb47-38"><a href="#cb47-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-39"><a href="#cb47-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the original sample mean popularity</span></span>
<span id="cb47-40"><a href="#cb47-40" aria-hidden="true" tabindex="-1"></a>samp_mean <span class="op">=</span> spotify_sample[<span class="st">'popularity'</span>].mean()</span>
<span id="cb47-41"><a href="#cb47-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-42"><a href="#cb47-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the sampling dist'n estimate of mean popularity</span></span>
<span id="cb47-43"><a href="#cb47-43" aria-hidden="true" tabindex="-1"></a>samp_distn_mean <span class="op">=</span> np.mean(sampling_distribution)</span>
<span id="cb47-44"><a href="#cb47-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-45"><a href="#cb47-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the bootstrap dist'n estimate of mean popularity</span></span>
<span id="cb47-46"><a href="#cb47-46" aria-hidden="true" tabindex="-1"></a>boot_distn_mean <span class="op">=</span> np.mean(bootstrap_distribution)</span>
<span id="cb47-47"><a href="#cb47-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-48"><a href="#cb47-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the means</span></span>
<span id="cb47-49"><a href="#cb47-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([pop_mean, samp_mean, samp_distn_mean, boot_distn_mean])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[54.837142308430955, 55.266, 54.833191, 55.25912]</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>The sampling distribution mean can be used to estimate the population mean, but that is not the case with the bootstrap distribution.</em></p>
</div>
</div>
</section>
</section>
<section id="exercise-4.2.3" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="exercise-4.2.3"><span class="header-section-number">4.6</span> Exercise 4.2.3</h3>
<section id="compare-sampling-and-bootstrap-standard-deviations" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="compare-sampling-and-bootstrap-standard-deviations">Compare sampling and bootstrap standard deviations</h4>
<p>In the same way that you looked at how the sampling distribution and bootstrap distribution could be used to estimate the population mean, you’ll now take a look at how they can be used to estimate variation, or more specifically, the standard deviation, in the population.</p>
<p>Recall that the sample size is 5000.</p>
</section>
<section id="instructions-23" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="instructions-23">Instructions</h4>
<p>Calculate the standard deviation of <code>popularity</code> in 4 ways. - Population: from <code>spotify</code>, take the standard deviation of <code>popularity</code>. - Original sample: from <code>spotify_sample</code>, take the standard deviation of <code>popularity</code>. - Sampling distribution: from <code>sampling_distribution</code>, take its standard deviation and multiply by the square root of the sample size (5000). - Bootstrap distribution: from <code>bootstrap_distribution</code>, take its standard deviation and multiply by the square root of the sample size.</p>
<div id="ef15d504" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course array</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify.sample(n<span class="op">=</span><span class="dv">5000</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_samp <span class="op">=</span> []</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a sampling distribution of 2000 replicates</span></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_samp.append(</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample 500 rows and calculate the mean popularity </span></span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>        spotify.sample(n<span class="op">=</span><span class="dv">5000</span>)[<span class="st">'popularity'</span>].mean()</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a><span class="co"># The sampling distribution results</span></span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>sampling_distribution <span class="op">=</span> mean_popularity_2000_samp </span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_boot <span class="op">=</span> []</span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a bootstrap distribution of 2000 replicates</span></span>
<span id="cb49-27"><a href="#cb49-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb49-28"><a href="#cb49-28" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_boot.append(</span>
<span id="cb49-29"><a href="#cb49-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resample 500 rows and calculate the mean popularity     </span></span>
<span id="cb49-30"><a href="#cb49-30" aria-hidden="true" tabindex="-1"></a>        np.mean(spotify_sample.sample(frac<span class="op">=</span><span class="dv">1</span>, replace<span class="op">=</span><span class="va">True</span>)[<span class="st">'popularity'</span>])</span>
<span id="cb49-31"><a href="#cb49-31" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb49-32"><a href="#cb49-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-33"><a href="#cb49-33" aria-hidden="true" tabindex="-1"></a><span class="co"># The bootstrap distribution results</span></span>
<span id="cb49-34"><a href="#cb49-34" aria-hidden="true" tabindex="-1"></a>bootstrap_distribution <span class="op">=</span> mean_popularity_2000_boot</span>
<span id="cb49-35"><a href="#cb49-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-36"><a href="#cb49-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the population std dev popularity</span></span>
<span id="cb49-37"><a href="#cb49-37" aria-hidden="true" tabindex="-1"></a>pop_sd <span class="op">=</span> spotify[<span class="st">'popularity'</span>].std(ddof<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb49-38"><a href="#cb49-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-39"><a href="#cb49-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the original sample std dev popularity</span></span>
<span id="cb49-40"><a href="#cb49-40" aria-hidden="true" tabindex="-1"></a>samp_sd <span class="op">=</span> spotify_sample[<span class="st">'popularity'</span>].std(ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb49-41"><a href="#cb49-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-42"><a href="#cb49-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the sampling dist'n estimate of std dev popularity</span></span>
<span id="cb49-43"><a href="#cb49-43" aria-hidden="true" tabindex="-1"></a>samp_distn_sd <span class="op">=</span> np.std(sampling_distribution, ddof<span class="op">=</span><span class="dv">1</span>) <span class="op">*</span> np.sqrt(<span class="dv">5000</span>)</span>
<span id="cb49-44"><a href="#cb49-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-45"><a href="#cb49-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the bootstrap dist'n estimate of std dev popularity</span></span>
<span id="cb49-46"><a href="#cb49-46" aria-hidden="true" tabindex="-1"></a>boot_distn_sd <span class="op">=</span> np.std(bootstrap_distribution, ddof<span class="op">=</span><span class="dv">1</span>) <span class="op">*</span> np.sqrt(<span class="dv">5000</span>)</span>
<span id="cb49-47"><a href="#cb49-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-48"><a href="#cb49-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the standard deviations</span></span>
<span id="cb49-49"><a href="#cb49-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([pop_sd, samp_sd, samp_distn_sd, boot_distn_sd])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[10.880065274257536, 10.975581356685552, 10.32099231645863, 10.648136574361413]</code></pre>
</div>
</div>
</section>
</section>
<section id="chapter-4.3-confidence-intervals" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="chapter-4.3-confidence-intervals"><span class="header-section-number">4.7</span> Chapter 4.3: Confidence intervals</h3>
<p>In the last few exercises, you looked at relationships between the sampling distribution and the bootstrap distribution.</p>
<p>One way to quantify these distributions is the idea of “values within one standard deviation of the mean”, which gives a good sense of where most of the values in a distribution lie. In this final lesson, we’ll formalize the idea of values close to a statistic by defining the term “confidence interval”.</p>
<section id="predicting-the-weather" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="predicting-the-weather">Predicting the weather</h4>
<p>Consider meteorologists predicting weather in one of the world’s most unpredictable regions - the northern Great Plains of the US and Canada. Rapid City, South Dakota was ranked as the least predictable of the 120 US cities with a National Weather Service forecast office. Suppose we’ve taken a job as a meteorologist at a news station in Rapid City. Our job is to predict tomorrow’s high temperature.</p>
</section>
<section id="our-weather-prediction" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="our-weather-prediction">Our weather prediction</h4>
<p>We analyze the weather data using the best forecasting tools available to us and predict a high temperature of 47 degrees Fahrenheit. In this case, 47 degrees is our point estimate. Since the weather is variable, and many South Dakotans will plan their day tomorrow based on our forecast, we’d instead like to present a range of plausible values for the high temperature. On our weather show, we report that the high temperature will be between forty and fifty-four degrees tomorrow.</p>
</section>
<section id="we-just-reported-a-confidence-interval" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="we-just-reported-a-confidence-interval">We just reported a confidence interval!</h4>
<p>This prediction of forty to fifty-four degrees can be thought of as a confidence interval for the unknown quantity of tomorrow’s high temperature. Although we can’t be sure of the exact temperature, we are confident that it will be in that range. These results are often written as the point estimate followed by the confidence interval’s lower and upper bounds in parentheses or square brackets. When the confidence interval is symmetric around the point estimate, we can represent it as the point estimate plus or minus the margin of error, in this case, seven degrees.</p>
</section>
<section id="bootstrap-distribution-of-mean-flavor" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bootstrap-distribution-of-mean-flavor">Bootstrap distribution of mean flavor</h4>
<p>Here’s the bootstrap distribution of the mean flavor from the coffee dataset.</p>
</section>
<section id="mean-of-the-resamples" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="mean-of-the-resamples">Mean of the resamples</h4>
<p>We can calculate the mean of these resampled mean flavors.</p>
</section>
<section id="mean-plus-or-minus-one-standard-deviation" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="mean-plus-or-minus-one-standard-deviation">Mean plus or minus one standard deviation</h4>
<p>If we create a confidence interval by adding and subtracting one standard deviation from the mean, we see that there are lots of values in the bootstrap distribution outside of this one standard deviation confidence interval.</p>
</section>
<section id="quantile-method-for-confidence-intervals" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="quantile-method-for-confidence-intervals">Quantile method for confidence intervals</h4>
<p>If we want to include ninety-five percent of the values in the confidence interval, we can use quantiles. Recall that quantiles split distributions into sections containing a particular proportion of the total data. To get the middle ninety-five percent of values, we go from the point-zero-two-five quantile to the point-nine-seven-five quantile since the difference between those two numbers is point-nine-five. To calculate the lower and upper bounds for this confidence interval, we call quantile from NumPy, passing the distribution values and the quantile values to use. The confidence interval is from around seven-point-four-eight to seven-point-five-four.</p>
</section>
<section id="inverse-cumulative-distribution-function" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="inverse-cumulative-distribution-function">Inverse cumulative distribution function</h4>
<p>There is a second method to calculate confidence intervals. To understand it, we need to be familiar with the normal distribution’s inverse cumulative distribution function. The bell curve we’ve seen before is the probability density function or PDF. Using calculus, if we integrate this, we get the cumulative distribution function or CDF. If we flip the x and y axes, we get the inverse <code>CDF</code>. We can use <code>scipy.stats</code> and call <code>norm.ppf</code> to get the inverse <code>CDF</code>. It takes a quantile between zero and one and returns the values of the normal distribution for that quantile. The parameters of <code>loc</code> and <code>scale</code> are set to 0 and 1 by default, corresponding to the standard normal distribution. Notice that the values corresponding to point-zero-two-five and point-nine-seven-five are about minus and plus two for the standard normal distribution.</p>
</section>
<section id="standard-error-method-for-confidence-interval" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="standard-error-method-for-confidence-interval">Standard error method for confidence interval</h4>
<p>This second method for calculating a confidence interval is called the standard error method. First, we calculate the point estimate, which is the mean of the bootstrap distribution, and the standard error, which is estimated by the standard deviation of the bootstrap distribution. Then we call <code>norm.ppf</code> to get the inverse <code>CDF</code> of the normal distribution with the same mean and standard deviation as the bootstrap distribution. Again, the confidence interval is from seven-point-four-eight to seven-point-five-four, though the numbers differ slightly from last time since our bootstrap distribution isn’t perfectly normal.</p>
</section>
</section>
<section id="exercise-4.3.1" class="level3" data-number="4.8">
<h3 data-number="4.8" class="anchored" data-anchor-id="exercise-4.3.1"><span class="header-section-number">4.8</span> Exercise 4.3.1</h3>
<section id="calculating-confidence-intervals" class="level4" data-number="4.8.1">
<h4 data-number="4.8.1" class="anchored" data-anchor-id="calculating-confidence-intervals"><span class="header-section-number">4.8.1</span> Calculating confidence intervals</h4>
<p>You have learned about two methods for calculating confidence intervals: <em>the quantile method</em> and <em>the standard error method</em>. The standard error method involves using the inverse cumulative distribution function (inverse CDF) of the normal distribution to calculate confidence intervals. In this exercise, you’ll perform these two methods on the Spotify data.</p>
</section>
<section id="instructions-24" class="level4" data-number="4.8.2">
<h4 data-number="4.8.2" class="anchored" data-anchor-id="instructions-24"><span class="header-section-number">4.8.2</span> Instructions</h4>
<ol type="1">
<li><p>Generate a 95% confidence interval using the quantile method on the bootstrap distribution, setting the <code>0.025</code> quantile as <code>lower_quant</code> and the <code>0.975</code> quantile as <code>upper_quant</code>.</p></li>
<li><p>Generate a 95% confidence interval using the standard error method from the bootstrap distribution.</p></li>
</ol>
<ul>
<li>Calculate <code>point_estimate</code> as the mean of <code>bootstrap_distribution</code>, and <code>standard_error</code> as the standard deviation of <code>bootstrap_distribution</code>.</li>
<li>Calculate <code>lower_se</code> as the <code>0.025</code> quantile of an inv. CDF from a normal distribution with mean <code>point_estimate</code> and standard deviation <code>standard_error</code>.</li>
<li>Calculate <code>upper_se</code> as the <code>0.975</code> quantile of that same inv. CDF.</li>
</ul>
<div id="2128f827" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course array</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify.sample(n<span class="op">=</span><span class="dv">5000</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_boot <span class="op">=</span> []</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a bootstrap distribution of 2000 replicates</span></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_boot.append(</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resample 500 rows and calculate the mean popularity     </span></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>        np.mean(spotify_sample.sample(frac<span class="op">=</span><span class="dv">1</span>, replace<span class="op">=</span><span class="va">True</span>)[<span class="st">'popularity'</span>])</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a><span class="co"># The bootstrap distribution results</span></span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>bootstrap_distribution <span class="op">=</span> mean_popularity_2000_boot</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a 95% confidence interval using the quantile method</span></span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>lower_quant <span class="op">=</span> np.quantile(bootstrap_distribution, <span class="fl">0.025</span>)</span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>upper_quant <span class="op">=</span> np.quantile(bootstrap_distribution, <span class="fl">0.975</span>)</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Print quantile method confidence interval</span></span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>((lower_quant, upper_quant))</span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the mean and std dev of the bootstrap distribution</span></span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a>point_estimate <span class="op">=</span> np.mean(bootstrap_distribution)</span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a>standard_error <span class="op">=</span> np.std(bootstrap_distribution, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the lower limit of the confidence interval</span></span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a>lower_se <span class="op">=</span> norm.ppf(<span class="fl">0.025</span>, loc<span class="op">=</span>point_estimate, scale<span class="op">=</span>standard_error)</span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the upper limit of the confidence interval</span></span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true" tabindex="-1"></a>upper_se <span class="op">=</span> norm.ppf(<span class="fl">0.975</span>, loc<span class="op">=</span>point_estimate, scale<span class="op">=</span>standard_error)</span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Print standard error method confidence interval</span></span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>((lower_se, upper_se))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(54.47574, 55.07474499999999)
(54.48036899601746, 55.079173603982525)</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="reference" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="reference"><span class="header-section-number">5</span> Reference</h2>
<p>Sampling in Python in Intermediate Python Course for Associate Data Scientist in Python Carrer Track in DataCamp Inc by James Chapman.</p>
<!-- -->

</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb53" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "COURSE 19 | SAMPLING AND POINT IN PYTHON"</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="co">  - name:  "Lawal's Note"</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: "Associate Data Science Course in Python by DataCamp Inc"</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-12-26"</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a><span class="an">highlight-style:</span><span class="co"> pygments</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a><span class="co">  pdf:</span></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a><span class="co">    geometry:</span></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a><span class="co">      - top=30mm</span></span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a><span class="co">      - left=20mm</span></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a><span class="co">  docx: default</span></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a><span class="co">  warning: false</span></span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true   </span></span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a><span class="co">  eval: true  </span></span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a><span class="co">  output: true </span></span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a><span class="co">  error: false   </span></span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a><span class="co">  cache: false</span></span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a><span class="co">  include_metadata: false</span></span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a><span class="al">![](Sampling.jpg)</span></span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter 1: Introduction to Sampling</span></span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>Learn what sampling is and why it is so powerful. You’ll also learn about the problems caused by convenience sampling and the differences between true randomness and pseudo-randomness.</span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 1.1: Sampling and point estimates</span></span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a>Hi! Welcome to the course! I’m James, and I’ll be your host as we delve into the world of sampling data with Python. To start, let’s look at what sampling is and why it might be useful.</span>
<span id="cb53-39"><a href="#cb53-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-40"><a href="#cb53-40" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Estimating the population of France {.unnumbered}</span></span>
<span id="cb53-41"><a href="#cb53-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-42"><a href="#cb53-42" aria-hidden="true" tabindex="-1"></a>Let's consider the problem of counting how many people live in France. The standard approach is to take a census. This means contacting every household and asking how many people live there. There are lots of people in France. Since there are millions of people in France, this is a really expensive process. Even with modern data collection technology, most countries will only conduct a census every five or ten years due to the cost.</span>
<span id="cb53-43"><a href="#cb53-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-44"><a href="#cb53-44" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Sampling households {.unnumbered}</span></span>
<span id="cb53-45"><a href="#cb53-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-46"><a href="#cb53-46" aria-hidden="true" tabindex="-1"></a>In 1786, Pierre-Simon Laplace realized you could estimate the population with less effort. Rather than asking every household who lived there, he asked a small number of households and used statistics to estimate the number of people in the whole population. This technique of working with a subset of the whole population is called sampling.</span>
<span id="cb53-47"><a href="#cb53-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-48"><a href="#cb53-48" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Population vs. sample {.unnumbered}</span></span>
<span id="cb53-49"><a href="#cb53-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-50"><a href="#cb53-50" aria-hidden="true" tabindex="-1"></a>Two definitions are important for this course. The population is the complete set of data that we are interested in. The previous example involved the literal population of France, but in statistics, it doesn't have to refer to people. One thing to bear in mind is that there is usually no equivalent of the census, so typically, we won't know what the whole population is like - more on this in a moment. </span>
<span id="cb53-51"><a href="#cb53-51" aria-hidden="true" tabindex="-1"></a>The sample is the subset of data that we are working with.</span>
<span id="cb53-52"><a href="#cb53-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-53"><a href="#cb53-53" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Coffee rating dataset {.unnumbered}</span></span>
<span id="cb53-54"><a href="#cb53-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-55"><a href="#cb53-55" aria-hidden="true" tabindex="-1"></a>Picture a dataset of professional ratings of coffees. Each row corresponds to one coffee, and there are thirteen hundred and thirty-eight rows in the dataset. The coffee is given a score from zero to one hundred, which is stored in the total_cup_points column. Other columns contain contextual information like the variety and country of origin and scores between zero and ten for attributes of the coffee such as aroma and body. These scores are averaged across all the reviewers for that particular coffee. It doesn't contain every coffee in the world, so we don't know exactly what the population of coffees is. However, there are enough here that we can think of it as our population of interest.</span>
<span id="cb53-56"><a href="#cb53-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-57"><a href="#cb53-57" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Points vs. flavor: population {.unnumbered}</span></span>
<span id="cb53-58"><a href="#cb53-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-59"><a href="#cb53-59" aria-hidden="true" tabindex="-1"></a>Let's consider the relationship between cup points and flavor by selecting those two columns. This dataset contains all thirteen hundred and thirty-eight rows from the original dataset.</span>
<span id="cb53-60"><a href="#cb53-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-61"><a href="#cb53-61" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Points vs. flavor: 10 row sample {.unnumbered}</span></span>
<span id="cb53-62"><a href="#cb53-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-63"><a href="#cb53-63" aria-hidden="true" tabindex="-1"></a>The pandas <span class="in">`.sample`</span> method returns a random subset of rows. Setting n to ten means ten random rows are returned. By default, rows from the original dataset can't appear in the sample dataset multiple times, so we are guaranteed to have ten unique rows in our sample.</span>
<span id="cb53-64"><a href="#cb53-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-65"><a href="#cb53-65" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Python sampling for Series {.unnumbered}</span></span>
<span id="cb53-66"><a href="#cb53-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-67"><a href="#cb53-67" aria-hidden="true" tabindex="-1"></a>The <span class="in">`.sample`</span> method also works on pandas Series. Here, using square-bracket subsetting retrieves the <span class="in">`total_cup_points`</span> column as a Series, and the <span class="in">`n`</span> argument specifies how many random values to return.</span>
<span id="cb53-68"><a href="#cb53-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-69"><a href="#cb53-69" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Population parameters &amp; point estimates {.unnumbered}</span></span>
<span id="cb53-70"><a href="#cb53-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-71"><a href="#cb53-71" aria-hidden="true" tabindex="-1"></a>A population parameter is a calculation made on the population dataset. We aren't limited to counting values either; here, we calculate the mean of the cup points using NumPy. By contrast, a point estimate, or sample statistic, is a calculation based on the sample dataset. Here, the mean of the total cup points is calculated on the sample. Notice that the means are very similar but not identical.</span>
<span id="cb53-72"><a href="#cb53-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-73"><a href="#cb53-73" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Point estimates with pandas {.unnumbered}</span></span>
<span id="cb53-74"><a href="#cb53-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-75"><a href="#cb53-75" aria-hidden="true" tabindex="-1"></a>Working with pandas can be easier than working with NumPy. These mean calculations can be performed using the <span class="in">`.mean`</span> pandas method.</span>
<span id="cb53-76"><a href="#cb53-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-77"><a href="#cb53-77" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.1.1</span></span>
<span id="cb53-78"><a href="#cb53-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-79"><a href="#cb53-79" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Simple sampling with pandas {.unnumbered}</span></span>
<span id="cb53-80"><a href="#cb53-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-81"><a href="#cb53-81" aria-hidden="true" tabindex="-1"></a>Throughout this chapter, you'll be exploring song data from Spotify. Each row of this population dataset represents a song, and there are over 40,000 rows. Columns include the song name, the artists who performed it, the release year, and attributes of the song like its duration, tempo, and danceability. You'll start by looking at the durations.</span>
<span id="cb53-82"><a href="#cb53-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-83"><a href="#cb53-83" aria-hidden="true" tabindex="-1"></a>Your first task is to sample the Spotify dataset and compare the mean duration of the population with the sample.</span>
<span id="cb53-84"><a href="#cb53-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-85"><a href="#cb53-85" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-86"><a href="#cb53-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-87"><a href="#cb53-87" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sample 1000 rows from <span class="in">`spotify`</span>, assigning to <span class="in">`spotify_sample`</span>.</span>
<span id="cb53-88"><a href="#cb53-88" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Calculate the mean duration in minutes from <span class="in">`spotify`</span> using pandas.</span>
<span id="cb53-89"><a href="#cb53-89" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Calculate the mean duration in minutes from <span class="in">`spotify_sample`</span> using pandas.</span>
<span id="cb53-90"><a href="#cb53-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-93"><a href="#cb53-93" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-94"><a href="#cb53-94" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb53-95"><a href="#cb53-95" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-96"><a href="#cb53-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-97"><a href="#cb53-97" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-98"><a href="#cb53-98" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb53-99"><a href="#cb53-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-100"><a href="#cb53-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-101"><a href="#cb53-101" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 1000 rows from spotify_population</span></span>
<span id="cb53-102"><a href="#cb53-102" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify.sample(n<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb53-103"><a href="#cb53-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-104"><a href="#cb53-104" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb53-105"><a href="#cb53-105" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(spotify_sample)</span>
<span id="cb53-106"><a href="#cb53-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-107"><a href="#cb53-107" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean duration in mins from spotify_population</span></span>
<span id="cb53-108"><a href="#cb53-108" aria-hidden="true" tabindex="-1"></a>mean_dur_pop <span class="op">=</span> spotify[<span class="st">'duration_minutes'</span>].mean()</span>
<span id="cb53-109"><a href="#cb53-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-110"><a href="#cb53-110" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean duration in mins from spotify_sample</span></span>
<span id="cb53-111"><a href="#cb53-111" aria-hidden="true" tabindex="-1"></a>mean_dur_samp <span class="op">=</span> spotify_sample[<span class="st">'duration_minutes'</span>].mean()</span>
<span id="cb53-112"><a href="#cb53-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-113"><a href="#cb53-113" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the means</span></span>
<span id="cb53-114"><a href="#cb53-114" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_dur_pop)</span>
<span id="cb53-115"><a href="#cb53-115" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_dur_samp)</span>
<span id="cb53-116"><a href="#cb53-116" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-117"><a href="#cb53-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-118"><a href="#cb53-118" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb53-119"><a href="#cb53-119" aria-hidden="true" tabindex="-1"></a>*Notice that the mean song duration in the sample is similar, but not identical to the mean song duration in the whole population.*</span>
<span id="cb53-120"><a href="#cb53-120" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-121"><a href="#cb53-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-122"><a href="#cb53-122" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.1.2</span></span>
<span id="cb53-123"><a href="#cb53-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-124"><a href="#cb53-124" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Simple sampling and calculating with NumPy {.unnumbered}</span></span>
<span id="cb53-125"><a href="#cb53-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-126"><a href="#cb53-126" aria-hidden="true" tabindex="-1"></a>You can also use numpy to calculate parameters or statistics from a list or pandas Series.</span>
<span id="cb53-127"><a href="#cb53-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-128"><a href="#cb53-128" aria-hidden="true" tabindex="-1"></a>You'll be turning it up to eleven and looking at the loudness property of each song.</span>
<span id="cb53-129"><a href="#cb53-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-130"><a href="#cb53-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-131"><a href="#cb53-131" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-132"><a href="#cb53-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-133"><a href="#cb53-133" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Create a pandas Series, <span class="in">`loudness_pop`</span>, by subsetting the <span class="in">`loudness`</span> column from <span class="in">`spotify`</span>.</span>
<span id="cb53-134"><a href="#cb53-134" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Sample <span class="in">`loudness_pop`</span> to get 100 random values, assigning to <span class="in">`loudness_samp`</span>.</span>
<span id="cb53-135"><a href="#cb53-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-136"><a href="#cb53-136" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Calculate the mean of <span class="in">`loudness_pop`</span> using <span class="in">`numpy`</span>.</span>
<span id="cb53-137"><a href="#cb53-137" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Calculate the mean of <span class="in">`loudness_samp`</span> using <span class="in">`numpy`</span>.</span>
<span id="cb53-138"><a href="#cb53-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-141"><a href="#cb53-141" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-142"><a href="#cb53-142" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb53-143"><a href="#cb53-143" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-144"><a href="#cb53-144" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-145"><a href="#cb53-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-146"><a href="#cb53-146" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-147"><a href="#cb53-147" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb53-148"><a href="#cb53-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-149"><a href="#cb53-149" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a pandas Series from the loudness column of spotify_population</span></span>
<span id="cb53-150"><a href="#cb53-150" aria-hidden="true" tabindex="-1"></a>loudness_pop <span class="op">=</span> spotify[<span class="st">'loudness'</span>]</span>
<span id="cb53-151"><a href="#cb53-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-152"><a href="#cb53-152" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 100 values of loudness_pop</span></span>
<span id="cb53-153"><a href="#cb53-153" aria-hidden="true" tabindex="-1"></a>loudness_samp <span class="op">=</span> loudness_pop.sample(n<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb53-154"><a href="#cb53-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-155"><a href="#cb53-155" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loudness_samp)</span>
<span id="cb53-156"><a href="#cb53-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-157"><a href="#cb53-157" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean of loudness_pop</span></span>
<span id="cb53-158"><a href="#cb53-158" aria-hidden="true" tabindex="-1"></a>mean_loudness_pop <span class="op">=</span> np.mean(loudness_pop)</span>
<span id="cb53-159"><a href="#cb53-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-160"><a href="#cb53-160" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean of loudness_samp</span></span>
<span id="cb53-161"><a href="#cb53-161" aria-hidden="true" tabindex="-1"></a>mean_loudness_samp <span class="op">=</span> np.mean(loudness_samp)</span>
<span id="cb53-162"><a href="#cb53-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-163"><a href="#cb53-163" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_loudness_pop)</span>
<span id="cb53-164"><a href="#cb53-164" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_loudness_samp)</span>
<span id="cb53-165"><a href="#cb53-165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-166"><a href="#cb53-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-167"><a href="#cb53-167" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb53-168"><a href="#cb53-168" aria-hidden="true" tabindex="-1"></a>*Again, notice that the calculated value (the mean) is close but not identical in each case.*</span>
<span id="cb53-169"><a href="#cb53-169" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-170"><a href="#cb53-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-171"><a href="#cb53-171" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 1.2: Convenience sampling</span></span>
<span id="cb53-172"><a href="#cb53-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-173"><a href="#cb53-173" aria-hidden="true" tabindex="-1"></a>The point estimates you calculated in the previous exercises were very close to the population parameters that they were based on, but is this always the case?</span>
<span id="cb53-174"><a href="#cb53-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-175"><a href="#cb53-175" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Literary Digest election prediction {.unnumbered}</span></span>
<span id="cb53-176"><a href="#cb53-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-177"><a href="#cb53-177" aria-hidden="true" tabindex="-1"></a>In 1936, a newspaper called The Literary Digest ran an extensive poll to try to predict the next US presidential election. They phoned ten million voters and had over two million responses. About one-point-three million people said they would vote for Landon, and just under one million people said they would vote for Roosevelt. That is, Landon was predicted to get fifty-seven percent of the vote, and Roosevelt was predicted to get forty-three percent of the vote. Since the sample size was so large, it was presumed that this poll would be very accurate. However, in the election, Roosevelt won by a landslide with sixty-two percent of the vote. So what went wrong? Well, in 1936, telephones were a luxury, so the only people who had been contacted by The Literary Digest were relatively rich. The sample of voters was not representative of the whole population of voters, and so the poll suffered from sample bias. The data was collected by the easiest method, in this case, telephoning people. This is called convenience sampling and is often prone to sample bias. Before sampling, we need to think about our data collection process to avoid biased results.</span>
<span id="cb53-178"><a href="#cb53-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-179"><a href="#cb53-179" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Finding the mean age of French people {.unnumbered}</span></span>
<span id="cb53-180"><a href="#cb53-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-181"><a href="#cb53-181" aria-hidden="true" tabindex="-1"></a>Let's look at another example. While on vacation at Disneyland Paris, you start wondering about the mean age of French people. To get an answer, you ask ten people stood nearby about their ages. Their mean age is twenty-four-point-six years old. Do you think this will be a good estimate of the mean age of all French citizens?</span>
<span id="cb53-182"><a href="#cb53-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-183"><a href="#cb53-183" aria-hidden="true" tabindex="-1"></a><span class="fu">#### How accurate was the survey? {.unnumbered}</span></span>
<span id="cb53-184"><a href="#cb53-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-185"><a href="#cb53-185" aria-hidden="true" tabindex="-1"></a>On the left, you can see mean ages taken from the French census. Notice that the population has been gradually getting older as birth rates decrease and life expectancy increases. In 2015, the mean age was over forty, so our estimate of twenty-four-point-six is way off. The problem is that the family-friendly fun at Disneyland means that the sample ages weren't representative of the general population. There are generally more eight-year-olds than eighty-year-olds riding rollercoasters.</span>
<span id="cb53-186"><a href="#cb53-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-187"><a href="#cb53-187" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Convenience sampling coffee ratings {.unnumbered}</span></span>
<span id="cb53-188"><a href="#cb53-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-189"><a href="#cb53-189" aria-hidden="true" tabindex="-1"></a>Let's return to the coffee ratings dataset and look at the mean cup points population parameter. The mean is about eighty-two. One form of convenience sampling would be to take the first ten rows, rather than the random rows we saw in the previous video. We can take the first 10 rows with the pandas <span class="in">`head`</span> method. The mean cup points from this sample is higher at eighty-nine. The discrepancy suggests that coffees with higher cup points appear near the start of the dataset. Again, the convenience sample isn't representative of the whole population.</span>
<span id="cb53-190"><a href="#cb53-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-191"><a href="#cb53-191" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing selection bias {.unnumbered}</span></span>
<span id="cb53-192"><a href="#cb53-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-193"><a href="#cb53-193" aria-hidden="true" tabindex="-1"></a>Histograms are a great way to visualize the selection bias. We can create a histogram of the total cup points from the population, which contains values ranging from around 59 to around 91. The <span class="in">`np.arange`</span> function can be used to create bins of width 2 from 59 to 91. Recall that the stop value in <span class="in">`np.arange`</span> is exclusive, so we specify 93, not 91. Here's the same code to generate a histogram for the convenience sample.</span>
<span id="cb53-194"><a href="#cb53-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-195"><a href="#cb53-195" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Distribution of a population and of a convenience sample {.unnumbered}</span></span>
<span id="cb53-196"><a href="#cb53-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-197"><a href="#cb53-197" aria-hidden="true" tabindex="-1"></a>Comparing the two histograms, it is clear that the distribution of the sample is not the same as the population: all of the sample values are on the right-hand side of the plot.</span>
<span id="cb53-198"><a href="#cb53-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-199"><a href="#cb53-199" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing selection bias for a random sample {.unnumbered}</span></span>
<span id="cb53-200"><a href="#cb53-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-201"><a href="#cb53-201" aria-hidden="true" tabindex="-1"></a>This time, we'll compare the <span class="in">`total_cup_points`</span> distribution of the population with a random sample of 10 coffees.</span>
<span id="cb53-202"><a href="#cb53-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-203"><a href="#cb53-203" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Distribution of a population and of a simple random sample {.unnumbered}</span></span>
<span id="cb53-204"><a href="#cb53-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-205"><a href="#cb53-205" aria-hidden="true" tabindex="-1"></a>Notice how the shape of the distributions is more closely aligned when random sampling is used.</span>
<span id="cb53-206"><a href="#cb53-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-207"><a href="#cb53-207" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.2.1</span></span>
<span id="cb53-208"><a href="#cb53-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-209"><a href="#cb53-209" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Are findings from the sample generalizable? {.unnumbered}</span></span>
<span id="cb53-210"><a href="#cb53-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-211"><a href="#cb53-211" aria-hidden="true" tabindex="-1"></a>You just saw how convenience sampling—collecting data using the easiest method—can result in samples that aren't representative of the population. Equivalently, this means findings from the sample are not generalizable to the population. Visualizing the distributions of the population and the sample can help determine whether or not the sample is representative of the population.</span>
<span id="cb53-212"><a href="#cb53-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-213"><a href="#cb53-213" aria-hidden="true" tabindex="-1"></a>The Spotify dataset contains an <span class="in">`acousticness`</span> column, which is a confidence measure from zero to one of whether the track was made with instruments that aren't plugged in. You'll compare the <span class="in">`acousticness`</span> distribution of the total population of songs with a sample of those songs.</span>
<span id="cb53-214"><a href="#cb53-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-215"><a href="#cb53-215" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-216"><a href="#cb53-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-217"><a href="#cb53-217" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Plot a histogram of the <span class="in">`acousticness`</span> from <span class="in">`spotify`</span> with bins of width <span class="in">`0.01`</span> from <span class="in">`0`</span> to <span class="in">`1`</span> using pandas <span class="in">`.hist()`</span>.</span>
<span id="cb53-218"><a href="#cb53-218" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Update the histogram code to use the <span class="in">`spotify_mysterious_sample`</span> dataset.</span>
<span id="cb53-219"><a href="#cb53-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-222"><a href="#cb53-222" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-223"><a href="#cb53-223" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb53-224"><a href="#cb53-224" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-225"><a href="#cb53-225" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-226"><a href="#cb53-226" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-227"><a href="#cb53-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-228"><a href="#cb53-228" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-229"><a href="#cb53-229" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb53-230"><a href="#cb53-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-231"><a href="#cb53-231" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the distribution of acousticness with a histogram</span></span>
<span id="cb53-232"><a href="#cb53-232" aria-hidden="true" tabindex="-1"></a>spotify[<span class="st">'acousticness'</span>].hist(bins<span class="op">=</span>np.arange(<span class="dv">0</span>,<span class="fl">1.01</span>,<span class="fl">0.01</span>))</span>
<span id="cb53-233"><a href="#cb53-233" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-234"><a href="#cb53-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-235"><a href="#cb53-235" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a convenience sample where acousticness is consistently higher</span></span>
<span id="cb53-236"><a href="#cb53-236" aria-hidden="true" tabindex="-1"></a>spotify_high_acousticness <span class="op">=</span> spotify[(spotify[<span class="st">'acousticness'</span>] <span class="op">&gt;=</span> <span class="fl">0.85</span>) <span class="op">&amp;</span> (spotify[<span class="st">'acousticness'</span>] <span class="op">&lt;=</span> <span class="fl">1.0</span>)]</span>
<span id="cb53-237"><a href="#cb53-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-238"><a href="#cb53-238" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 1107 entries from the high acousticness subset</span></span>
<span id="cb53-239"><a href="#cb53-239" aria-hidden="true" tabindex="-1"></a>spotify_mysterious_sample <span class="op">=</span> spotify_high_acousticness.sample(n<span class="op">=</span><span class="dv">1107</span>)</span>
<span id="cb53-240"><a href="#cb53-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-241"><a href="#cb53-241" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the histogram to use spotify_mysterious_sample</span></span>
<span id="cb53-242"><a href="#cb53-242" aria-hidden="true" tabindex="-1"></a>spotify_mysterious_sample[<span class="st">'acousticness'</span>].hist(bins<span class="op">=</span>np.arange(<span class="dv">0</span>, <span class="fl">1.01</span>, <span class="fl">0.01</span>))</span>
<span id="cb53-243"><a href="#cb53-243" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-244"><a href="#cb53-244" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-245"><a href="#cb53-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-246"><a href="#cb53-246" aria-hidden="true" tabindex="-1"></a><span class="fu">### Question </span></span>
<span id="cb53-247"><a href="#cb53-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-248"><a href="#cb53-248" aria-hidden="true" tabindex="-1"></a>*Compare the two histograms you drew*. Are the <span class="in">`acousticness`</span> values in the sample generalizable to the general population?</span>
<span id="cb53-249"><a href="#cb53-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-250"><a href="#cb53-250" aria-hidden="true" tabindex="-1"></a>**No. The acousticness samples are consistently higher than those in the general population.**</span>
<span id="cb53-251"><a href="#cb53-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-252"><a href="#cb53-252" aria-hidden="true" tabindex="-1"></a>**The acousticness values in the sample are all greater than 0.85, whereas they range from 0 to 1 in the whole population.**</span>
<span id="cb53-253"><a href="#cb53-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-254"><a href="#cb53-254" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.2.2</span></span>
<span id="cb53-255"><a href="#cb53-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-256"><a href="#cb53-256" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Are these findings generalizable? {.unnumbered}</span></span>
<span id="cb53-257"><a href="#cb53-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-258"><a href="#cb53-258" aria-hidden="true" tabindex="-1"></a>Let's look at another sample to see if it is representative of the population. This time, you'll look at the <span class="in">`duration_minutes`</span> column of the <span class="in">`Spotify`</span> dataset, which contains the length of the song in minutes.</span>
<span id="cb53-259"><a href="#cb53-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-260"><a href="#cb53-260" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-261"><a href="#cb53-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-262"><a href="#cb53-262" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Plot a histogram of <span class="in">`duration_minutes`</span> from <span class="in">`spotify`</span> with bins of width <span class="in">`0.5`</span> from <span class="in">`0`</span> to <span class="in">`15`</span> using pandas <span class="in">`.hist()`</span>.</span>
<span id="cb53-263"><a href="#cb53-263" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Update the histogram code to use the <span class="in">`spotify_mysterious_sample2`</span> dataset.</span>
<span id="cb53-264"><a href="#cb53-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-267"><a href="#cb53-267" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-268"><a href="#cb53-268" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb53-269"><a href="#cb53-269" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-270"><a href="#cb53-270" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-271"><a href="#cb53-271" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-272"><a href="#cb53-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-273"><a href="#cb53-273" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-274"><a href="#cb53-274" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb53-275"><a href="#cb53-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-276"><a href="#cb53-276" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a convenience sample where duration_minutes is within the specified range</span></span>
<span id="cb53-277"><a href="#cb53-277" aria-hidden="true" tabindex="-1"></a>spotify_duration_range <span class="op">=</span> spotify[(spotify[<span class="st">'duration_minutes'</span>] <span class="op">&gt;=</span> <span class="fl">0.8079999999</span>) <span class="op">&amp;</span> (spotify[<span class="st">'duration_minutes'</span>] <span class="op">&lt;=</span> <span class="fl">9.822</span>)]</span>
<span id="cb53-278"><a href="#cb53-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-279"><a href="#cb53-279" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 50 entries from the spotify_mysterious_sample2 dataset</span></span>
<span id="cb53-280"><a href="#cb53-280" aria-hidden="true" tabindex="-1"></a>spotify_mysterious_sample2 <span class="op">=</span> spotify_duration_range.sample(n<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb53-281"><a href="#cb53-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-282"><a href="#cb53-282" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the distribution of duration_minutes in the population with a histogram</span></span>
<span id="cb53-283"><a href="#cb53-283" aria-hidden="true" tabindex="-1"></a>spotify[<span class="st">'duration_minutes'</span>].hist(bins<span class="op">=</span>np.arange(<span class="dv">0</span>,<span class="fl">15.5</span>,<span class="fl">0.5</span>))</span>
<span id="cb53-284"><a href="#cb53-284" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-285"><a href="#cb53-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-286"><a href="#cb53-286" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the distribution of duration_minutes as a histogram</span></span>
<span id="cb53-287"><a href="#cb53-287" aria-hidden="true" tabindex="-1"></a>spotify_mysterious_sample2[<span class="st">'duration_minutes'</span>].hist(bins<span class="op">=</span>np.arange(<span class="dv">0</span>, <span class="fl">15.5</span>, <span class="fl">0.5</span>))</span>
<span id="cb53-288"><a href="#cb53-288" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-289"><a href="#cb53-289" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-290"><a href="#cb53-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-291"><a href="#cb53-291" aria-hidden="true" tabindex="-1"></a><span class="fu">### Question</span></span>
<span id="cb53-292"><a href="#cb53-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-293"><a href="#cb53-293" aria-hidden="true" tabindex="-1"></a>*Compare the two histograms you drew*. Are the duration values in the sample generalizable to the general population?</span>
<span id="cb53-294"><a href="#cb53-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-295"><a href="#cb53-295" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Answer</span></span>
<span id="cb53-296"><a href="#cb53-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-297"><a href="#cb53-297" aria-hidden="true" tabindex="-1"></a>Yes. The sample selected is likely a random sample of all songs in the population.</span>
<span id="cb53-298"><a href="#cb53-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-299"><a href="#cb53-299" aria-hidden="true" tabindex="-1"></a>*The duration values in the sample show a similar distribution to those in the whole population, so the results are generalizable.*</span>
<span id="cb53-300"><a href="#cb53-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-301"><a href="#cb53-301" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 1.3: Pseudo-random number generation</span></span>
<span id="cb53-302"><a href="#cb53-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-303"><a href="#cb53-303" aria-hidden="true" tabindex="-1"></a>You previously saw how to use a random sample to get results similar to those in the population. But how does a computer actually do this random sampling?</span>
<span id="cb53-304"><a href="#cb53-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-305"><a href="#cb53-305" aria-hidden="true" tabindex="-1"></a><span class="fu">#### What does random mean? {.unnumbered}</span></span>
<span id="cb53-306"><a href="#cb53-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-307"><a href="#cb53-307" aria-hidden="true" tabindex="-1"></a>There are several meanings of random in English. This definition from Oxford Languages is the most interesting for us. If we want to choose data points at random from a population, we shouldn't be able to predict which data points would be selected ahead of time in some systematic way.</span>
<span id="cb53-308"><a href="#cb53-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-309"><a href="#cb53-309" aria-hidden="true" tabindex="-1"></a><span class="fu">#### True random numbers {.unnumbered}</span></span>
<span id="cb53-310"><a href="#cb53-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-311"><a href="#cb53-311" aria-hidden="true" tabindex="-1"></a>To generate truly random numbers, we typically have to use a physical process like flipping coins or rolling dice. The Hotbits service generates numbers from radioactive decay, and RANDOM.ORG generates numbers from atmospheric noise, which are radio signals generated by lightning. Unfortunately, these processes are fairly slow and expensive for generating random numbers.</span>
<span id="cb53-312"><a href="#cb53-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-313"><a href="#cb53-313" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">https://www.fourmilab.ch/hotbits</span><span class="co">](https://www.fourmilab.ch/hotbits)</span></span>
<span id="cb53-314"><a href="#cb53-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-315"><a href="#cb53-315" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">https://www.random.org</span><span class="co">](https://www.random.org)</span></span>
<span id="cb53-316"><a href="#cb53-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-317"><a href="#cb53-317" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Pseudo-random number generation {.unnumbered}</span></span>
<span id="cb53-318"><a href="#cb53-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-319"><a href="#cb53-319" aria-hidden="true" tabindex="-1"></a>For most use cases, pseudo-random number generation is better since it is cheap and fast. Pseudo-random means that although each value appears to be random, it is actually calculated from the previous random number. Since you have to start the calculations somewhere, the first random number is calculated from what is known as a seed value. The word random is in quotes to emphasize that this process isn't really random. If we start from a particular seed value, all future numbers will be the same.</span>
<span id="cb53-320"><a href="#cb53-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-321"><a href="#cb53-321" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Pseudo-random number generation example {.unnumbered}</span></span>
<span id="cb53-322"><a href="#cb53-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-323"><a href="#cb53-323" aria-hidden="true" tabindex="-1"></a>For example, suppose we have a function to generate pseudo-random values called <span class="in">`calc_next_random`</span>. To begin, we pick a seed number, in this case, one. <span class="in">`calc_next_random`</span> does some calculations and returns three. We then feed three into <span class="in">`calc_next_random`</span>, and it does the same set of calculations and returns two. And if we can keep feeding in the last number, it will return something apparently random. Although the process is deterministic, the trick to a random number generator is to make it look like the values are random.</span>
<span id="cb53-324"><a href="#cb53-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-325"><a href="#cb53-325" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Random number generating functions {.unnumbered}</span></span>
<span id="cb53-326"><a href="#cb53-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-327"><a href="#cb53-327" aria-hidden="true" tabindex="-1"></a>NumPy has many functions for generating random numbers from statistical distributions. To use each of these, make sure to prepend each function name with <span class="in">`numpy.random`</span> or <span class="in">`np.random`</span>. Some of them, like <span class="in">`.uniform`</span> and <span class="in">`.normal`</span>, may be familiar. Others have more niche applications.</span>
<span id="cb53-328"><a href="#cb53-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-329"><a href="#cb53-329" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing random numbers {.unnumbered}</span></span>
<span id="cb53-330"><a href="#cb53-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-331"><a href="#cb53-331" aria-hidden="true" tabindex="-1"></a>Let's generate some pseudo-random numbers. The first arguments to each random number function specify distribution parameters. The size argument specifies how many numbers to generate, in this case, five thousand. We've chosen the beta distribution, and its parameters are named a and b. These random numbers come from a continuous distribution, so a great way to visualize them is with a histogram. Here, because the numbers were generated from the beta distribution, all the values are between zero and one.</span>
<span id="cb53-332"><a href="#cb53-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-333"><a href="#cb53-333" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Random numbers seeds {.unnumbered}</span></span>
<span id="cb53-334"><a href="#cb53-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-335"><a href="#cb53-335" aria-hidden="true" tabindex="-1"></a>To set a random seed with NumPy, we use the <span class="in">`.random.seed`</span> method. <span class="in">`Random.seed`</span> takes an integer for the seed number, which can be any number you like. <span class="in">`.normal`</span> generates pseudo-random numbers from the normal distribution. The loc and scale arguments set the mean and standard deviation of the distribution, and the size argument determines how many random numbers from that distribution will be returned. If we call <span class="in">`.normal`</span> a second time, we get two different random numbers. If we reset the seed by calling <span class="in">`random.seed`</span> with the same seed number, then call <span class="in">`.normal`</span> again, we get the same numbers as before. This makes our code reproducible.</span>
<span id="cb53-336"><a href="#cb53-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-337"><a href="#cb53-337" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Using a different seed {.unnumbered}</span></span>
<span id="cb53-338"><a href="#cb53-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-339"><a href="#cb53-339" aria-hidden="true" tabindex="-1"></a>Now let's try a different seed. This time, calling <span class="in">`.normal`</span> generates different numbers.</span>
<span id="cb53-340"><a href="#cb53-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-341"><a href="#cb53-341" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.3.1</span></span>
<span id="cb53-342"><a href="#cb53-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-343"><a href="#cb53-343" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Generating random numbers {.unnumbered}</span></span>
<span id="cb53-344"><a href="#cb53-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-345"><a href="#cb53-345" aria-hidden="true" tabindex="-1"></a>You've used <span class="in">`.sample()`</span> to generate pseudo-random numbers from a set of values in a DataFrame. A related task is to generate random numbers that follow a statistical distribution, like the uniform distribution or the normal distribution.</span>
<span id="cb53-346"><a href="#cb53-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-347"><a href="#cb53-347" aria-hidden="true" tabindex="-1"></a>Each random number generation function has distribution-specific arguments and an argument for specifying the number of random numbers to generate.</span>
<span id="cb53-348"><a href="#cb53-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-349"><a href="#cb53-349" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-350"><a href="#cb53-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-351"><a href="#cb53-351" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Generate 5000 numbers from a uniform distribution, setting the parameters <span class="in">`low`</span> to <span class="in">`-3`</span> and <span class="in">`high`</span> to <span class="in">`3`</span>.</span>
<span id="cb53-352"><a href="#cb53-352" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Generate 5000 numbers from a normal distribution, setting the parameters <span class="in">`loc`</span> to <span class="in">`5`</span> and <span class="in">`scale`</span> to <span class="in">`2`</span>.</span>
<span id="cb53-353"><a href="#cb53-353" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Plot a histogram of <span class="in">`uniforms`</span> with <span class="in">`bins`</span> of width of <span class="in">`0.25`</span> from <span class="in">`-3`</span> to <span class="in">`3`</span> using <span class="in">`plt.hist()`</span>.</span>
<span id="cb53-354"><a href="#cb53-354" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Plot a histogram of <span class="in">`normals`</span> with <span class="in">`bins`</span> of width of <span class="in">`0.5`</span> from <span class="in">`-2`</span> to <span class="in">`13`</span> using <span class="in">`plt.hist()`</span>.</span>
<span id="cb53-355"><a href="#cb53-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-358"><a href="#cb53-358" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-359"><a href="#cb53-359" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-360"><a href="#cb53-360" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-361"><a href="#cb53-361" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-362"><a href="#cb53-362" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-363"><a href="#cb53-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-364"><a href="#cb53-364" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random numbers from a Uniform(-3, 3)</span></span>
<span id="cb53-365"><a href="#cb53-365" aria-hidden="true" tabindex="-1"></a>uniforms <span class="op">=</span> np.random.uniform(low<span class="op">=-</span><span class="dv">3</span>, high<span class="op">=</span><span class="dv">3</span>, size<span class="op">=</span><span class="dv">5000</span>)</span>
<span id="cb53-366"><a href="#cb53-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-367"><a href="#cb53-367" aria-hidden="true" tabindex="-1"></a><span class="co"># Print uniforms</span></span>
<span id="cb53-368"><a href="#cb53-368" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(uniforms)</span>
<span id="cb53-369"><a href="#cb53-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-370"><a href="#cb53-370" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random numbers from a Normal(5, 2)</span></span>
<span id="cb53-371"><a href="#cb53-371" aria-hidden="true" tabindex="-1"></a>normals <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">5</span>, scale <span class="op">=</span> <span class="dv">2</span>, size<span class="op">=</span> <span class="dv">5000</span>)</span>
<span id="cb53-372"><a href="#cb53-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-373"><a href="#cb53-373" aria-hidden="true" tabindex="-1"></a><span class="co"># Print normals</span></span>
<span id="cb53-374"><a href="#cb53-374" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(normals)</span>
<span id="cb53-375"><a href="#cb53-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-376"><a href="#cb53-376" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot a histogram of uniform values, binwidth 0.25</span></span>
<span id="cb53-377"><a href="#cb53-377" aria-hidden="true" tabindex="-1"></a>plt.hist(uniforms, bins<span class="op">=</span>np.arange(<span class="op">-</span><span class="dv">3</span>,<span class="fl">3.25</span>,<span class="fl">0.25</span>))</span>
<span id="cb53-378"><a href="#cb53-378" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-379"><a href="#cb53-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-380"><a href="#cb53-380" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot a histogram of normal values, binwidth 0.5</span></span>
<span id="cb53-381"><a href="#cb53-381" aria-hidden="true" tabindex="-1"></a>plt.hist(normals, bins <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">2</span>, <span class="fl">13.5</span>, <span class="fl">0.5</span>))</span>
<span id="cb53-382"><a href="#cb53-382" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-383"><a href="#cb53-383" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-384"><a href="#cb53-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-385"><a href="#cb53-385" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.3.2</span></span>
<span id="cb53-386"><a href="#cb53-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-387"><a href="#cb53-387" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Understanding random seeds {.unnumbered}</span></span>
<span id="cb53-388"><a href="#cb53-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-389"><a href="#cb53-389" aria-hidden="true" tabindex="-1"></a>While random numbers are important for many analyses, they create a problem: the results you get can vary slightly. This can cause awkward conversations with your boss when your script for calculating the sales forecast gives different answers each time.</span>
<span id="cb53-390"><a href="#cb53-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-391"><a href="#cb53-391" aria-hidden="true" tabindex="-1"></a>Setting the seed for <span class="in">`numpy`</span>'s random number generator helps avoid such problems by making the random number generation reproducible. </span>
<span id="cb53-392"><a href="#cb53-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-393"><a href="#cb53-393" aria-hidden="true" tabindex="-1"></a><span class="fu">##### Question 1 {.unnumbered}</span></span>
<span id="cb53-394"><a href="#cb53-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-395"><a href="#cb53-395" aria-hidden="true" tabindex="-1"></a>Which statement about x and y is true?</span>
<span id="cb53-396"><a href="#cb53-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-397"><a href="#cb53-397" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-398"><a href="#cb53-398" aria-hidden="true" tabindex="-1"></a><span class="in">import numpy as np</span></span>
<span id="cb53-399"><a href="#cb53-399" aria-hidden="true" tabindex="-1"></a><span class="in">np.random.seed(123)</span></span>
<span id="cb53-400"><a href="#cb53-400" aria-hidden="true" tabindex="-1"></a><span class="in">x = np.random.normal(size=5)</span></span>
<span id="cb53-401"><a href="#cb53-401" aria-hidden="true" tabindex="-1"></a><span class="in">y = np.random.normal(size=5)</span></span>
<span id="cb53-402"><a href="#cb53-402" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-403"><a href="#cb53-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-404"><a href="#cb53-404" aria-hidden="true" tabindex="-1"></a>*The values of x are different from those of y*</span>
<span id="cb53-405"><a href="#cb53-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-406"><a href="#cb53-406" aria-hidden="true" tabindex="-1"></a><span class="fu">##### Question 2 {.unnumbered}</span></span>
<span id="cb53-407"><a href="#cb53-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-408"><a href="#cb53-408" aria-hidden="true" tabindex="-1"></a>Which statement about x and y is true?</span>
<span id="cb53-409"><a href="#cb53-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-410"><a href="#cb53-410" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-411"><a href="#cb53-411" aria-hidden="true" tabindex="-1"></a><span class="in">import numpy as np</span></span>
<span id="cb53-412"><a href="#cb53-412" aria-hidden="true" tabindex="-1"></a><span class="in">np.random.seed(123)</span></span>
<span id="cb53-413"><a href="#cb53-413" aria-hidden="true" tabindex="-1"></a><span class="in">x = np.random.normal(size=5)</span></span>
<span id="cb53-414"><a href="#cb53-414" aria-hidden="true" tabindex="-1"></a><span class="in">np.random.seed(123)</span></span>
<span id="cb53-415"><a href="#cb53-415" aria-hidden="true" tabindex="-1"></a><span class="in">y = np.random.normal(size=5)</span></span>
<span id="cb53-416"><a href="#cb53-416" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-417"><a href="#cb53-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-418"><a href="#cb53-418" aria-hidden="true" tabindex="-1"></a>*x and y have identical values.*</span>
<span id="cb53-419"><a href="#cb53-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-420"><a href="#cb53-420" aria-hidden="true" tabindex="-1"></a><span class="fu">##### Question 3 {.unnumbered}</span></span>
<span id="cb53-421"><a href="#cb53-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-422"><a href="#cb53-422" aria-hidden="true" tabindex="-1"></a>Which statement about x and y is true?</span>
<span id="cb53-423"><a href="#cb53-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-424"><a href="#cb53-424" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-425"><a href="#cb53-425" aria-hidden="true" tabindex="-1"></a><span class="in">import numpy as np</span></span>
<span id="cb53-426"><a href="#cb53-426" aria-hidden="true" tabindex="-1"></a><span class="in">np.random.seed(123)</span></span>
<span id="cb53-427"><a href="#cb53-427" aria-hidden="true" tabindex="-1"></a><span class="in">x = np.random.normal(size=5)</span></span>
<span id="cb53-428"><a href="#cb53-428" aria-hidden="true" tabindex="-1"></a><span class="in">np.random.seed(456)</span></span>
<span id="cb53-429"><a href="#cb53-429" aria-hidden="true" tabindex="-1"></a><span class="in">y = np.random.normal(size=5)</span></span>
<span id="cb53-430"><a href="#cb53-430" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-431"><a href="#cb53-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-432"><a href="#cb53-432" aria-hidden="true" tabindex="-1"></a>*The values of x are different from those of y.*</span>
<span id="cb53-433"><a href="#cb53-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-434"><a href="#cb53-434" aria-hidden="true" tabindex="-1"></a><span class="fu">## CHAPTER 2: Sampling Methods </span></span>
<span id="cb53-435"><a href="#cb53-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-436"><a href="#cb53-436" aria-hidden="true" tabindex="-1"></a>It’s time to get hands-on and perform the four random sampling methods in Python: simple, systematic, stratified, and cluster.</span>
<span id="cb53-437"><a href="#cb53-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-438"><a href="#cb53-438" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 2.1: Simple random and systematic sampling</span></span>
<span id="cb53-439"><a href="#cb53-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-440"><a href="#cb53-440" aria-hidden="true" tabindex="-1"></a>There are several methods of sampling from a population. In this video, we'll look at simple random sampling and systematic random sampling.</span>
<span id="cb53-441"><a href="#cb53-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-442"><a href="#cb53-442" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Simple random sampling {.unnumbered}</span></span>
<span id="cb53-443"><a href="#cb53-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-444"><a href="#cb53-444" aria-hidden="true" tabindex="-1"></a>Simple random sampling works like a raffle or lottery. We start with our population of raffle tickets or lottery balls and randomly pick them out one at a time.</span>
<span id="cb53-445"><a href="#cb53-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-446"><a href="#cb53-446" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Simple random sampling of coffees {.unnumbered}</span></span>
<span id="cb53-447"><a href="#cb53-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-448"><a href="#cb53-448" aria-hidden="true" tabindex="-1"></a>In our coffee ratings dataset, instead of raffle tickets or lottery balls, the population consists of coffee varieties. To perform simple random sampling, we take some at random, one at a time. Each coffee has the same chance as any other of being picked. When using this technique, sometimes we might end up with two coffees that were next to each other in the dataset, and sometimes we might end up with large areas of the dataset that were not selected from at all.</span>
<span id="cb53-449"><a href="#cb53-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-450"><a href="#cb53-450" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Simple random sampling with pandas {.unnumbered}</span></span>
<span id="cb53-451"><a href="#cb53-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-452"><a href="#cb53-452" aria-hidden="true" tabindex="-1"></a>We've already seen how to do simple random sampling with pandas. We call <span class="in">`.sample`</span> and set <span class="in">`n`</span> to the size of the sample. We can also set the seed using the random_state argument to generate reproducible results, just like we did pseudo-random number generation. Previously, by not setting <span class="in">`random_state`</span> when sampling, our code would generate a different random sample each time it was run.</span>
<span id="cb53-453"><a href="#cb53-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-454"><a href="#cb53-454" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Systematic sampling {.unnumbered}</span></span>
<span id="cb53-455"><a href="#cb53-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-456"><a href="#cb53-456" aria-hidden="true" tabindex="-1"></a>Another sampling method is known as systematic sampling. This samples the population at regular intervals. Here, looking from top to bottom and left to right within each row, every fifth coffee is sampled.</span>
<span id="cb53-457"><a href="#cb53-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-458"><a href="#cb53-458" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Systematic sampling - defining the interval {.unnumbered}</span></span>
<span id="cb53-459"><a href="#cb53-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-460"><a href="#cb53-460" aria-hidden="true" tabindex="-1"></a>Systematic sampling with pandas is slightly trickier than simple random sampling. The tricky part is determining how big the interval between each row should be for a given sample size. Suppose we want a sample size of five coffees. The population size is the number of rows in the whole dataset, and in this case, it's one thousand three hundred and thirty-eight. The interval is the population size divided by the sample size, but because we want the answer to be an integer, we perform integer division with two forward slashes. This is like standard division but discards any fractional part. One-three-three-eight divided by five is actually two hundred and sixty-seven-point-six, and discarding the fractional part leaves two hundred and sixty-seven. Thus, to get a systematic sample of five coffees, we will select every two hundred sixty-seventh coffee in the dataset.</span>
<span id="cb53-461"><a href="#cb53-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-462"><a href="#cb53-462" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Systematic sampling - selecting the rows {.unnumbered}</span></span>
<span id="cb53-463"><a href="#cb53-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-464"><a href="#cb53-464" aria-hidden="true" tabindex="-1"></a>To select every two hundred and sixty-seventh row, we call dot-iloc on coffee_ratings and pass double-colons and the interval, which is 267 in this case. Double-colon interval tells pandas to select every two hundred and sixty-seventh row from zero to the end of the DataFrame.</span>
<span id="cb53-465"><a href="#cb53-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-466"><a href="#cb53-466" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The trouble with systematic sampling {.unnumbered}</span></span>
<span id="cb53-467"><a href="#cb53-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-468"><a href="#cb53-468" aria-hidden="true" tabindex="-1"></a>There is a problem with systematic sampling, though. Suppose we are interested in statistics about the aftertaste attribute of the coffees. To examine this, first, we use <span class="in">`reset_index`</span> to create a column of index values in our DataFrame that we can plot. Plotting aftertaste against index shows a pattern. Earlier rows generally have higher aftertaste scores than later rows. This introduces bias into the statistics that we calculate. In general, it is only safe to use systematic sampling if a plot like this has no pattern; that is, it just looks like noise.</span>
<span id="cb53-469"><a href="#cb53-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-470"><a href="#cb53-470" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Making systematic sampling safe {.unnumbered}</span></span>
<span id="cb53-471"><a href="#cb53-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-472"><a href="#cb53-472" aria-hidden="true" tabindex="-1"></a>To ensure that systematic sampling is safe, we can randomize the row order before sampling. dot-sample has an argument named frac that lets us specify the proportion of the dataset to return in the sample, rather than the absolute number of rows that n specifies. Setting frac to one randomly samples the whole dataset. In effect, this randomly shuffles the rows. Next, the indices need to be reset so that they go in order from zero again. Specifying drop equals True clears the previous row indexes, and chaining to another <span class="in">`reset_index`</span> call creates a column containing these new indexes. Redrawing the plot with the shuffled dataset shows no pattern between aftertaste and index. This is great, but note that once we've shuffled the rows, systematic sampling is essentially the same as simple random sampling.</span>
<span id="cb53-473"><a href="#cb53-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-474"><a href="#cb53-474" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.1.1</span></span>
<span id="cb53-475"><a href="#cb53-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-476"><a href="#cb53-476" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Simple random sampling {.unnumbered}</span></span>
<span id="cb53-477"><a href="#cb53-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-478"><a href="#cb53-478" aria-hidden="true" tabindex="-1"></a>The simplest method of sampling a population is the one you've seen already. It is known as *simple random sampling* (sometimes abbreviated to "SRS"), and involves picking rows at random, one at a time, where each row has the same chance of being picked as any other.</span>
<span id="cb53-479"><a href="#cb53-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-480"><a href="#cb53-480" aria-hidden="true" tabindex="-1"></a>In this chapter, you'll apply sampling methods to a synthetic (fictional) employee attrition dataset from IBM, where "attrition" in this context means leaving the company.</span>
<span id="cb53-481"><a href="#cb53-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-482"><a href="#cb53-482" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-483"><a href="#cb53-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-484"><a href="#cb53-484" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sample 70 rows from <span class="in">`attrition`</span> using simple random sampling, setting the random seed to 18900217.</span>
<span id="cb53-485"><a href="#cb53-485" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Print the sample dataset, <span class="in">`attrition_samp`</span>. What do you notice about the indices?</span>
<span id="cb53-486"><a href="#cb53-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-489"><a href="#cb53-489" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-490"><a href="#cb53-490" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb53-491"><a href="#cb53-491" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-492"><a href="#cb53-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-493"><a href="#cb53-493" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-494"><a href="#cb53-494" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb53-495"><a href="#cb53-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-496"><a href="#cb53-496" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 70 rows using simple random sampling and set the seed</span></span>
<span id="cb53-497"><a href="#cb53-497" aria-hidden="true" tabindex="-1"></a>attrition_samp <span class="op">=</span> attrition.sample(n<span class="op">=</span><span class="dv">70</span>, random_state<span class="op">=</span><span class="dv">18900217</span>)</span>
<span id="cb53-498"><a href="#cb53-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-499"><a href="#cb53-499" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb53-500"><a href="#cb53-500" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_samp)</span>
<span id="cb53-501"><a href="#cb53-501" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-502"><a href="#cb53-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-503"><a href="#cb53-503" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.1.2</span></span>
<span id="cb53-504"><a href="#cb53-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-505"><a href="#cb53-505" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Systematic sampling {.unnumbered}</span></span>
<span id="cb53-506"><a href="#cb53-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-507"><a href="#cb53-507" aria-hidden="true" tabindex="-1"></a>One sampling method that avoids randomness is called systematic sampling. Here, you pick rows from the population at regular intervals.</span>
<span id="cb53-508"><a href="#cb53-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-509"><a href="#cb53-509" aria-hidden="true" tabindex="-1"></a>For example, if the population dataset had one thousand rows, and you wanted a sample size of five, you could pick rows 0, 200, 400, 600, and 800.</span>
<span id="cb53-510"><a href="#cb53-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-511"><a href="#cb53-511" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-512"><a href="#cb53-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-513"><a href="#cb53-513" aria-hidden="true" tabindex="-1"></a>1.Set the sample size to 70.</span>
<span id="cb53-514"><a href="#cb53-514" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Calculate the population size from <span class="in">`attrition`</span>.</span>
<span id="cb53-515"><a href="#cb53-515" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Calculate the interval between the rows to be sampled.</span>
<span id="cb53-516"><a href="#cb53-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-517"><a href="#cb53-517" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Systematically sample <span class="in">`attrition`</span> to get the rows of the population at each <span class="in">`interval`</span>, starting at 0; assign the rows to <span class="in">`attrition_sys_samp`</span></span>
<span id="cb53-518"><a href="#cb53-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-521"><a href="#cb53-521" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-522"><a href="#cb53-522" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb53-523"><a href="#cb53-523" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-524"><a href="#cb53-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-525"><a href="#cb53-525" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-526"><a href="#cb53-526" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb53-527"><a href="#cb53-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-528"><a href="#cb53-528" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the sample size to 70</span></span>
<span id="cb53-529"><a href="#cb53-529" aria-hidden="true" tabindex="-1"></a>sample_size <span class="op">=</span> <span class="dv">70</span></span>
<span id="cb53-530"><a href="#cb53-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-531"><a href="#cb53-531" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the population size from attrition_pop</span></span>
<span id="cb53-532"><a href="#cb53-532" aria-hidden="true" tabindex="-1"></a>pop_size <span class="op">=</span> <span class="bu">len</span>(attrition)</span>
<span id="cb53-533"><a href="#cb53-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-534"><a href="#cb53-534" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the interval</span></span>
<span id="cb53-535"><a href="#cb53-535" aria-hidden="true" tabindex="-1"></a>interval <span class="op">=</span> pop_size<span class="op">//</span>sample_size</span>
<span id="cb53-536"><a href="#cb53-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-537"><a href="#cb53-537" aria-hidden="true" tabindex="-1"></a><span class="co"># Systematically sample 70 rows</span></span>
<span id="cb53-538"><a href="#cb53-538" aria-hidden="true" tabindex="-1"></a>attrition_sys_samp <span class="op">=</span> attrition.iloc[::interval]</span>
<span id="cb53-539"><a href="#cb53-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-540"><a href="#cb53-540" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb53-541"><a href="#cb53-541" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_sys_samp)</span>
<span id="cb53-542"><a href="#cb53-542" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-543"><a href="#cb53-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-544"><a href="#cb53-544" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.1.3</span></span>
<span id="cb53-545"><a href="#cb53-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-546"><a href="#cb53-546" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Is systematic sampling OK? {.unnumbered}</span></span>
<span id="cb53-547"><a href="#cb53-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-548"><a href="#cb53-548" aria-hidden="true" tabindex="-1"></a>Systematic sampling has a problem: if the data has been sorted, or there is some sort of pattern or meaning behind the row order, then the resulting sample may not be representative of the whole population. The problem can be solved by shuffling the rows, but then systematic sampling is equivalent to simple random sampling.</span>
<span id="cb53-549"><a href="#cb53-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-550"><a href="#cb53-550" aria-hidden="true" tabindex="-1"></a>Here you'll look at how to determine whether or not there is a problem.</span>
<span id="cb53-551"><a href="#cb53-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-552"><a href="#cb53-552" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-553"><a href="#cb53-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-554"><a href="#cb53-554" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Add an index column to <span class="in">`attrition`</span>, assigning the result to <span class="in">`attrition_id`</span>.</span>
<span id="cb53-555"><a href="#cb53-555" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Create a scatter plot of <span class="in">`YearsAtCompany`</span> versus <span class="in">`index`</span> for <span class="in">`attrition_id`</span> using pandas <span class="in">`.plot()`</span>.</span>
<span id="cb53-556"><a href="#cb53-556" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Randomly shuffle the rows of <span class="in">`attrition`</span>.</span>
<span id="cb53-557"><a href="#cb53-557" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Reset the row indexes, and add an <span class="in">`index`</span> column to <span class="in">`attrition`</span>.</span>
<span id="cb53-558"><a href="#cb53-558" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Repeat the scatter plot of <span class="in">`YearsAtCompany`</span> versus <span class="in">`index`</span>, this time using <span class="in">`attrition_shuffled`</span>.</span>
<span id="cb53-559"><a href="#cb53-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-562"><a href="#cb53-562" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-563"><a href="#cb53-563" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-564"><a href="#cb53-564" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-565"><a href="#cb53-565" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-566"><a href="#cb53-566" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-567"><a href="#cb53-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-568"><a href="#cb53-568" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-569"><a href="#cb53-569" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb53-570"><a href="#cb53-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-571"><a href="#cb53-571" aria-hidden="true" tabindex="-1"></a><span class="co"># Add an index column to attrition_pop</span></span>
<span id="cb53-572"><a href="#cb53-572" aria-hidden="true" tabindex="-1"></a>attrition_id <span class="op">=</span> attrition.reset_index()</span>
<span id="cb53-573"><a href="#cb53-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-574"><a href="#cb53-574" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot YearsAtCompany vs. index for attrition_pop_id</span></span>
<span id="cb53-575"><a href="#cb53-575" aria-hidden="true" tabindex="-1"></a>attrition_id.plot(x<span class="op">=</span><span class="st">"index"</span>, y<span class="op">=</span><span class="st">"YearsAtCompany"</span>, kind<span class="op">=</span><span class="st">"scatter"</span>)</span>
<span id="cb53-576"><a href="#cb53-576" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-577"><a href="#cb53-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-578"><a href="#cb53-578" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the rows of attrition_pop</span></span>
<span id="cb53-579"><a href="#cb53-579" aria-hidden="true" tabindex="-1"></a>attrition_shuffled <span class="op">=</span> attrition.sample(frac<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb53-580"><a href="#cb53-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-581"><a href="#cb53-581" aria-hidden="true" tabindex="-1"></a><span class="co"># Reset the row indexes and create an index column</span></span>
<span id="cb53-582"><a href="#cb53-582" aria-hidden="true" tabindex="-1"></a>attrition_shuffled <span class="op">=</span> attrition_shuffled.reset_index(drop<span class="op">=</span><span class="va">True</span>).reset_index()</span>
<span id="cb53-583"><a href="#cb53-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-584"><a href="#cb53-584" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot YearsAtCompany vs. index for attrition_shuffled</span></span>
<span id="cb53-585"><a href="#cb53-585" aria-hidden="true" tabindex="-1"></a>attrition_shuffled.plot(x<span class="op">=</span><span class="st">"index"</span>, y<span class="op">=</span><span class="st">"YearsAtCompany"</span>, kind<span class="op">=</span><span class="st">"scatter"</span>)</span>
<span id="cb53-586"><a href="#cb53-586" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-587"><a href="#cb53-587" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-588"><a href="#cb53-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-589"><a href="#cb53-589" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb53-590"><a href="#cb53-590" aria-hidden="true" tabindex="-1"></a><span class="fu">### Question {.unnumbered}</span></span>
<span id="cb53-591"><a href="#cb53-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-592"><a href="#cb53-592" aria-hidden="true" tabindex="-1"></a>Does a systematic sample always produce a sample similar to a simple random sample?</span>
<span id="cb53-593"><a href="#cb53-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-594"><a href="#cb53-594" aria-hidden="true" tabindex="-1"></a>*No, Systematic sampling has problems when the data are sorted or contain a pattern. Shuffling the rows makes it equivalent to simple random sampling*.</span>
<span id="cb53-595"><a href="#cb53-595" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-596"><a href="#cb53-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-597"><a href="#cb53-597" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 2.2: Stratified and weighted random sampling </span></span>
<span id="cb53-598"><a href="#cb53-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-599"><a href="#cb53-599" aria-hidden="true" tabindex="-1"></a>Stratified sampling is a technique that allows us to sample a population that contains subgroups.</span>
<span id="cb53-600"><a href="#cb53-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-601"><a href="#cb53-601" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Coffees by country  {.unnumbered}</span></span>
<span id="cb53-602"><a href="#cb53-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-603"><a href="#cb53-603" aria-hidden="true" tabindex="-1"></a>For example, we could group the coffee ratings by country. If we count the number of coffees by country using the <span class="in">`value_counts`</span> method, we can see that these six countries have the most data.</span>
<span id="cb53-604"><a href="#cb53-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-605"><a href="#cb53-605" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>1 The dataset lists Hawaii and Taiwan as countries for convenience, as they are notable coffee-growing regions.</span>
<span id="cb53-606"><a href="#cb53-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-607"><a href="#cb53-607" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Filtering for 6 countries  {.unnumbered}</span></span>
<span id="cb53-608"><a href="#cb53-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-609"><a href="#cb53-609" aria-hidden="true" tabindex="-1"></a>To make it easier to think about sampling subgroups, let's limit our analysis to these six countries. We can use the <span class="in">`.isin`</span> method to filter the population and only return the rows corresponding to these six countries. This filtered dataset is stored as <span class="in">`coffee_ratings_top`</span>.</span>
<span id="cb53-610"><a href="#cb53-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-611"><a href="#cb53-611" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Counts of a simple random sample  {.unnumbered}</span></span>
<span id="cb53-612"><a href="#cb53-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-613"><a href="#cb53-613" aria-hidden="true" tabindex="-1"></a>Let's take a ten percent simple random sample of the dataset using <span class="in">`.sample`</span> with <span class="in">`frac`</span> set to 0.1. We also set the <span class="in">`random_state`</span> argument to ensure reproducibility. As with the whole dataset, we can look at the counts for each country. To make comparisons easier, we set <span class="in">`normalize`</span> to <span class="in">`True`</span> to convert the counts into a proportion, which shows what proportion of coffees in the sample came from each country.</span>
<span id="cb53-614"><a href="#cb53-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-615"><a href="#cb53-615" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Comparing proportions  {.unnumbered}</span></span>
<span id="cb53-616"><a href="#cb53-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-617"><a href="#cb53-617" aria-hidden="true" tabindex="-1"></a>Here are the proportions for the population and the ten percent sample side by side. Just by chance, in this sample, Taiwanese coffees form a disproportionately low percentage. The different makeup of the sample compared to the population could be a problem if we want to analyze the country of origin, for example.</span>
<span id="cb53-618"><a href="#cb53-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-619"><a href="#cb53-619" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Proportional stratified sampling  {.unnumbered}</span></span>
<span id="cb53-620"><a href="#cb53-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-621"><a href="#cb53-621" aria-hidden="true" tabindex="-1"></a>If we care about the proportions of each country in the sample closely matching those in the population, then we can group the data by country before taking the simple random sample. Note that we used the Python line continuation backslash here, which can be useful for breaking up longer chains of pandas code like this. Calling the <span class="in">`.sample`</span> method after grouping takes a simple random sample within each country. Now the proportions of each country in the stratified sample are much closer to those in the population.</span>
<span id="cb53-622"><a href="#cb53-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-623"><a href="#cb53-623" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Equal counts stratified sampling  {.unnumbered}</span></span>
<span id="cb53-624"><a href="#cb53-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-625"><a href="#cb53-625" aria-hidden="true" tabindex="-1"></a>One variation of stratified sampling is to sample equal counts from each group, rather than an equal proportion. The code only has one change from before. This time, we use the <span class="in">`n`</span> argument in <span class="in">`.sample`</span> instead of <span class="in">`frac`</span> to extract fifteen randomly-selected rows from each country. Here, the resulting sample has equal proportions of one-sixth from each country.</span>
<span id="cb53-626"><a href="#cb53-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-627"><a href="#cb53-627" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Weighted random sampling {.unnumbered}</span></span>
<span id="cb53-628"><a href="#cb53-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-629"><a href="#cb53-629" aria-hidden="true" tabindex="-1"></a>A close relative of stratified sampling that provides even more flexibility is weighted random sampling. In this variant, we create a column of weights that adjust the relative probability of sampling each row. For example, suppose we thought that it was important to have a higher proportion of Taiwanese coffees in the sample than in the population. We create a condition, in this case, rows where the country of origin is Taiwan. Using the <span class="in">`where`</span> function from NumPy, we can set a weight of two for rows that match the condition and a weight of one for rows that don't match the condition. This means when each row is randomly sampled, Taiwanese coffees have two times the chance of being picked compared to other coffees. When we call <span class="in">`.sample`</span>, we pass the column of weights to the weights argument.</span>
<span id="cb53-630"><a href="#cb53-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-631"><a href="#cb53-631" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Weighted random sampling results  {.unnumbered}</span></span>
<span id="cb53-632"><a href="#cb53-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-633"><a href="#cb53-633" aria-hidden="true" tabindex="-1"></a>Here, we can see that Taiwan now contains seventeen percent of the sampled dataset, compared to eight-point-five percent in the population. This sort of weighted sampling is common in political polling, where we need to correct for under- or over-representation of demographic groups.</span>
<span id="cb53-634"><a href="#cb53-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-635"><a href="#cb53-635" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Exercise 2.2.1</span></span>
<span id="cb53-636"><a href="#cb53-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-637"><a href="#cb53-637" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Proportional stratified sampling {.unnumbered}</span></span>
<span id="cb53-638"><a href="#cb53-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-639"><a href="#cb53-639" aria-hidden="true" tabindex="-1"></a>If you are interested in subgroups within the population, then you may need to carefully control the counts of each subgroup within the population. *Proportional stratified sampling* results in subgroup sizes within the sample that are representative of the subgroup sizes within the population. It is equivalent to performing a simple random sample on each subgroup.</span>
<span id="cb53-640"><a href="#cb53-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-641"><a href="#cb53-641" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-642"><a href="#cb53-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-643"><a href="#cb53-643" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Get the proportion of employees by Education level from <span class="in">`attrition`</span>.</span>
<span id="cb53-644"><a href="#cb53-644" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Use proportional stratified sampling on <span class="in">`attrition_pop`</span> to sample 40% of each <span class="in">`Education`</span> group, setting the seed to <span class="in">`2022`</span>.</span>
<span id="cb53-645"><a href="#cb53-645" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Get the proportion of employees by <span class="in">`Education`</span> level from <span class="in">`attrition_strat`</span>.</span>
<span id="cb53-646"><a href="#cb53-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-649"><a href="#cb53-649" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-650"><a href="#cb53-650" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-651"><a href="#cb53-651" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-652"><a href="#cb53-652" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-653"><a href="#cb53-653" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-654"><a href="#cb53-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-655"><a href="#cb53-655" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-656"><a href="#cb53-656" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb53-657"><a href="#cb53-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-658"><a href="#cb53-658" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportion of employees by Education level</span></span>
<span id="cb53-659"><a href="#cb53-659" aria-hidden="true" tabindex="-1"></a>education_counts_pop <span class="op">=</span> attrition[<span class="st">'Education'</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb53-660"><a href="#cb53-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-661"><a href="#cb53-661" aria-hidden="true" tabindex="-1"></a><span class="co"># Print education_counts_pop</span></span>
<span id="cb53-662"><a href="#cb53-662" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(education_counts_pop)</span>
<span id="cb53-663"><a href="#cb53-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-664"><a href="#cb53-664" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportional stratified sampling for 40% of each Education group</span></span>
<span id="cb53-665"><a href="#cb53-665" aria-hidden="true" tabindex="-1"></a>attrition_strat <span class="op">=</span> attrition.groupby(<span class="st">'Education'</span>)<span class="op">\</span></span>
<span id="cb53-666"><a href="#cb53-666" aria-hidden="true" tabindex="-1"></a>.sample(frac<span class="op">=</span><span class="fl">0.4</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb53-667"><a href="#cb53-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-668"><a href="#cb53-668" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb53-669"><a href="#cb53-669" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_strat)</span>
<span id="cb53-670"><a href="#cb53-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-671"><a href="#cb53-671" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the Education level proportions from attrition_strat</span></span>
<span id="cb53-672"><a href="#cb53-672" aria-hidden="true" tabindex="-1"></a>education_counts_strat <span class="op">=</span> attrition_strat[<span class="st">'Education'</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb53-673"><a href="#cb53-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-674"><a href="#cb53-674" aria-hidden="true" tabindex="-1"></a><span class="co"># Print education_counts_strat</span></span>
<span id="cb53-675"><a href="#cb53-675" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(education_counts_strat)</span>
<span id="cb53-676"><a href="#cb53-676" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-677"><a href="#cb53-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-678"><a href="#cb53-678" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb53-679"><a href="#cb53-679" aria-hidden="true" tabindex="-1"></a>*By grouping then sampling, the size of each group in the sample is representative of the size of the sample in the population*.</span>
<span id="cb53-680"><a href="#cb53-680" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-681"><a href="#cb53-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-682"><a href="#cb53-682" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.2.2</span></span>
<span id="cb53-683"><a href="#cb53-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-684"><a href="#cb53-684" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Equal counts stratified sampling {.unnumbered}</span></span>
<span id="cb53-685"><a href="#cb53-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-686"><a href="#cb53-686" aria-hidden="true" tabindex="-1"></a>If one subgroup is larger than another subgroup in the population, but you don't want to reflect that difference in your analysis, then you can use *equal counts stratified sampling* to generate samples where each subgroup has the same amount of data. For example, if you are analyzing blood types, O is the most common blood type worldwide, but you may wish to have equal amounts of O, A, B, and AB in your sample.</span>
<span id="cb53-687"><a href="#cb53-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-688"><a href="#cb53-688" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-689"><a href="#cb53-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-690"><a href="#cb53-690" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Use equal counts stratified sampling on <span class="in">`attrition`</span> to get 30 employees from each <span class="in">`Education`</span> group, setting the seed to <span class="in">`2022`</span>.</span>
<span id="cb53-691"><a href="#cb53-691" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Get the proportion of employees by <span class="in">`Education`</span> level from <span class="in">`attrition_eq`</span>.</span>
<span id="cb53-692"><a href="#cb53-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-695"><a href="#cb53-695" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-696"><a href="#cb53-696" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-697"><a href="#cb53-697" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-698"><a href="#cb53-698" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-699"><a href="#cb53-699" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-700"><a href="#cb53-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-701"><a href="#cb53-701" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-702"><a href="#cb53-702" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb53-703"><a href="#cb53-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-704"><a href="#cb53-704" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportion of employees by Education level</span></span>
<span id="cb53-705"><a href="#cb53-705" aria-hidden="true" tabindex="-1"></a>education_counts_pop <span class="op">=</span> attrition[<span class="st">'Education'</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb53-706"><a href="#cb53-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-707"><a href="#cb53-707" aria-hidden="true" tabindex="-1"></a><span class="co"># Print education_counts_pop</span></span>
<span id="cb53-708"><a href="#cb53-708" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(education_counts_pop)</span>
<span id="cb53-709"><a href="#cb53-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-710"><a href="#cb53-710" aria-hidden="true" tabindex="-1"></a><span class="co"># Get 30 employees from each Education group</span></span>
<span id="cb53-711"><a href="#cb53-711" aria-hidden="true" tabindex="-1"></a>attrition_eq <span class="op">=</span> attrition.groupby(<span class="st">'Education'</span>)<span class="op">\</span></span>
<span id="cb53-712"><a href="#cb53-712" aria-hidden="true" tabindex="-1"></a>.sample(n<span class="op">=</span><span class="dv">30</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb53-713"><a href="#cb53-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-714"><a href="#cb53-714" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb53-715"><a href="#cb53-715" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_eq)</span>
<span id="cb53-716"><a href="#cb53-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-717"><a href="#cb53-717" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the proportions from attrition_eq</span></span>
<span id="cb53-718"><a href="#cb53-718" aria-hidden="true" tabindex="-1"></a>education_counts_eq <span class="op">=</span> attrition_eq[<span class="st">'Education'</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb53-719"><a href="#cb53-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-720"><a href="#cb53-720" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb53-721"><a href="#cb53-721" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(education_counts_eq)</span>
<span id="cb53-722"><a href="#cb53-722" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-723"><a href="#cb53-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-724"><a href="#cb53-724" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb53-725"><a href="#cb53-725" aria-hidden="true" tabindex="-1"></a>*If you want each subgroup to have equal weight in your analysis, then equal counts stratified sampling is the appropriate technique.*</span>
<span id="cb53-726"><a href="#cb53-726" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-727"><a href="#cb53-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-728"><a href="#cb53-728" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.2.3</span></span>
<span id="cb53-729"><a href="#cb53-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-730"><a href="#cb53-730" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Weighted sampling {.unnumbered}</span></span>
<span id="cb53-731"><a href="#cb53-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-732"><a href="#cb53-732" aria-hidden="true" tabindex="-1"></a>Stratified sampling provides rules about the probability of picking rows from your dataset at the subgroup level. A generalization of this is weighted sampling, which lets you specify rules about the probability of picking rows at the row level. The probability of picking any given row is proportional to the weight value for that row.</span>
<span id="cb53-733"><a href="#cb53-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-734"><a href="#cb53-734" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-735"><a href="#cb53-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-736"><a href="#cb53-736" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Plot <span class="in">`YearsAtCompany`</span> from <span class="in">`attrition`</span> as a histogram with bins of width <span class="in">`1`</span> from <span class="in">`0`</span> to <span class="in">`40`</span>.</span>
<span id="cb53-737"><a href="#cb53-737" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Sample 400 employees from <span class="in">`attrition`</span> weighted by <span class="in">`YearsAtCompany`</span>.</span>
<span id="cb53-738"><a href="#cb53-738" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Plot <span class="in">`YearsAtCompany`</span> from <span class="in">`attrition_weight`</span> as a histogram with bins of width <span class="in">`1`</span> from <span class="in">`0`</span> to <span class="in">`40`</span>.</span>
<span id="cb53-739"><a href="#cb53-739" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Which is higher? The mean <span class="in">`YearsAtCompany`</span> from <span class="in">`attrition`</span> or the mean <span class="in">`YearsAtCompany`</span> from <span class="in">`attrition_weight`</span>? **Answer**: *The weighted sample mean is around 11, which is higher than the population mean of around 7. The fact that the two numbers are different means that the weighted simple random sample is biased.*</span>
<span id="cb53-740"><a href="#cb53-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-743"><a href="#cb53-743" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-744"><a href="#cb53-744" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-745"><a href="#cb53-745" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-746"><a href="#cb53-746" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-747"><a href="#cb53-747" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-748"><a href="#cb53-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-749"><a href="#cb53-749" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-750"><a href="#cb53-750" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb53-751"><a href="#cb53-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-752"><a href="#cb53-752" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot YearsAtCompany from attrition_pop as a histogram</span></span>
<span id="cb53-753"><a href="#cb53-753" aria-hidden="true" tabindex="-1"></a>attrition[<span class="st">'YearsAtCompany'</span>].hist(bins<span class="op">=</span>np.arange(<span class="dv">0</span>,<span class="dv">41</span>,<span class="dv">1</span>))</span>
<span id="cb53-754"><a href="#cb53-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-755"><a href="#cb53-755" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample 400 employees weighted by YearsAtCompany</span></span>
<span id="cb53-756"><a href="#cb53-756" aria-hidden="true" tabindex="-1"></a>attrition_weight <span class="op">=</span> attrition.sample(n<span class="op">=</span><span class="dv">400</span>, weights<span class="op">=</span><span class="st">'YearsAtCompany'</span>)</span>
<span id="cb53-757"><a href="#cb53-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-758"><a href="#cb53-758" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb53-759"><a href="#cb53-759" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_weight)</span>
<span id="cb53-760"><a href="#cb53-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-761"><a href="#cb53-761" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot YearsAtCompany from attrition_weight as a histogram</span></span>
<span id="cb53-762"><a href="#cb53-762" aria-hidden="true" tabindex="-1"></a>attrition_weight[<span class="st">'YearsAtCompany'</span>].hist(bins<span class="op">=</span>np.arange(<span class="dv">0</span>, <span class="dv">41</span>, <span class="dv">1</span>))</span>
<span id="cb53-763"><a href="#cb53-763" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-764"><a href="#cb53-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-765"><a href="#cb53-765" aria-hidden="true" tabindex="-1"></a><span class="co"># The mean YearsAtCompany from attrition dataset </span></span>
<span id="cb53-766"><a href="#cb53-766" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition[<span class="st">'YearsAtCompany'</span>].mean())</span>
<span id="cb53-767"><a href="#cb53-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-768"><a href="#cb53-768" aria-hidden="true" tabindex="-1"></a><span class="co"># The mean YearsAtCompany from attrition_weight </span></span>
<span id="cb53-769"><a href="#cb53-769" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_weight[<span class="st">'YearsAtCompany'</span>].mean())</span>
<span id="cb53-770"><a href="#cb53-770" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-771"><a href="#cb53-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-772"><a href="#cb53-772" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 2.3: Cluster sampling</span></span>
<span id="cb53-773"><a href="#cb53-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-774"><a href="#cb53-774" aria-hidden="true" tabindex="-1"></a>One problem with stratified sampling is that we need to collect data from every subgroup. In cases where collecting data is expensive, for example, when we have to physically travel to a location to collect it, it can make our analysis prohibitively expensive. There's a cheaper alternative called cluster sampling.</span>
<span id="cb53-775"><a href="#cb53-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-776"><a href="#cb53-776" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Stratified sampling vs. cluster sampling {.unnumbered}</span></span>
<span id="cb53-777"><a href="#cb53-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-778"><a href="#cb53-778" aria-hidden="true" tabindex="-1"></a>The stratified sampling approach was to split the population into subgroups, then use simple random sampling on each of them. Cluster sampling means that we limit the number of subgroups in the analysis by picking a few of them with simple random sampling. We then perform simple random sampling on each subgroup as before.</span>
<span id="cb53-779"><a href="#cb53-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-780"><a href="#cb53-780" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Varieties of coffee {.unnumbered}</span></span>
<span id="cb53-781"><a href="#cb53-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-782"><a href="#cb53-782" aria-hidden="true" tabindex="-1"></a>Let's return to the coffee dataset and look at the varieties of coffee. In this image, each bean represents the whole subgroup rather than an individual coffee, and there are twenty-eight of them. To extract unique varieties, we use the <span class="in">`.unique`</span> method. This returns an array, so wrapping it in the list function creates a list of unique varieties. Let's suppose that it's expensive to work with all of the different varieties. Enter cluster sampling.</span>
<span id="cb53-783"><a href="#cb53-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-784"><a href="#cb53-784" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Stage 1: sampling for subgroups {.unnumbered}</span></span>
<span id="cb53-785"><a href="#cb53-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-786"><a href="#cb53-786" aria-hidden="true" tabindex="-1"></a>The first stage of cluster sampling is to randomly cut down the number of varieties, and we do this by randomly selecting them. Here, we've used the <span class="in">`random.sample`</span> function from the random package to get three varieties, specified using the argument k.</span>
<span id="cb53-787"><a href="#cb53-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-788"><a href="#cb53-788" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Stage 2: sampling each group {.unnumbered}</span></span>
<span id="cb53-789"><a href="#cb53-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-790"><a href="#cb53-790" aria-hidden="true" tabindex="-1"></a>The second stage of cluster sampling is to perform simple random sampling on each of the three varieties we randomly selected. We first filter the dataset for rows where the variety is one of the three selected, using the <span class="in">`.isin`</span> method. To ensure that the <span class="in">`isin`</span> filtering removes levels with zero rows, we apply the <span class="in">`cat.remove_unused_categories`</span> method on the Series of focus, which is variety here. If we exclude this method, we might receive an error when sampling by variety level. The pandas code is the same as for stratified sampling. Here, we've opted for equal counts sampling, with five rows from each remaining variety.</span>
<span id="cb53-791"><a href="#cb53-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-792"><a href="#cb53-792" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Stage 2 output {.unnumbered}</span></span>
<span id="cb53-793"><a href="#cb53-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-794"><a href="#cb53-794" aria-hidden="true" tabindex="-1"></a>Here's the first few columns of the result. Notice that there are the fifteen rows, which we'd expect from sampling five rows from three varieties.</span>
<span id="cb53-795"><a href="#cb53-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-796"><a href="#cb53-796" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Multistage sampling {.unnumbered}</span></span>
<span id="cb53-797"><a href="#cb53-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-798"><a href="#cb53-798" aria-hidden="true" tabindex="-1"></a>Note that we had two stages in the cluster sampling. We randomly sampled the subgroups to include, then we randomly sampled rows from those subgroups. Cluster sampling is a special case of multistage sampling. It's possible to use more than two stages. A common example is national surveys, which can include several levels of administrative regions, like states, counties, cities, and neighborhoods.</span>
<span id="cb53-799"><a href="#cb53-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-800"><a href="#cb53-800" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.3.1</span></span>
<span id="cb53-801"><a href="#cb53-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-802"><a href="#cb53-802" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Performing cluster sampling {.unnumbered}</span></span>
<span id="cb53-803"><a href="#cb53-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-804"><a href="#cb53-804" aria-hidden="true" tabindex="-1"></a>Now that you know when to use cluster sampling, it's time to put it into action. In this exercise, you'll explore the JobRole column of the attrition dataset. You can think of each job role as a subgroup of the whole population of employees.</span>
<span id="cb53-805"><a href="#cb53-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-806"><a href="#cb53-806" aria-hidden="true" tabindex="-1"></a>Use a seed of 19790801 to set the seed with <span class="in">`random.seed()`</span>.</span>
<span id="cb53-807"><a href="#cb53-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-808"><a href="#cb53-808" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-809"><a href="#cb53-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-810"><a href="#cb53-810" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span></span>
<span id="cb53-811"><a href="#cb53-811" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Create a list of unique <span class="in">`JobRole`</span> values from <span class="in">`attrition`</span>, and assign to <span class="in">`job_roles_pop`</span>.</span>
<span id="cb53-812"><a href="#cb53-812" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Randomly sample four <span class="in">`JobRole`</span> values from <span class="in">`job_roles_pop`</span>.</span>
<span id="cb53-813"><a href="#cb53-813" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Subset attrition_pop for the sampled job roles by filtering for rows where <span class="in">`JobRole`</span> is in <span class="in">`job_roles_samp`</span>.</span>
<span id="cb53-814"><a href="#cb53-814" aria-hidden="true" tabindex="-1"></a>3.</span>
<span id="cb53-815"><a href="#cb53-815" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Remove any unused categories from <span class="in">`JobRole`</span>.</span>
<span id="cb53-816"><a href="#cb53-816" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For each job role in the filtered dataset, take a random sample of ten rows, setting the seed to <span class="in">`2022`</span>.</span>
<span id="cb53-817"><a href="#cb53-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-820"><a href="#cb53-820" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-821"><a href="#cb53-821" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-822"><a href="#cb53-822" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-823"><a href="#cb53-823" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-824"><a href="#cb53-824" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-825"><a href="#cb53-825" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-826"><a href="#cb53-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-827"><a href="#cb53-827" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-828"><a href="#cb53-828" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb53-829"><a href="#cb53-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-830"><a href="#cb53-830" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the seed</span></span>
<span id="cb53-831"><a href="#cb53-831" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">19790801</span>)</span>
<span id="cb53-832"><a href="#cb53-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-833"><a href="#cb53-833" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of unique JobRole values</span></span>
<span id="cb53-834"><a href="#cb53-834" aria-hidden="true" tabindex="-1"></a>job_roles_pop <span class="op">=</span> <span class="bu">list</span>(attrition[<span class="st">'JobRole'</span>].unique())</span>
<span id="cb53-835"><a href="#cb53-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-836"><a href="#cb53-836" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly sample four JobRole values</span></span>
<span id="cb53-837"><a href="#cb53-837" aria-hidden="true" tabindex="-1"></a>job_roles_samp <span class="op">=</span> random.sample(job_roles_pop, k<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb53-838"><a href="#cb53-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-839"><a href="#cb53-839" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb53-840"><a href="#cb53-840" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(job_roles_samp)</span>
<span id="cb53-841"><a href="#cb53-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-842"><a href="#cb53-842" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter for rows where JobRole is in job_roles_samp</span></span>
<span id="cb53-843"><a href="#cb53-843" aria-hidden="true" tabindex="-1"></a>jobrole_condition <span class="op">=</span> attrition[<span class="st">'JobRole'</span>].isin(job_roles_samp)</span>
<span id="cb53-844"><a href="#cb53-844" aria-hidden="true" tabindex="-1"></a>attrition_filtered <span class="op">=</span> attrition[jobrole_condition]</span>
<span id="cb53-845"><a href="#cb53-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-846"><a href="#cb53-846" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb53-847"><a href="#cb53-847" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_filtered)</span>
<span id="cb53-848"><a href="#cb53-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-849"><a href="#cb53-849" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove categories with no rows</span></span>
<span id="cb53-850"><a href="#cb53-850" aria-hidden="true" tabindex="-1"></a>attrition_filtered[<span class="st">'JobRole'</span>] <span class="op">=</span> attrition_filtered[<span class="st">'JobRole'</span>].cat.remove_unused_categories()</span>
<span id="cb53-851"><a href="#cb53-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-852"><a href="#cb53-852" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly sample 10 employees from each sampled job role</span></span>
<span id="cb53-853"><a href="#cb53-853" aria-hidden="true" tabindex="-1"></a>attrition_clust <span class="op">=</span> attrition_filtered.groupby(<span class="st">'JobRole'</span>)<span class="op">\</span></span>
<span id="cb53-854"><a href="#cb53-854" aria-hidden="true" tabindex="-1"></a>.sample(n<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb53-855"><a href="#cb53-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-856"><a href="#cb53-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-857"><a href="#cb53-857" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sample</span></span>
<span id="cb53-858"><a href="#cb53-858" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_clust)</span>
<span id="cb53-859"><a href="#cb53-859" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-860"><a href="#cb53-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-861"><a href="#cb53-861" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 2.4: Comparing sampling methods</span></span>
<span id="cb53-862"><a href="#cb53-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-863"><a href="#cb53-863" aria-hidden="true" tabindex="-1"></a>Let's review the various sampling techniques we learned about.</span>
<span id="cb53-864"><a href="#cb53-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-865"><a href="#cb53-865" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Review of sampling techniques - setup {.unnumbered}</span></span>
<span id="cb53-866"><a href="#cb53-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-867"><a href="#cb53-867" aria-hidden="true" tabindex="-1"></a>For convenience, we'll stick to the six countries with the most coffee varieties that we used before. This corresponds to eight hundred and eighty rows and eight columns, retrieved using the <span class="in">`.shape`</span> attribute.</span>
<span id="cb53-868"><a href="#cb53-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-869"><a href="#cb53-869" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Review of simple random sampling {.unnumbered}</span></span>
<span id="cb53-870"><a href="#cb53-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-871"><a href="#cb53-871" aria-hidden="true" tabindex="-1"></a>Simple random sampling uses <span class="in">`.sample`</span> with either <span class="in">`n`</span> or <span class="in">`frac`</span> set to determine how many rows to pseudo-randomly choose, given a seed value set with <span class="in">`random_state`</span>. The simple random sample returns two hundred and ninety-three rows because we specified <span class="in">`frac`</span> as one-third, and one-third of eight hundred and eighty is just over two hundred and ninety-three.</span>
<span id="cb53-872"><a href="#cb53-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-873"><a href="#cb53-873" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Review of stratified sampling {.unnumbered}</span></span>
<span id="cb53-874"><a href="#cb53-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-875"><a href="#cb53-875" aria-hidden="true" tabindex="-1"></a>Stratified sampling groups by the country subgroup before performing simple random sampling on each subgroup. Given each of these top countries have quite a few rows, stratifying produces the same number of rows as the simple random sample.</span>
<span id="cb53-876"><a href="#cb53-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-877"><a href="#cb53-877" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Review of cluster sampling {.unnumbered}</span></span>
<span id="cb53-878"><a href="#cb53-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-879"><a href="#cb53-879" aria-hidden="true" tabindex="-1"></a>In the cluster sample, we've used two out of six countries to roughly mimic frac equals one-third from the other sample types. Setting n equal to one-sixth of the total number of rows gives roughly equal sample sizes in each of the two subgroups. Using <span class="in">`.shape`</span> again, we see that this cluster sample has close to the same number of rows: two-hundred-ninety-two compared to two-hundred-ninety-three for the other sample types.</span>
<span id="cb53-880"><a href="#cb53-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-881"><a href="#cb53-881" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Calculating mean cup points {.unnumbered}</span></span>
<span id="cb53-882"><a href="#cb53-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-883"><a href="#cb53-883" aria-hidden="true" tabindex="-1"></a>Let's calculate a population parameter, the mean of the total cup points. When the population parameter is the mean of a field, it's often called the population mean. Remember that in real-life scenarios, we typically wouldn't know what the population mean is. Since we have it here, though, we can use this value of eighty-one-point-nine as a gold standard to measure against. Now we'll calculate the same value using each of the sampling techniques we've discussed. These are point estimates of the mean, often called sample means. The simple and stratified sample means are really close to the population mean. Cluster sampling isn't quite as close, but that's typical. Cluster sampling is designed to give us an answer that's almost as good while using less data.</span>
<span id="cb53-884"><a href="#cb53-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-885"><a href="#cb53-885" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Mean cup points by country: simple random {.unnumbered}</span></span>
<span id="cb53-886"><a href="#cb53-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-887"><a href="#cb53-887" aria-hidden="true" tabindex="-1"></a>Here's a slightly more complicated calculation of the mean total cup points for each country. We group by country before calculating the mean to return six numbers. So how do the numbers from the simple random sample compare? The sample means are pretty close to the population means.</span>
<span id="cb53-888"><a href="#cb53-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-889"><a href="#cb53-889" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Mean cup points by country: stratified {.unnumbered}</span></span>
<span id="cb53-890"><a href="#cb53-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-891"><a href="#cb53-891" aria-hidden="true" tabindex="-1"></a>The same is true of the sample means from the stratified technique. Each sample mean is relatively close to the population mean.</span>
<span id="cb53-892"><a href="#cb53-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-893"><a href="#cb53-893" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Mean cup points by country: cluster {.unnumbered}</span></span>
<span id="cb53-894"><a href="#cb53-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-895"><a href="#cb53-895" aria-hidden="true" tabindex="-1"></a>With cluster sampling, while the sample means are pretty close to the population means, the obvious limitation is that we only get values for the two countries that were included in the sample. If the mean cup points for each country is an important metric in our analysis, cluster sampling would be a bad idea.</span>
<span id="cb53-896"><a href="#cb53-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-897"><a href="#cb53-897" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.4.1</span></span>
<span id="cb53-898"><a href="#cb53-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-899"><a href="#cb53-899" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 3 kinds of sampling {.unnumbered}</span></span>
<span id="cb53-900"><a href="#cb53-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-901"><a href="#cb53-901" aria-hidden="true" tabindex="-1"></a>You're going to compare the performance of point estimates using simple, stratified, and cluster sampling. Before doing that, you'll have to set up the samples.</span>
<span id="cb53-902"><a href="#cb53-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-903"><a href="#cb53-903" aria-hidden="true" tabindex="-1"></a>You'll use the <span class="in">`RelationshipSatisfaction`</span> column of the <span class="in">`attrition`</span> dataset, which categorizes the employee's relationship with the company. It has four levels: <span class="in">`Low`</span>, <span class="in">`Medium`</span>, <span class="in">`High`</span>, and <span class="in">`Very_High`</span>. </span>
<span id="cb53-904"><a href="#cb53-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-905"><a href="#cb53-905" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-906"><a href="#cb53-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-907"><a href="#cb53-907" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Perform simple random sampling on <span class="in">`attrition`</span> to get one-quarter of the population, setting the seed to <span class="in">`2022`</span>.</span>
<span id="cb53-908"><a href="#cb53-908" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Perform stratified sampling on <span class="in">`attrition`</span> to sample one-quarter of each <span class="in">`RelationshipSatisfaction`</span> group, setting the seed to <span class="in">`2022`</span>.</span>
<span id="cb53-909"><a href="#cb53-909" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Create a list of unique values from <span class="in">`attrition`</span>'s <span class="in">`RelationshipSatisfaction`</span> column.</span>
<span id="cb53-910"><a href="#cb53-910" aria-hidden="true" tabindex="-1"></a>Randomly sample <span class="in">`satisfaction_unique`</span> to get two values.</span>
<span id="cb53-911"><a href="#cb53-911" aria-hidden="true" tabindex="-1"></a>Subset the population for rows where <span class="in">`RelationshipSatisfaction`</span> is in <span class="in">`satisfaction_samp`</span> and clear any unused categories from <span class="in">`RelationshipSatisfaction`</span>; assign to <span class="in">`attrition_clust_prep`</span>.</span>
<span id="cb53-912"><a href="#cb53-912" aria-hidden="true" tabindex="-1"></a>Perform cluster sampling on the selected satisfaction groups, sampling one quarter of the *population* and setting the seed to <span class="in">`2022`</span>.</span>
<span id="cb53-913"><a href="#cb53-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-916"><a href="#cb53-916" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-917"><a href="#cb53-917" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-918"><a href="#cb53-918" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-919"><a href="#cb53-919" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-920"><a href="#cb53-920" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-921"><a href="#cb53-921" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-922"><a href="#cb53-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-923"><a href="#cb53-923" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-924"><a href="#cb53-924" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb53-925"><a href="#cb53-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-926"><a href="#cb53-926" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform simple random sampling to get 0.25 of the population</span></span>
<span id="cb53-927"><a href="#cb53-927" aria-hidden="true" tabindex="-1"></a>attrition_srs <span class="op">=</span> attrition.sample(frac<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb53-928"><a href="#cb53-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-929"><a href="#cb53-929" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform stratified sampling to get 0.25 of each relationship group</span></span>
<span id="cb53-930"><a href="#cb53-930" aria-hidden="true" tabindex="-1"></a>attrition_strat <span class="op">=</span> attrition.groupby(<span class="st">'RelationshipSatisfaction'</span>)<span class="op">\</span></span>
<span id="cb53-931"><a href="#cb53-931" aria-hidden="true" tabindex="-1"></a>.sample(frac<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb53-932"><a href="#cb53-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-933"><a href="#cb53-933" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of unique RelationshipSatisfaction values</span></span>
<span id="cb53-934"><a href="#cb53-934" aria-hidden="true" tabindex="-1"></a>satisfaction_unique <span class="op">=</span> <span class="bu">list</span>(attrition[<span class="st">'RelationshipSatisfaction'</span>].unique())</span>
<span id="cb53-935"><a href="#cb53-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-936"><a href="#cb53-936" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly sample 2 unique satisfaction values</span></span>
<span id="cb53-937"><a href="#cb53-937" aria-hidden="true" tabindex="-1"></a>satisfaction_samp <span class="op">=</span> random.sample(satisfaction_unique, k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb53-938"><a href="#cb53-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-939"><a href="#cb53-939" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter for satisfaction_samp and clear unused categories from RelationshipSatisfaction</span></span>
<span id="cb53-940"><a href="#cb53-940" aria-hidden="true" tabindex="-1"></a>satis_condition <span class="op">=</span> attrition[<span class="st">'RelationshipSatisfaction'</span>].isin(satisfaction_samp)</span>
<span id="cb53-941"><a href="#cb53-941" aria-hidden="true" tabindex="-1"></a>attrition_clust_prep <span class="op">=</span> attrition[satis_condition]</span>
<span id="cb53-942"><a href="#cb53-942" aria-hidden="true" tabindex="-1"></a>attrition_clust_prep[<span class="st">'RelationshipSatisfaction'</span>] <span class="op">=</span> attrition_clust_prep[<span class="st">'RelationshipSatisfaction'</span>].cat.remove_unused_categories()</span>
<span id="cb53-943"><a href="#cb53-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-944"><a href="#cb53-944" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform cluster sampling on the selected group, getting 0.25 of attrition_clust_prep</span></span>
<span id="cb53-945"><a href="#cb53-945" aria-hidden="true" tabindex="-1"></a>attrition_clust <span class="op">=</span> attrition_clust_prep.groupby(<span class="st">"RelationshipSatisfaction"</span>)<span class="op">\</span></span>
<span id="cb53-946"><a href="#cb53-946" aria-hidden="true" tabindex="-1"></a>.sample(n<span class="op">=</span><span class="bu">len</span>(attrition) <span class="op">//</span> <span class="dv">6</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb53-947"><a href="#cb53-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-948"><a href="#cb53-948" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attrition_clust)</span>
<span id="cb53-949"><a href="#cb53-949" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-950"><a href="#cb53-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-951"><a href="#cb53-951" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.4.4</span></span>
<span id="cb53-952"><a href="#cb53-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-953"><a href="#cb53-953" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Comparing point estimates {.unnumbered}</span></span>
<span id="cb53-954"><a href="#cb53-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-955"><a href="#cb53-955" aria-hidden="true" tabindex="-1"></a>Now that you have three types of sample (simple, stratified, and cluster), you can compare point estimates from each sample to the population parameter. That is, you can calculate the same summary statistic on each sample and see how it compares to the summary statistic for the population.</span>
<span id="cb53-956"><a href="#cb53-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-957"><a href="#cb53-957" aria-hidden="true" tabindex="-1"></a>Here, we'll look at how satisfaction with the company affects whether or not the employee leaves the company. That is, you'll calculate the proportion of employees who left the company (they have an Attrition value of <span class="in">`1`</span>) for each value of <span class="in">`RelationshipSatisfaction`</span>.</span>
<span id="cb53-958"><a href="#cb53-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-959"><a href="#cb53-959" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-960"><a href="#cb53-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-961"><a href="#cb53-961" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Group <span class="in">`attrition`</span> by <span class="in">`RelationshipSatisfaction`</span> levels and calculate the mean of Attrition for each level.</span>
<span id="cb53-962"><a href="#cb53-962" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the simple random sample, <span class="in">`attrition_srs`</span>.</span>
<span id="cb53-963"><a href="#cb53-963" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the stratified sample, <span class="in">`attrition_strat`</span>.</span>
<span id="cb53-964"><a href="#cb53-964" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Calculate the proportion of employee attrition for each relationship satisfaction group, this time on the cluster sample, <span class="in">`attrition_clust`</span>.</span>
<span id="cb53-965"><a href="#cb53-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-968"><a href="#cb53-968" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-969"><a href="#cb53-969" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-970"><a href="#cb53-970" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-971"><a href="#cb53-971" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-972"><a href="#cb53-972" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-973"><a href="#cb53-973" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-974"><a href="#cb53-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-975"><a href="#cb53-975" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-976"><a href="#cb53-976" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb53-977"><a href="#cb53-977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-978"><a href="#cb53-978" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform simple random sampling to get 0.25 of the population</span></span>
<span id="cb53-979"><a href="#cb53-979" aria-hidden="true" tabindex="-1"></a>attrition_srs <span class="op">=</span> attrition.sample(frac<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb53-980"><a href="#cb53-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-981"><a href="#cb53-981" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform stratified sampling to get 0.25 of each relationship group</span></span>
<span id="cb53-982"><a href="#cb53-982" aria-hidden="true" tabindex="-1"></a>attrition_strat <span class="op">=</span> attrition.groupby(<span class="st">'RelationshipSatisfaction'</span>)<span class="op">\</span></span>
<span id="cb53-983"><a href="#cb53-983" aria-hidden="true" tabindex="-1"></a>.sample(frac<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb53-984"><a href="#cb53-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-985"><a href="#cb53-985" aria-hidden="true" tabindex="-1"></a><span class="co"># Mean Attrition by RelationshipSatisfaction group</span></span>
<span id="cb53-986"><a href="#cb53-986" aria-hidden="true" tabindex="-1"></a>mean_attrition_pop <span class="op">=</span> attrition.groupby(<span class="st">'RelationshipSatisfaction'</span>)<span class="op">\</span></span>
<span id="cb53-987"><a href="#cb53-987" aria-hidden="true" tabindex="-1"></a>[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-988"><a href="#cb53-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-989"><a href="#cb53-989" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb53-990"><a href="#cb53-990" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_attrition_pop)</span>
<span id="cb53-991"><a href="#cb53-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-992"><a href="#cb53-992" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the same thing for the simple random sample </span></span>
<span id="cb53-993"><a href="#cb53-993" aria-hidden="true" tabindex="-1"></a>mean_attrition_srs <span class="op">=</span> attrition_srs.groupby(<span class="st">'RelationshipSatisfaction'</span>)<span class="op">\</span></span>
<span id="cb53-994"><a href="#cb53-994" aria-hidden="true" tabindex="-1"></a>[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-995"><a href="#cb53-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-996"><a href="#cb53-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-997"><a href="#cb53-997" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb53-998"><a href="#cb53-998" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_attrition_srs)</span>
<span id="cb53-999"><a href="#cb53-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1000"><a href="#cb53-1000" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the same thing for the stratified sample </span></span>
<span id="cb53-1001"><a href="#cb53-1001" aria-hidden="true" tabindex="-1"></a>mean_attrition_strat <span class="op">=</span> attrition_strat.groupby(<span class="st">'RelationshipSatisfaction'</span>)<span class="op">\</span></span>
<span id="cb53-1002"><a href="#cb53-1002" aria-hidden="true" tabindex="-1"></a>[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-1003"><a href="#cb53-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1004"><a href="#cb53-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1005"><a href="#cb53-1005" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb53-1006"><a href="#cb53-1006" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_attrition_strat)</span>
<span id="cb53-1007"><a href="#cb53-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1008"><a href="#cb53-1008" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the same thing for the cluster sample </span></span>
<span id="cb53-1009"><a href="#cb53-1009" aria-hidden="true" tabindex="-1"></a>mean_attrition_clust <span class="op">=</span> attrition_clust.groupby(<span class="st">'RelationshipSatisfaction'</span>)<span class="op">\</span></span>
<span id="cb53-1010"><a href="#cb53-1010" aria-hidden="true" tabindex="-1"></a>[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-1011"><a href="#cb53-1011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1012"><a href="#cb53-1012" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb53-1013"><a href="#cb53-1013" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_attrition_clust)</span>
<span id="cb53-1014"><a href="#cb53-1014" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1015"><a href="#cb53-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1016"><a href="#cb53-1016" aria-hidden="true" tabindex="-1"></a><span class="fu">## CHAPTER 3: Sampling Distributions</span></span>
<span id="cb53-1017"><a href="#cb53-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1018"><a href="#cb53-1018" aria-hidden="true" tabindex="-1"></a>Let’s test your sampling. In this chapter, you’ll discover how to quantify the accuracy of sample statistics using relative errors, and measure variation in your estimates by generating sampling distributions.</span>
<span id="cb53-1019"><a href="#cb53-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1020"><a href="#cb53-1020" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 3.1: Relative error of point estimates</span></span>
<span id="cb53-1021"><a href="#cb53-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1022"><a href="#cb53-1022" aria-hidden="true" tabindex="-1"></a>Let's see how the size of the sample affects the accuracy of the point estimates we calculate.</span>
<span id="cb53-1023"><a href="#cb53-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1024"><a href="#cb53-1024" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Sample size is number of rows {.unnumbered}</span></span>
<span id="cb53-1025"><a href="#cb53-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1026"><a href="#cb53-1026" aria-hidden="true" tabindex="-1"></a>The sample size, calculated here with the <span class="in">`len`</span> function, is the number of observations, that is, the number of rows in the sample. That's true whichever method we use to create the sample. We'll stick to looking at simple random sampling since it works well in most cases and it's easier to reason about.</span>
<span id="cb53-1027"><a href="#cb53-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1028"><a href="#cb53-1028" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Various sample sizes {.unnumbered}</span></span>
<span id="cb53-1029"><a href="#cb53-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1030"><a href="#cb53-1030" aria-hidden="true" tabindex="-1"></a>Let's calculate a population parameter, the mean cup points of the coffees. It's around eighty-two-point-one-five. This is our gold standard to compare against. If we take a sample size of ten, the point estimate of this parameter is wrong by about point-eight-eight. Increasing the sample size to one hundred gets us closer; the estimate is only wrong by about point-three-four. Increasing the sample size further to one thousand brings the estimate to about point-zero-three away from the population parameter. In general, larger sample sizes will give us more accurate results.</span>
<span id="cb53-1031"><a href="#cb53-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1032"><a href="#cb53-1032" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Relative errors {.unnumbered}</span></span>
<span id="cb53-1033"><a href="#cb53-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1034"><a href="#cb53-1034" aria-hidden="true" tabindex="-1"></a>For any of these sample sizes, we want to compare the population mean to the sample mean. This is the same code we just saw, but with the numerical sample size replaced with a variable named <span class="in">`sample_size`</span>. The most common metric for assessing the difference between the population and a sample mean is the relative error. The relative error is the absolute difference between the two numbers; that is, we ignore any minus signs, divided by the population mean. Here, we also multiply by one hundred to make it a percentage.</span>
<span id="cb53-1035"><a href="#cb53-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1036"><a href="#cb53-1036" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Relative error vs. sample size {.unnumbered}</span></span>
<span id="cb53-1037"><a href="#cb53-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1038"><a href="#cb53-1038" aria-hidden="true" tabindex="-1"></a>Here's a line plot of relative error versus sample size. We see that the relative error decreases as the sample size increases, and beyond that, the plot has other important properties. Firstly, the blue line is really noisy, particularly for small sample sizes. If our sample size is small, the sample mean we calculate can be wildly different by adding one or two more random rows to the sample. Secondly, the amplitude of the line is quite steep, to begin with. When we have a small sample size, adding just a few more samples can give us much better accuracy. Further to the right of the plot, the line is less steep. If we already have a large sample size, adding a few more rows to the sample doesn't bring as much benefit. Finally, at the far right of the plot, where the sample size is the whole population, the relative error decreases to zero.</span>
<span id="cb53-1039"><a href="#cb53-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1040"><a href="#cb53-1040" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.1.1</span></span>
<span id="cb53-1041"><a href="#cb53-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1042"><a href="#cb53-1042" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Calculating relative errors {.unnumbered}</span></span>
<span id="cb53-1043"><a href="#cb53-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1044"><a href="#cb53-1044" aria-hidden="true" tabindex="-1"></a>The size of the sample you take affects how accurately the point estimates reflect the corresponding population parameter. For example, when you calculate a sample mean, you want it to be close to the population mean. However, if your sample is too small, this might not be the case.</span>
<span id="cb53-1045"><a href="#cb53-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1046"><a href="#cb53-1046" aria-hidden="true" tabindex="-1"></a>The most common metric for assessing accuracy is *relative error*. This is the absolute difference between the population parameter and the point estimate, all divided by the population parameter. It is sometimes expressed as a percentage.</span>
<span id="cb53-1047"><a href="#cb53-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1048"><a href="#cb53-1048" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-1049"><a href="#cb53-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1050"><a href="#cb53-1050" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Generate a simple random sample from attrition_pop of fifty rows, setting the seed to 2022.</span>
<span id="cb53-1051"><a href="#cb53-1051" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate the mean employee <span class="in">`Attrition`</span> in the sample.</span>
<span id="cb53-1052"><a href="#cb53-1052" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate the relative error between <span class="in">`mean_attrition_srs50`</span> and <span class="in">`mean_attrition_pop`</span> as a *percentage*.</span>
<span id="cb53-1053"><a href="#cb53-1053" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Calculate the *relative error percentage* again. This time, use a simple random sample of one hundred rows of <span class="in">`attrition`</span>.</span>
<span id="cb53-1054"><a href="#cb53-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1057"><a href="#cb53-1057" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-1058"><a href="#cb53-1058" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-1059"><a href="#cb53-1059" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-1060"><a href="#cb53-1060" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-1061"><a href="#cb53-1061" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-1062"><a href="#cb53-1062" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-1063"><a href="#cb53-1063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1064"><a href="#cb53-1064" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-1065"><a href="#cb53-1065" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb53-1066"><a href="#cb53-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1067"><a href="#cb53-1067" aria-hidden="true" tabindex="-1"></a><span class="co"># Population Attrtion mean </span></span>
<span id="cb53-1068"><a href="#cb53-1068" aria-hidden="true" tabindex="-1"></a>mean_attrition_pop <span class="op">=</span> attrition[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-1069"><a href="#cb53-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1070"><a href="#cb53-1070" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb53-1071"><a href="#cb53-1071" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_attrition_pop)</span>
<span id="cb53-1072"><a href="#cb53-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1073"><a href="#cb53-1073" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a simple random sample of 50 rows, with seed 2022</span></span>
<span id="cb53-1074"><a href="#cb53-1074" aria-hidden="true" tabindex="-1"></a>attrition_srs50 <span class="op">=</span> attrition.sample(n<span class="op">=</span><span class="dv">50</span>, random_state <span class="op">=</span> <span class="dv">2022</span>)</span>
<span id="cb53-1075"><a href="#cb53-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1076"><a href="#cb53-1076" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean employee attrition in the sample</span></span>
<span id="cb53-1077"><a href="#cb53-1077" aria-hidden="true" tabindex="-1"></a>mean_attrition_srs50 <span class="op">=</span> attrition_srs50[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-1078"><a href="#cb53-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1079"><a href="#cb53-1079" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the relative error percentage</span></span>
<span id="cb53-1080"><a href="#cb53-1080" aria-hidden="true" tabindex="-1"></a>rel_error_pct50 <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> <span class="bu">abs</span>(mean_attrition_pop <span class="op">-</span> mean_attrition_srs50)<span class="op">/</span>mean_attrition_pop</span>
<span id="cb53-1081"><a href="#cb53-1081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1082"><a href="#cb53-1082" aria-hidden="true" tabindex="-1"></a><span class="co"># Print rel_error_pct50</span></span>
<span id="cb53-1083"><a href="#cb53-1083" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rel_error_pct50)</span>
<span id="cb53-1084"><a href="#cb53-1084" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1085"><a href="#cb53-1085" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a simple random sample of 100 rows, with seed 2022</span></span>
<span id="cb53-1086"><a href="#cb53-1086" aria-hidden="true" tabindex="-1"></a>attrition_srs100 <span class="op">=</span> attrition.sample(n<span class="op">=</span><span class="dv">100</span>, random_state <span class="op">=</span> <span class="dv">2022</span>)</span>
<span id="cb53-1087"><a href="#cb53-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1088"><a href="#cb53-1088" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean employee attrition in the sample</span></span>
<span id="cb53-1089"><a href="#cb53-1089" aria-hidden="true" tabindex="-1"></a>mean_attrition_srs100 <span class="op">=</span> attrition_srs100[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-1090"><a href="#cb53-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1091"><a href="#cb53-1091" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the relative error percentage</span></span>
<span id="cb53-1092"><a href="#cb53-1092" aria-hidden="true" tabindex="-1"></a>rel_error_pct100 <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> <span class="bu">abs</span>(mean_attrition_pop <span class="op">-</span> mean_attrition_srs100)<span class="op">/</span>mean_attrition_pop</span>
<span id="cb53-1093"><a href="#cb53-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1094"><a href="#cb53-1094" aria-hidden="true" tabindex="-1"></a><span class="co"># Print rel_error_pct100</span></span>
<span id="cb53-1095"><a href="#cb53-1095" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(rel_error_pct100)</span>
<span id="cb53-1096"><a href="#cb53-1096" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1097"><a href="#cb53-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1098"><a href="#cb53-1098" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 3.2: Creating a sampling distribution</span></span>
<span id="cb53-1099"><a href="#cb53-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1100"><a href="#cb53-1100" aria-hidden="true" tabindex="-1"></a>We just saw how point estimates like the sample mean will vary depending on which rows end up in the sample.</span>
<span id="cb53-1101"><a href="#cb53-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1102"><a href="#cb53-1102" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Same code, different answer {.unnumbered}</span></span>
<span id="cb53-1103"><a href="#cb53-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1104"><a href="#cb53-1104" aria-hidden="true" tabindex="-1"></a>For example, this same code to calculate the mean cup points from a simple random sample of thirty coffees gives a slightly different answer each time. Let's try to visualize and quantify this variation.</span>
<span id="cb53-1105"><a href="#cb53-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1106"><a href="#cb53-1106" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Same code, 1000 times {.unnumbered}</span></span>
<span id="cb53-1107"><a href="#cb53-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1108"><a href="#cb53-1108" aria-hidden="true" tabindex="-1"></a>A for loop lets us run the same code many times. It's especially useful for situations like this where the result contains some randomness. We start by creating an empty list to store the means. Then, we set up the for loop to repeatedly sample 30 coffees from coffee_ratings a total of 1000 times, calculating the mean cup points each time. After each calculation, we append the result, also called a replicate, to the list. Each time the code is run, we get one sample mean, so running the code a thousand times generates a list of one thousand sample means.</span>
<span id="cb53-1109"><a href="#cb53-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1110"><a href="#cb53-1110" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Distribution of sample means for size 30 {.unnumbered}</span></span>
<span id="cb53-1111"><a href="#cb53-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1112"><a href="#cb53-1112" aria-hidden="true" tabindex="-1"></a>The one thousand sample means form a distribution of sample means. To visualize a distribution, the best plot is often a histogram. Here we can see that most of the results lie between eighty-one and eighty-three, and they roughly follow a bell-shaped curve, like a normal distribution. There's an important piece of jargon we need to know here. A distribution of replicates of sample means, or other point estimates, is known as a sampling distribution.</span>
<span id="cb53-1113"><a href="#cb53-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1114"><a href="#cb53-1114" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Different sample sizes {.unnumbered}</span></span>
<span id="cb53-1115"><a href="#cb53-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1116"><a href="#cb53-1116" aria-hidden="true" tabindex="-1"></a>Here are histograms from running the same code again with different sample sizes. When we decrease the original sample size of thirty to six, we can see from the x-values that the range of the results is broader. The bulk of the results now lie between eighty and eighty-four. On the other hand, increasing the sample size to one hundred and fifty results in a much narrower range. Now most of the results are between eighty-one-point-eight and eighty-two-point-six. As we saw previously, bigger sample sizes give us more accurate results. By replicating the sampling many times, as we've done here, we can quantify that accuracy.</span>
<span id="cb53-1117"><a href="#cb53-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1118"><a href="#cb53-1118" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.2.1</span></span>
<span id="cb53-1119"><a href="#cb53-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1120"><a href="#cb53-1120" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Replicating samples {.unnumbered}</span></span>
<span id="cb53-1121"><a href="#cb53-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1122"><a href="#cb53-1122" aria-hidden="true" tabindex="-1"></a>When you calculate a point estimate such as a sample mean, the value you calculate depends on the rows that were included in the sample. That means that there is some randomness in the answer. In order to quantify the variation caused by this randomness, you can create many samples and calculate the sample mean (or another statistic) for each sample.</span>
<span id="cb53-1123"><a href="#cb53-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1124"><a href="#cb53-1124" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-1125"><a href="#cb53-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1126"><a href="#cb53-1126" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Replicate the provided code so that it runs 500 times. Assign the resulting list of sample means to <span class="in">`mean_attritions`</span>.</span>
<span id="cb53-1127"><a href="#cb53-1127" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Draw a histogram of the <span class="in">`mean_attritions`</span> list with 16 bins.</span>
<span id="cb53-1128"><a href="#cb53-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1131"><a href="#cb53-1131" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-1132"><a href="#cb53-1132" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-1133"><a href="#cb53-1133" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-1134"><a href="#cb53-1134" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-1135"><a href="#cb53-1135" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-1136"><a href="#cb53-1136" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-1137"><a href="#cb53-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1138"><a href="#cb53-1138" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-1139"><a href="#cb53-1139" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb53-1140"><a href="#cb53-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1141"><a href="#cb53-1141" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an empty list</span></span>
<span id="cb53-1142"><a href="#cb53-1142" aria-hidden="true" tabindex="-1"></a>mean_attritions <span class="op">=</span> []</span>
<span id="cb53-1143"><a href="#cb53-1143" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop 500 times to create 500 sample means</span></span>
<span id="cb53-1144"><a href="#cb53-1144" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb53-1145"><a href="#cb53-1145" aria-hidden="true" tabindex="-1"></a>    mean_attritions.append(</span>
<span id="cb53-1146"><a href="#cb53-1146" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">60</span>)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-1147"><a href="#cb53-1147" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1148"><a href="#cb53-1148" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-1149"><a href="#cb53-1149" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the first few entries of the list</span></span>
<span id="cb53-1150"><a href="#cb53-1150" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_attritions[<span class="dv">0</span>:<span class="dv">5</span>])</span>
<span id="cb53-1151"><a href="#cb53-1151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1152"><a href="#cb53-1152" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a histogram of the 500 sample means</span></span>
<span id="cb53-1153"><a href="#cb53-1153" aria-hidden="true" tabindex="-1"></a>plt.hist(mean_attritions, bins<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb53-1154"><a href="#cb53-1154" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-1155"><a href="#cb53-1155" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1156"><a href="#cb53-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1157"><a href="#cb53-1157" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 3.3: Approximate sampling distributions</span></span>
<span id="cb53-1158"><a href="#cb53-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1159"><a href="#cb53-1159" aria-hidden="true" tabindex="-1"></a>In the last exercise, we saw that while increasing the number of replicates didn't affect the relative error of the sample means; it did result in a more consistent shape to the distribution.</span>
<span id="cb53-1160"><a href="#cb53-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1161"><a href="#cb53-1161" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 4 dice {.unnumbered}</span></span>
<span id="cb53-1162"><a href="#cb53-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1163"><a href="#cb53-1163" aria-hidden="true" tabindex="-1"></a>Let's consider the case of four six-sided dice rolls. We can generate all possible combinations of rolls using the <span class="in">`expand_grid`</span> function, which is defined in the pandas documentation, and uses the <span class="in">`itertools`</span> package. There are six to the power four, or one-thousand-two-hundred-ninety-six possible dice roll combinations.</span>
<span id="cb53-1164"><a href="#cb53-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1165"><a href="#cb53-1165" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Mean roll {.unnumbered}</span></span>
<span id="cb53-1166"><a href="#cb53-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1167"><a href="#cb53-1167" aria-hidden="true" tabindex="-1"></a>Let's consider the mean of the four rolls by adding a column to our DataFrame called <span class="in">`mean_roll`</span>. <span class="in">`mean_roll`</span> ranges from 1, when four ones are rolled, to 6, when four sixes are rolled.</span>
<span id="cb53-1168"><a href="#cb53-1168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1169"><a href="#cb53-1169" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Exact sampling distribution {.unnumbered}</span></span>
<span id="cb53-1170"><a href="#cb53-1170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1171"><a href="#cb53-1171" aria-hidden="true" tabindex="-1"></a>Since the mean roll takes discrete values instead of continuous values, the best way to see the distribution of mean_roll is to draw a bar plot. First, we convert mean_roll to a categorical by setting its type to category. We are interested in the counts of each value, so we use dot-<span class="in">`value_counts`</span>, passing the sort equals False argument. This ensures the x-axis ranges from one to six instead of sorting the bars by frequency. Chaining <span class="in">`.plot`</span> to <span class="in">`value_counts`</span>, and setting kind to <span class="in">`"bar"`</span>, produces a bar plot of the mean roll distribution. This is the exact sampling distribution of the mean roll because it contains every single combination of die rolls.</span>
<span id="cb53-1172"><a href="#cb53-1172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1173"><a href="#cb53-1173" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The number of outcomes increases fast {.unnumbered}</span></span>
<span id="cb53-1174"><a href="#cb53-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1175"><a href="#cb53-1175" aria-hidden="true" tabindex="-1"></a>If we increase the number of dice in our scenario, the number of possible outcomes increases by a factor of six each time. These values can be shown by creating a DataFrame with two columns: <span class="in">`n_dice`</span>, ranging from 1 to 100, and <span class="in">`n_outcomes`</span>, which is the number of possible outcomes, calculated using six to the power of the number of dice. With just one hundred dice, the number of outcomes is about the same as the number of atoms in the universe: six-point-five times ten to the seventy-seventh power. Long before you start dealing with big datasets, it becomes computationally impossible to calculate the exact sampling distribution. That means we need to rely on approximations.</span>
<span id="cb53-1176"><a href="#cb53-1176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1177"><a href="#cb53-1177" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Simulating the mean of four dice rolls {.unnumbered}</span></span>
<span id="cb53-1178"><a href="#cb53-1178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1179"><a href="#cb53-1179" aria-hidden="true" tabindex="-1"></a>We can generate a sample mean of four dice rolls using NumPy's <span class="in">`random.choice`</span> method, specifying size as four. This will randomly choose values from a specified list, in this case, four values from the numbers one to six, which is created using a range from one to seven wrapped in the list function. Notice that we set <span class="in">`replace`</span> equals <span class="in">`True`</span> because we can roll the same number several times.</span>
<span id="cb53-1180"><a href="#cb53-1180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1181"><a href="#cb53-1181" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Simulating the mean of four dice rolls {.unnumbered}</span></span>
<span id="cb53-1182"><a href="#cb53-1182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1183"><a href="#cb53-1183" aria-hidden="true" tabindex="-1"></a>Then we use a for loop to generate lots of sample means, in this case, one thousand. We again use the <span class="in">`.append`</span> method to populate the sample means list with our simulated sample means. The output contains a sampling of many of the same values we saw with the exact sampling distribution.</span>
<span id="cb53-1184"><a href="#cb53-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1185"><a href="#cb53-1185" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Approximate sampling distribution {.unnumbered}</span></span>
<span id="cb53-1186"><a href="#cb53-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1187"><a href="#cb53-1187" aria-hidden="true" tabindex="-1"></a>Here's a histogram of the approximate sampling distribution of mean rolls. This time, it uses the simulated rather than the exact values. It's known as an approximate sampling distribution. Notice that although it isn't perfect, it's pretty close to the exact sampling distribution. Usually, we don't have access to the whole population, so we can't calculate the exact sampling distribution. However, we can feel relatively confident that using an approximation will provide a good guess as to how the sampling distribution will behave.</span>
<span id="cb53-1188"><a href="#cb53-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1189"><a href="#cb53-1189" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.3.1</span></span>
<span id="cb53-1190"><a href="#cb53-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1191"><a href="#cb53-1191" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Exact sampling distribution {.unnumbered}</span></span>
<span id="cb53-1192"><a href="#cb53-1192" aria-hidden="true" tabindex="-1"></a>To quantify how the point estimate (sample statistic) you are interested in varies, you need to know all the possible values it can take and how often. That is, you need to know its distribution.</span>
<span id="cb53-1193"><a href="#cb53-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1194"><a href="#cb53-1194" aria-hidden="true" tabindex="-1"></a>The distribution of a sample statistic is called the *sampling distribution*. When we can calculate this exactly, rather than using an approximation, it is known as the *exact sampling distribution*.</span>
<span id="cb53-1195"><a href="#cb53-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1196"><a href="#cb53-1196" aria-hidden="true" tabindex="-1"></a>Let's take another look at the sampling distribution of dice rolls. This time, we'll look at five eight-sided dice. (These have the numbers one to eight.)</span>
<span id="cb53-1197"><a href="#cb53-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1198"><a href="#cb53-1198" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-1199"><a href="#cb53-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1200"><a href="#cb53-1200" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Expand a grid representing 5 8-sided dice. That is, create a DataFrame with five columns from a dictionary, named <span class="in">`die1`</span> to <span class="in">`die5`</span>. The rows should contain all possibilities for throwing five dice, each numbered <span class="in">`1`</span> to <span class="in">`8`</span>.</span>
<span id="cb53-1201"><a href="#cb53-1201" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Add a column, <span class="in">`mean_roll`</span>, to dice, that contains the mean of the five rolls as a categorical.</span>
<span id="cb53-1202"><a href="#cb53-1202" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Create a bar plot of the <span class="in">`mean_roll`</span> categorical column, so it displays the count of each <span class="in">`mean_roll`</span> in increasing order from <span class="in">`1.0`</span> to <span class="in">`8.0`</span>.</span>
<span id="cb53-1203"><a href="#cb53-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1206"><a href="#cb53-1206" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-1207"><a href="#cb53-1207" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-1208"><a href="#cb53-1208" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-1209"><a href="#cb53-1209" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-1210"><a href="#cb53-1210" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-1211"><a href="#cb53-1211" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-1212"><a href="#cb53-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1213"><a href="#cb53-1213" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to create a grid of all possible combinations</span></span>
<span id="cb53-1214"><a href="#cb53-1214" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> expand_grid(dictionary):</span>
<span id="cb53-1215"><a href="#cb53-1215" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> itertools <span class="im">import</span> product</span>
<span id="cb53-1216"><a href="#cb53-1216" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame([row <span class="cf">for</span> row <span class="kw">in</span> product(<span class="op">*</span>dictionary.values())], columns<span class="op">=</span>dictionary.keys())</span>
<span id="cb53-1217"><a href="#cb53-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1218"><a href="#cb53-1218" aria-hidden="true" tabindex="-1"></a><span class="co"># Expand a grid representing 5 8-sided dice</span></span>
<span id="cb53-1219"><a href="#cb53-1219" aria-hidden="true" tabindex="-1"></a>dice <span class="op">=</span> expand_grid(</span>
<span id="cb53-1220"><a href="#cb53-1220" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'die1'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>),</span>
<span id="cb53-1221"><a href="#cb53-1221" aria-hidden="true" tabindex="-1"></a>     <span class="st">'die2'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>),</span>
<span id="cb53-1222"><a href="#cb53-1222" aria-hidden="true" tabindex="-1"></a>     <span class="st">'die3'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>),</span>
<span id="cb53-1223"><a href="#cb53-1223" aria-hidden="true" tabindex="-1"></a>     <span class="st">'die4'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>),</span>
<span id="cb53-1224"><a href="#cb53-1224" aria-hidden="true" tabindex="-1"></a>     <span class="st">'die5'</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>)}</span>
<span id="cb53-1225"><a href="#cb53-1225" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb53-1226"><a href="#cb53-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1227"><a href="#cb53-1227" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb53-1228"><a href="#cb53-1228" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dice)</span>
<span id="cb53-1229"><a href="#cb53-1229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1230"><a href="#cb53-1230" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a column of mean rolls and convert to a categorical</span></span>
<span id="cb53-1231"><a href="#cb53-1231" aria-hidden="true" tabindex="-1"></a>dice[<span class="st">'mean_roll'</span>] <span class="op">=</span> (dice[<span class="st">'die1'</span>]<span class="op">+</span> dice[<span class="st">'die2'</span>]<span class="op">+</span> dice[<span class="st">'die3'</span>]<span class="op">+</span> dice[<span class="st">'die4'</span>]<span class="op">+</span> dice[<span class="st">'die5'</span>])<span class="op">/</span><span class="dv">5</span></span>
<span id="cb53-1232"><a href="#cb53-1232" aria-hidden="true" tabindex="-1"></a>                     </span>
<span id="cb53-1233"><a href="#cb53-1233" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb53-1234"><a href="#cb53-1234" aria-hidden="true" tabindex="-1"></a>dice[<span class="st">'mean_roll'</span>] <span class="op">=</span> dice[<span class="st">'mean_roll'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb53-1235"><a href="#cb53-1235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1236"><a href="#cb53-1236" aria-hidden="true" tabindex="-1"></a><span class="co"># Print result</span></span>
<span id="cb53-1237"><a href="#cb53-1237" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dice)</span>
<span id="cb53-1238"><a href="#cb53-1238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1239"><a href="#cb53-1239" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a bar plot of mean_roll</span></span>
<span id="cb53-1240"><a href="#cb53-1240" aria-hidden="true" tabindex="-1"></a>dice[<span class="st">'mean_roll'</span>].value_counts(sort<span class="op">=</span><span class="va">False</span>).plot(kind<span class="op">=</span><span class="st">'bar'</span>)</span>
<span id="cb53-1241"><a href="#cb53-1241" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-1242"><a href="#cb53-1242" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1243"><a href="#cb53-1243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1244"><a href="#cb53-1244" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.3.2</span></span>
<span id="cb53-1245"><a href="#cb53-1245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1246"><a href="#cb53-1246" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Generating an approximate sampling distribution {.unnumbered}</span></span>
<span id="cb53-1247"><a href="#cb53-1247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1248"><a href="#cb53-1248" aria-hidden="true" tabindex="-1"></a>Calculating the exact sampling distribution is only possible in very simple situations. With just five eight-sided dice, the number of possible rolls is <span class="in">`8**5`</span>, which is over thirty thousand. When the dataset is more complicated, for example, where a variable has hundreds or thousands of categories, the number of possible outcomes becomes too difficult to compute exactly.</span>
<span id="cb53-1249"><a href="#cb53-1249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1250"><a href="#cb53-1250" aria-hidden="true" tabindex="-1"></a>In this situation, you can calculate an *approximate sampling distribution* by simulating the exact sampling distribution. That is, you can repeat a procedure over and over again to simulate both the sampling process and the sample statistic calculation process.</span>
<span id="cb53-1251"><a href="#cb53-1251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1252"><a href="#cb53-1252" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-1253"><a href="#cb53-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1254"><a href="#cb53-1254" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sample one to eight, five times, with replacement. Assign to <span class="in">`five_rolls`</span>.</span>
<span id="cb53-1255"><a href="#cb53-1255" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Calculate the mean of <span class="in">`five_rolls`</span>.</span>
<span id="cb53-1256"><a href="#cb53-1256" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Replicate the sampling code 1000 times, assigning each result to the list <span class="in">`sample_means_1000`</span>.</span>
<span id="cb53-1257"><a href="#cb53-1257" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Plot <span class="in">`sample_means_1000`</span> as a histogram with 20 bins.</span>
<span id="cb53-1258"><a href="#cb53-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1261"><a href="#cb53-1261" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-1262"><a href="#cb53-1262" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-1263"><a href="#cb53-1263" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-1264"><a href="#cb53-1264" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-1265"><a href="#cb53-1265" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-1266"><a href="#cb53-1266" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-1267"><a href="#cb53-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1268"><a href="#cb53-1268" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample one to eight, five times, with replacement</span></span>
<span id="cb53-1269"><a href="#cb53-1269" aria-hidden="true" tabindex="-1"></a>five_rolls <span class="op">=</span> np.random.choice(<span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>)), size<span class="op">=</span><span class="dv">5</span>, replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb53-1270"><a href="#cb53-1270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1271"><a href="#cb53-1271" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the mean of five_rolls</span></span>
<span id="cb53-1272"><a href="#cb53-1272" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(five_rolls.mean())</span>
<span id="cb53-1273"><a href="#cb53-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1274"><a href="#cb53-1274" aria-hidden="true" tabindex="-1"></a><span class="co"># Replicate the sampling code 1000 times</span></span>
<span id="cb53-1275"><a href="#cb53-1275" aria-hidden="true" tabindex="-1"></a>sample_means_1000 <span class="op">=</span> []</span>
<span id="cb53-1276"><a href="#cb53-1276" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb53-1277"><a href="#cb53-1277" aria-hidden="true" tabindex="-1"></a>    sample_means_1000.append(</span>
<span id="cb53-1278"><a href="#cb53-1278" aria-hidden="true" tabindex="-1"></a>        np.random.choice(<span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">9</span>)), size<span class="op">=</span><span class="dv">5</span>, replace<span class="op">=</span><span class="va">True</span>).mean()</span>
<span id="cb53-1279"><a href="#cb53-1279" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1280"><a href="#cb53-1280" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-1281"><a href="#cb53-1281" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the first 10 entries of the result</span></span>
<span id="cb53-1282"><a href="#cb53-1282" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample_means_1000[<span class="dv">0</span>:<span class="dv">10</span>])</span>
<span id="cb53-1283"><a href="#cb53-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1284"><a href="#cb53-1284" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a histogram of sample_means_1000 with 20 bins</span></span>
<span id="cb53-1285"><a href="#cb53-1285" aria-hidden="true" tabindex="-1"></a>plt.hist(sample_means_1000, bins<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb53-1286"><a href="#cb53-1286" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-1287"><a href="#cb53-1287" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1288"><a href="#cb53-1288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1289"><a href="#cb53-1289" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 3.4: Standard errors and the Central Limit Theorem</span></span>
<span id="cb53-1290"><a href="#cb53-1290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1291"><a href="#cb53-1291" aria-hidden="true" tabindex="-1"></a>The Gaussian distribution (also known as the normal distribution) plays an important role in statistics. Its distinctive bell-shaped curve has been cropping up throughout this course.</span>
<span id="cb53-1292"><a href="#cb53-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1293"><a href="#cb53-1293" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Sampling distribution of mean cup points {.unnumbered}</span></span>
<span id="cb53-1294"><a href="#cb53-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1295"><a href="#cb53-1295" aria-hidden="true" tabindex="-1"></a>Here are approximate sampling distributions of the mean cup points from the coffee dataset. Each histogram shows five thousand replicates, with different sample sizes in each case. Look at the x-axis labels. We already saw how increasing the sample size results in greater accuracy in our estimates of the population parameter, so the width of the distribution shrinks as the sample size increases. When the sample size is five, the x-axis ranges from seventy-six to eighty-six, whereas, for a sample size of three hundred and twenty, the range is from eighty-one-point-six to eighty-two-point-six. Now, look at the shape of each distribution. As the sample size increases, we can see that the shape of the curve gets closer and closer to being a normal distribution. At sample size five, the curve is only a very loose approximation since it isn't very symmetric. By sample size eighty, it is a very reasonable approximation.</span>
<span id="cb53-1296"><a href="#cb53-1296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1297"><a href="#cb53-1297" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Consequences of the central limit theorem {.unnumbered}</span></span>
<span id="cb53-1298"><a href="#cb53-1298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1299"><a href="#cb53-1299" aria-hidden="true" tabindex="-1"></a>What we just saw is, in essence, what the central limit theorem tells us. The means of independent samples have normal distributions. Then, as the sample size increases, we see two things. The distribution of these averages gets closer to being normal, and the width of this sampling distribution gets narrower.</span>
<span id="cb53-1300"><a href="#cb53-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1301"><a href="#cb53-1301" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Population &amp; sampling distribution means {.unnumbered}</span></span>
<span id="cb53-1302"><a href="#cb53-1302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1303"><a href="#cb53-1303" aria-hidden="true" tabindex="-1"></a>Recall the population parameter of the mean cup points. We've seen this calculation before, and its value is eighty-two-point-one-five. We can also calculate summary statistics on our sampling distributions to see how they compare. For each of our four sampling distributions, if we take the mean of our sample means, we can see that we get values that are pretty close to the population parameter that the sampling distributions are trying to estimate.</span>
<span id="cb53-1304"><a href="#cb53-1304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1305"><a href="#cb53-1305" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Population &amp; sampling distribution standard deviations {.unnumbered}</span></span>
<span id="cb53-1306"><a href="#cb53-1306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1307"><a href="#cb53-1307" aria-hidden="true" tabindex="-1"></a>Now let's consider the standard deviation of the population cup points. It's about two-point-seven. By comparison, if we take the standard deviation of the sample means from each of the sampling distributions using NumPy, we get much smaller numbers, and they decrease as the sample size increases. Note that when we are calculating a population standard deviation with pandas <span class="in">`.std`</span>, we must specify <span class="in">`ddof`</span> equals zero, as <span class="in">`.std`</span> calculates a sample standard deviation by default. When we are calculating a standard deviation on a sample of the population using NumPy's std function, like in these calculations on the sampling distribution, we must specify a <span class="in">`ddof`</span> of one. So what are these smaller standard deviation values?</span>
<span id="cb53-1308"><a href="#cb53-1308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1309"><a href="#cb53-1309" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Population mean over square root sample size {.unnumbered}</span></span>
<span id="cb53-1310"><a href="#cb53-1310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1311"><a href="#cb53-1311" aria-hidden="true" tabindex="-1"></a>One other consequence of the central limit theorem is that if we divide the population standard deviation, in this case around 2.7, by the square root of the sample size, we get an estimate of the standard deviation of the sampling distribution for that sample size. It isn't exact because of the randomness involved in the sampling process, but it's pretty close.</span>
<span id="cb53-1312"><a href="#cb53-1312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1313"><a href="#cb53-1313" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Standard error {.unnumbered}</span></span>
<span id="cb53-1314"><a href="#cb53-1314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1315"><a href="#cb53-1315" aria-hidden="true" tabindex="-1"></a>We just saw the impact of the sample size on the standard deviation of the sampling distribution. This standard deviation of the sampling distribution has a special name: the standard error. It is useful in a variety of contexts, from estimating population standard deviation to setting expectations on what level of variability we would expect from the sampling process.</span>
<span id="cb53-1316"><a href="#cb53-1316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1317"><a href="#cb53-1317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1318"><a href="#cb53-1318" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.4.1</span></span>
<span id="cb53-1319"><a href="#cb53-1319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1320"><a href="#cb53-1320" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Population &amp; sampling distribution means {.unnumbered}</span></span>
<span id="cb53-1321"><a href="#cb53-1321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1322"><a href="#cb53-1322" aria-hidden="true" tabindex="-1"></a>One of the useful features of sampling distributions is that you can quantify them. Specifically, you can calculate summary statistics on them. Here, you'll look at the relationship between the mean of the sampling distribution and the population parameter's mean.</span>
<span id="cb53-1323"><a href="#cb53-1323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1324"><a href="#cb53-1324" aria-hidden="true" tabindex="-1"></a>Three sampling distributions are provided. For each, the employee attrition dataset was sampled using simple random sampling, then the mean attrition was calculated. This was done 1000 times to get a sampling distribution of mean attritions. One sampling distribution used a sample size of 5 for each replicate, one used 50, and one used 500.</span>
<span id="cb53-1325"><a href="#cb53-1325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1326"><a href="#cb53-1326" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-1327"><a href="#cb53-1327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1328"><a href="#cb53-1328" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Calculate the mean of <span class="in">`sampling_distribution_5`</span>, <span class="in">`sampling_distribution_50`</span>, and <span class="in">`sampling_distribution_500`</span> (a mean of sample means).</span>
<span id="cb53-1329"><a href="#cb53-1329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1332"><a href="#cb53-1332" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-1333"><a href="#cb53-1333" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-1334"><a href="#cb53-1334" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-1335"><a href="#cb53-1335" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-1336"><a href="#cb53-1336" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-1337"><a href="#cb53-1337" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-1338"><a href="#cb53-1338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1339"><a href="#cb53-1339" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-1340"><a href="#cb53-1340" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb53-1341"><a href="#cb53-1341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1342"><a href="#cb53-1342" aria-hidden="true" tabindex="-1"></a><span class="co"># Set a seed for reproducibility</span></span>
<span id="cb53-1343"><a href="#cb53-1343" aria-hidden="true" tabindex="-1"></a>random_seed <span class="op">=</span> <span class="dv">2021</span></span>
<span id="cb53-1344"><a href="#cb53-1344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1345"><a href="#cb53-1345" aria-hidden="true" tabindex="-1"></a><span class="co"># Create three empty lists to hold the sampling distributions</span></span>
<span id="cb53-1346"><a href="#cb53-1346" aria-hidden="true" tabindex="-1"></a>sampling_distribution_5 <span class="op">=</span> []   <span class="co"># Sample size of 5</span></span>
<span id="cb53-1347"><a href="#cb53-1347" aria-hidden="true" tabindex="-1"></a>sampling_distribution_50 <span class="op">=</span> []  <span class="co"># Sample size of 50</span></span>
<span id="cb53-1348"><a href="#cb53-1348" aria-hidden="true" tabindex="-1"></a>sampling_distribution_500 <span class="op">=</span> [] <span class="co"># Sample size of 500</span></span>
<span id="cb53-1349"><a href="#cb53-1349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1350"><a href="#cb53-1350" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform biased sampling and calculate mean attrition 1000 times for each sample size</span></span>
<span id="cb53-1351"><a href="#cb53-1351" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb53-1352"><a href="#cb53-1352" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample size = 5 (heavier weights toward high attrition)</span></span>
<span id="cb53-1353"><a href="#cb53-1353" aria-hidden="true" tabindex="-1"></a>    sampling_distribution_5.append(</span>
<span id="cb53-1354"><a href="#cb53-1354" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span>random_seed <span class="op">+</span> i)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-1355"><a href="#cb53-1355" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1356"><a href="#cb53-1356" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-1357"><a href="#cb53-1357" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample size = 50 (bias reduces as sample size increases)</span></span>
<span id="cb53-1358"><a href="#cb53-1358" aria-hidden="true" tabindex="-1"></a>    sampling_distribution_50.append(</span>
<span id="cb53-1359"><a href="#cb53-1359" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">50</span>, random_state<span class="op">=</span>random_seed <span class="op">+</span> i)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-1360"><a href="#cb53-1360" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1361"><a href="#cb53-1361" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-1362"><a href="#cb53-1362" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample size = 500 (approaching unbiased mean)</span></span>
<span id="cb53-1363"><a href="#cb53-1363" aria-hidden="true" tabindex="-1"></a>    sampling_distribution_500.append(</span>
<span id="cb53-1364"><a href="#cb53-1364" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">500</span>, random_state<span class="op">=</span>random_seed <span class="op">+</span> i)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-1365"><a href="#cb53-1365" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1366"><a href="#cb53-1366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1367"><a href="#cb53-1367" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional: Convert the sampling distributions to DataFrame for analysis</span></span>
<span id="cb53-1368"><a href="#cb53-1368" aria-hidden="true" tabindex="-1"></a>sampling_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb53-1369"><a href="#cb53-1369" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sample_Size_5'</span>: sampling_distribution_5,</span>
<span id="cb53-1370"><a href="#cb53-1370" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sample_Size_50'</span>: sampling_distribution_50,</span>
<span id="cb53-1371"><a href="#cb53-1371" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sample_Size_500'</span>: sampling_distribution_500</span>
<span id="cb53-1372"><a href="#cb53-1372" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb53-1373"><a href="#cb53-1373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1374"><a href="#cb53-1374" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean of the mean attritions for each sampling distribution</span></span>
<span id="cb53-1375"><a href="#cb53-1375" aria-hidden="true" tabindex="-1"></a>mean_of_means_5 <span class="op">=</span> np.mean(sampling_distribution_5)</span>
<span id="cb53-1376"><a href="#cb53-1376" aria-hidden="true" tabindex="-1"></a>mean_of_means_50 <span class="op">=</span> np.mean(sampling_distribution_50)</span>
<span id="cb53-1377"><a href="#cb53-1377" aria-hidden="true" tabindex="-1"></a>mean_of_means_500 <span class="op">=</span> np.mean(sampling_distribution_500)</span>
<span id="cb53-1378"><a href="#cb53-1378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1379"><a href="#cb53-1379" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb53-1380"><a href="#cb53-1380" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_of_means_5)</span>
<span id="cb53-1381"><a href="#cb53-1381" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_of_means_50)</span>
<span id="cb53-1382"><a href="#cb53-1382" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_of_means_500)</span>
<span id="cb53-1383"><a href="#cb53-1383" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1384"><a href="#cb53-1384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1385"><a href="#cb53-1385" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb53-1386"><a href="#cb53-1386" aria-hidden="true" tabindex="-1"></a>*Even for small sample sizes, the mean of the sampling distribution is a good approximation of the population mean.*</span>
<span id="cb53-1387"><a href="#cb53-1387" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1388"><a href="#cb53-1388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1389"><a href="#cb53-1389" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.4.2 </span></span>
<span id="cb53-1390"><a href="#cb53-1390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1391"><a href="#cb53-1391" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Population &amp; sampling distribution variation {.unnumbered}</span></span>
<span id="cb53-1392"><a href="#cb53-1392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1393"><a href="#cb53-1393" aria-hidden="true" tabindex="-1"></a>You just calculated the mean of the sampling distribution and saw how it is an estimate of the corresponding population parameter. Similarly, as a result of the central limit theorem, the standard deviation of the sampling distribution has an interesting relationship with the population parameter's standard deviation and the sample size.</span>
<span id="cb53-1394"><a href="#cb53-1394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1395"><a href="#cb53-1395" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-1396"><a href="#cb53-1396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1397"><a href="#cb53-1397" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Calculate the standard deviation of <span class="in">`sampling_distribution_5`</span>, <span class="in">`sampling_distribution_50`</span>, and <span class="in">`sampling_distribution_500`</span> (a standard deviation of sample means).</span>
<span id="cb53-1398"><a href="#cb53-1398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1401"><a href="#cb53-1401" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-1402"><a href="#cb53-1402" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-1403"><a href="#cb53-1403" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-1404"><a href="#cb53-1404" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-1405"><a href="#cb53-1405" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-1406"><a href="#cb53-1406" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-1407"><a href="#cb53-1407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1408"><a href="#cb53-1408" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb53-1409"><a href="#cb53-1409" aria-hidden="true" tabindex="-1"></a>attrition <span class="op">=</span> pd.read_feather(<span class="st">"datasets/attrition.feather"</span>)</span>
<span id="cb53-1410"><a href="#cb53-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1411"><a href="#cb53-1411" aria-hidden="true" tabindex="-1"></a><span class="co"># Set a seed for reproducibility</span></span>
<span id="cb53-1412"><a href="#cb53-1412" aria-hidden="true" tabindex="-1"></a>random_seed <span class="op">=</span> <span class="dv">2021</span></span>
<span id="cb53-1413"><a href="#cb53-1413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1414"><a href="#cb53-1414" aria-hidden="true" tabindex="-1"></a><span class="co"># Create three empty lists to hold the sampling distributions</span></span>
<span id="cb53-1415"><a href="#cb53-1415" aria-hidden="true" tabindex="-1"></a>sampling_distribution_5 <span class="op">=</span> []   <span class="co"># Sample size of 5</span></span>
<span id="cb53-1416"><a href="#cb53-1416" aria-hidden="true" tabindex="-1"></a>sampling_distribution_50 <span class="op">=</span> []  <span class="co"># Sample size of 50</span></span>
<span id="cb53-1417"><a href="#cb53-1417" aria-hidden="true" tabindex="-1"></a>sampling_distribution_500 <span class="op">=</span> [] <span class="co"># Sample size of 500</span></span>
<span id="cb53-1418"><a href="#cb53-1418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1419"><a href="#cb53-1419" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform biased sampling and calculate mean attrition 1000 times for each sample size</span></span>
<span id="cb53-1420"><a href="#cb53-1420" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb53-1421"><a href="#cb53-1421" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample size = 5 (heavier weights toward high attrition)</span></span>
<span id="cb53-1422"><a href="#cb53-1422" aria-hidden="true" tabindex="-1"></a>    sampling_distribution_5.append(</span>
<span id="cb53-1423"><a href="#cb53-1423" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span>random_seed <span class="op">+</span> i)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-1424"><a href="#cb53-1424" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1425"><a href="#cb53-1425" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-1426"><a href="#cb53-1426" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample size = 50 (bias reduces as sample size increases)</span></span>
<span id="cb53-1427"><a href="#cb53-1427" aria-hidden="true" tabindex="-1"></a>    sampling_distribution_50.append(</span>
<span id="cb53-1428"><a href="#cb53-1428" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">50</span>, random_state<span class="op">=</span>random_seed <span class="op">+</span> i)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-1429"><a href="#cb53-1429" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1430"><a href="#cb53-1430" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-1431"><a href="#cb53-1431" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample size = 500 (approaching unbiased mean)</span></span>
<span id="cb53-1432"><a href="#cb53-1432" aria-hidden="true" tabindex="-1"></a>    sampling_distribution_500.append(</span>
<span id="cb53-1433"><a href="#cb53-1433" aria-hidden="true" tabindex="-1"></a>        attrition.sample(n<span class="op">=</span><span class="dv">500</span>, random_state<span class="op">=</span>random_seed <span class="op">+</span> i)[<span class="st">'Attrition'</span>].mean()</span>
<span id="cb53-1434"><a href="#cb53-1434" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1435"><a href="#cb53-1435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1436"><a href="#cb53-1436" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional: Convert the sampling distributions to DataFrame for analysis</span></span>
<span id="cb53-1437"><a href="#cb53-1437" aria-hidden="true" tabindex="-1"></a>sampling_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb53-1438"><a href="#cb53-1438" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sample_Size_5'</span>: sampling_distribution_5,</span>
<span id="cb53-1439"><a href="#cb53-1439" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sample_Size_50'</span>: sampling_distribution_50,</span>
<span id="cb53-1440"><a href="#cb53-1440" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sample_Size_500'</span>: sampling_distribution_500</span>
<span id="cb53-1441"><a href="#cb53-1441" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb53-1442"><a href="#cb53-1442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1443"><a href="#cb53-1443" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the std. dev. of the mean attritions for each sampling distribution</span></span>
<span id="cb53-1444"><a href="#cb53-1444" aria-hidden="true" tabindex="-1"></a>sd_of_means_5 <span class="op">=</span> np.std(sampling_distribution_5, ddof <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb53-1445"><a href="#cb53-1445" aria-hidden="true" tabindex="-1"></a>sd_of_means_50 <span class="op">=</span> np.std(sampling_distribution_50, ddof <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb53-1446"><a href="#cb53-1446" aria-hidden="true" tabindex="-1"></a>sd_of_means_500 <span class="op">=</span> np.std(sampling_distribution_500, ddof <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb53-1447"><a href="#cb53-1447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1448"><a href="#cb53-1448" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb53-1449"><a href="#cb53-1449" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sd_of_means_5)</span>
<span id="cb53-1450"><a href="#cb53-1450" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sd_of_means_50)</span>
<span id="cb53-1451"><a href="#cb53-1451" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sd_of_means_500)</span>
<span id="cb53-1452"><a href="#cb53-1452" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1453"><a href="#cb53-1453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1454"><a href="#cb53-1454" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb53-1455"><a href="#cb53-1455" aria-hidden="true" tabindex="-1"></a>*The amount of variation in the sampling distribution is related to the amount of variation in the population and the sample size. This is another consequence of the Central Limit Theorem.*</span>
<span id="cb53-1456"><a href="#cb53-1456" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1457"><a href="#cb53-1457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1458"><a href="#cb53-1458" aria-hidden="true" tabindex="-1"></a><span class="fu">## CHAPTER 4: Bootstrap Distributions</span></span>
<span id="cb53-1459"><a href="#cb53-1459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1460"><a href="#cb53-1460" aria-hidden="true" tabindex="-1"></a>You’ll get to grips with resampling to perform bootstrapping and estimate variation in an unknown population. You’ll learn the difference between sampling distributions and bootstrap distributions using resampling.</span>
<span id="cb53-1461"><a href="#cb53-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1462"><a href="#cb53-1462" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 4.1: Introduction to bootstrapping</span></span>
<span id="cb53-1463"><a href="#cb53-1463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1464"><a href="#cb53-1464" aria-hidden="true" tabindex="-1"></a>So far, we've mostly focused on the idea of sampling without replacement.</span>
<span id="cb53-1465"><a href="#cb53-1465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1466"><a href="#cb53-1466" aria-hidden="true" tabindex="-1"></a><span class="fu">#### With or without {.unnumbered}</span></span>
<span id="cb53-1467"><a href="#cb53-1467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1468"><a href="#cb53-1468" aria-hidden="true" tabindex="-1"></a>Sampling without replacement is like dealing a pack of cards. When we deal the ace of spades to one player, we can't then deal the ace of spades to another player. Sampling with replacement is like rolling dice. If we roll a six, we can still get a six on the next roll. Sampling with replacement is sometimes called resampling. We'll use the terms interchangeably.</span>
<span id="cb53-1469"><a href="#cb53-1469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1470"><a href="#cb53-1470" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Simple random sampling without replacement {.unnumbered}</span></span>
<span id="cb53-1471"><a href="#cb53-1471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1472"><a href="#cb53-1472" aria-hidden="true" tabindex="-1"></a>If we take a simple random sample without replacement, each row of the dataset, or each type of coffee, can only appear once in the sample.</span>
<span id="cb53-1473"><a href="#cb53-1473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1474"><a href="#cb53-1474" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Simple random sampling with replacement {.unnumbered}</span></span>
<span id="cb53-1475"><a href="#cb53-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1476"><a href="#cb53-1476" aria-hidden="true" tabindex="-1"></a>If we sample with replacement, it means that each row of the dataset, or each coffee, can be sampled multiple times.</span>
<span id="cb53-1477"><a href="#cb53-1477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1478"><a href="#cb53-1478" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Why sample with replacement? {.unnumbered}</span></span>
<span id="cb53-1479"><a href="#cb53-1479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1480"><a href="#cb53-1480" aria-hidden="true" tabindex="-1"></a>So far, we've been treating the <span class="in">`coffee_ratings`</span> dataset as the population of all coffees. Of course, it doesn't include every coffee in the world, so we could treat the coffee dataset as just being a big sample of coffees. To imagine what the whole population is like, we need to approximate the other coffees that aren't in the dataset. Each of the coffees in the sample dataset will have properties that are representative of the coffees that we don't have. Resampling lets us use the existing coffees to approximate those other theoretical coffees.</span>
<span id="cb53-1481"><a href="#cb53-1481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1482"><a href="#cb53-1482" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Coffee data preparation {.unnumbered}</span></span>
<span id="cb53-1483"><a href="#cb53-1483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1484"><a href="#cb53-1484" aria-hidden="true" tabindex="-1"></a>To keep it simple, let's focus on three columns of the coffee dataset. To make it easier to see which rows ended up in the sample, we'll add a row index column called index using the <span class="in">`reset_index`</span> method.</span>
<span id="cb53-1485"><a href="#cb53-1485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1486"><a href="#cb53-1486" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Resampling with `.sample()` {.unnumbered}</span></span>
<span id="cb53-1487"><a href="#cb53-1487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1488"><a href="#cb53-1488" aria-hidden="true" tabindex="-1"></a>To sample with replacement, we call sample as usual but set the <span class="in">`replace`</span> argument to <span class="in">`True`</span>. Setting <span class="in">`frac`</span> to <span class="in">`1`</span> produces a sample of the same size as the original dataset.</span>
<span id="cb53-1489"><a href="#cb53-1489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1490"><a href="#cb53-1490" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Repeated coffees {.unnumbered}</span></span>
<span id="cb53-1491"><a href="#cb53-1491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1492"><a href="#cb53-1492" aria-hidden="true" tabindex="-1"></a>Counting the values of the index column shows how many times each coffee ended up in the resampled dataset. Some coffees were sampled four or five times.</span>
<span id="cb53-1493"><a href="#cb53-1493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1494"><a href="#cb53-1494" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Missing coffees {.unnumbered}</span></span>
<span id="cb53-1495"><a href="#cb53-1495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1496"><a href="#cb53-1496" aria-hidden="true" tabindex="-1"></a>That means that some coffees didn't end up in the resample. By taking the number of distinct index values in the resampled dataset, using <span class="in">`len`</span> on <span class="in">`drop_duplicates`</span>, we see that eight hundred and sixty-eight different coffees were included. By comparing this number with the total number of coffees, we can see that four hundred and seventy coffees weren't included in the resample.</span>
<span id="cb53-1497"><a href="#cb53-1497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1498"><a href="#cb53-1498" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bootstrapping {.unnumbered}</span></span>
<span id="cb53-1499"><a href="#cb53-1499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1500"><a href="#cb53-1500" aria-hidden="true" tabindex="-1"></a>We're going to use resampling for a technique called bootstrapping. In some sense, bootstrapping is the opposite of sampling from a population. With sampling, we treat the dataset as the population and move to a smaller sample. With bootstrapping, we treat the dataset as a sample and use it to build up a theoretical population. A use case of bootstrapping is to try to understand the variability due to sampling. This is important in cases where we aren't able to sample the population multiple times to create a sampling distribution.</span>
<span id="cb53-1501"><a href="#cb53-1501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1502"><a href="#cb53-1502" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bootstrapping process {.unnumbered}</span></span>
<span id="cb53-1503"><a href="#cb53-1503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1504"><a href="#cb53-1504" aria-hidden="true" tabindex="-1"></a>The bootstrapping process has three steps. First, randomly sample with replacement to get a resample the same size as the original dataset. Then, calculate a statistic, such as a mean of one of the columns. Note that the mean isn't always the choice here and bootstrapping allows for complex statistics to be computed, too. Then, replicate this many times to get lots of these bootstrap statistics. Earlier in the course, we did something similar. We took a simple random sample, then calculated a summary statistic, then repeated those two steps to form a sampling distribution. This time, when we've used resampling instead of sampling, we get a bootstrap distribution.</span>
<span id="cb53-1505"><a href="#cb53-1505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1506"><a href="#cb53-1506" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bootstrapping coffee mean flavor {.unnumbered}</span></span>
<span id="cb53-1507"><a href="#cb53-1507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1508"><a href="#cb53-1508" aria-hidden="true" tabindex="-1"></a>The resampling step uses the code we just saw: calling sample with <span class="in">`frac`</span> set to <span class="in">`one`</span> and <span class="in">`replace`</span> set to <span class="in">`True`</span>. Calculating a bootstrap statistic can be done with mean from NumPy. In this case, we're calculating the mean flavor score. To repeat steps one and two one thousand times, we can wrap the code in a for loop and append the statistics to a list.</span>
<span id="cb53-1509"><a href="#cb53-1509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1510"><a href="#cb53-1510" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bootstrap distribution histogram {.unnumbered}</span></span>
<span id="cb53-1511"><a href="#cb53-1511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1512"><a href="#cb53-1512" aria-hidden="true" tabindex="-1"></a>Here's a histogram of the bootstrap distribution of the sample mean. Notice that it is close to following a normal distribution.</span>
<span id="cb53-1513"><a href="#cb53-1513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1514"><a href="#cb53-1514" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.1.1 </span></span>
<span id="cb53-1515"><a href="#cb53-1515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1516"><a href="#cb53-1516" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Generating a bootstrap distribution {.unnumbered}</span></span>
<span id="cb53-1517"><a href="#cb53-1517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1518"><a href="#cb53-1518" aria-hidden="true" tabindex="-1"></a>The process for generating a bootstrap distribution is similar to the process for generating a sampling distribution; only the first step is different.</span>
<span id="cb53-1519"><a href="#cb53-1519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1520"><a href="#cb53-1520" aria-hidden="true" tabindex="-1"></a>To make a sampling distribution, you start with the population and sample without replacement. To make a bootstrap distribution, you start with a sample and sample that with replacement. After that, the steps are the same: calculate the summary statistic that you are interested in on that sample/resample, then replicate the process many times. In each case, you can visualize the distribution with a histogram.</span>
<span id="cb53-1521"><a href="#cb53-1521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1522"><a href="#cb53-1522" aria-hidden="true" tabindex="-1"></a>Here, <span class="in">`spotify_sample`</span> is a subset of the <span class="in">`spotify`</span> dataset. To make it easier to see how resampling works, a row index column called <span class="in">`'index'`</span> has been added, and only the artist name, song name, and <span class="in">`danceability`</span> columns have been included.</span>
<span id="cb53-1523"><a href="#cb53-1523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1524"><a href="#cb53-1524" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-1525"><a href="#cb53-1525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1526"><a href="#cb53-1526" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Generate a single bootstrap resample from <span class="in">`spotify_sample`</span>.</span>
<span id="cb53-1527"><a href="#cb53-1527" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Calculate the mean of the <span class="in">`danceability`</span> column of <span class="in">`spotify_1_resample`</span> using <span class="in">`numpy`</span>.</span>
<span id="cb53-1528"><a href="#cb53-1528" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Replicate the expression provided 1000 times.</span>
<span id="cb53-1529"><a href="#cb53-1529" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Create a bootstrap distribution by drawing a histogram of <span class="in">`mean_danceability_1000`</span>.</span>
<span id="cb53-1530"><a href="#cb53-1530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1533"><a href="#cb53-1533" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-1534"><a href="#cb53-1534" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-1535"><a href="#cb53-1535" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-1536"><a href="#cb53-1536" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-1537"><a href="#cb53-1537" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-1538"><a href="#cb53-1538" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-1539"><a href="#cb53-1539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1540"><a href="#cb53-1540" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course array</span></span>
<span id="cb53-1541"><a href="#cb53-1541" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb53-1542"><a href="#cb53-1542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1543"><a href="#cb53-1543" aria-hidden="true" tabindex="-1"></a><span class="co"># Subset of spotify sample to use</span></span>
<span id="cb53-1544"><a href="#cb53-1544" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify.sample(n<span class="op">=</span><span class="dv">41656</span>)[[<span class="st">'artists'</span>, <span class="st">'name'</span>, <span class="st">'danceability'</span>]]</span>
<span id="cb53-1545"><a href="#cb53-1545" aria-hidden="true" tabindex="-1"></a>spotify_sample[<span class="st">'index'</span>] <span class="op">=</span> spotify_sample.index</span>
<span id="cb53-1546"><a href="#cb53-1546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1547"><a href="#cb53-1547" aria-hidden="true" tabindex="-1"></a><span class="co"># Reorder columns to make 'index' the first column</span></span>
<span id="cb53-1548"><a href="#cb53-1548" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify_sample[[<span class="st">'index'</span>, <span class="st">'artists'</span>, <span class="st">'name'</span>, <span class="st">'danceability'</span>]]</span>
<span id="cb53-1549"><a href="#cb53-1549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1550"><a href="#cb53-1550" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 1 bootstrap resample</span></span>
<span id="cb53-1551"><a href="#cb53-1551" aria-hidden="true" tabindex="-1"></a>spotify_1_resample <span class="op">=</span> spotify_sample.sample(frac<span class="op">=</span><span class="dv">1</span>, replace <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb53-1552"><a href="#cb53-1552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1553"><a href="#cb53-1553" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the resample</span></span>
<span id="cb53-1554"><a href="#cb53-1554" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(spotify_1_resample)</span>
<span id="cb53-1555"><a href="#cb53-1555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1556"><a href="#cb53-1556" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate of the danceability column of spotify_1_resample</span></span>
<span id="cb53-1557"><a href="#cb53-1557" aria-hidden="true" tabindex="-1"></a>mean_danceability_1 <span class="op">=</span> np.mean(spotify_1_resample[<span class="st">'danceability'</span>])</span>
<span id="cb53-1558"><a href="#cb53-1558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1559"><a href="#cb53-1559" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb53-1560"><a href="#cb53-1560" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_danceability_1)</span>
<span id="cb53-1561"><a href="#cb53-1561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1562"><a href="#cb53-1562" aria-hidden="true" tabindex="-1"></a><span class="co"># Replicate this 1000 times</span></span>
<span id="cb53-1563"><a href="#cb53-1563" aria-hidden="true" tabindex="-1"></a>mean_danceability_1000 <span class="op">=</span> []</span>
<span id="cb53-1564"><a href="#cb53-1564" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb53-1565"><a href="#cb53-1565" aria-hidden="true" tabindex="-1"></a>    mean_danceability_1000.append(</span>
<span id="cb53-1566"><a href="#cb53-1566" aria-hidden="true" tabindex="-1"></a>        np.mean(spotify_sample.sample(frac<span class="op">=</span><span class="dv">1</span>, replace<span class="op">=</span><span class="va">True</span>)[<span class="st">'danceability'</span>])</span>
<span id="cb53-1567"><a href="#cb53-1567" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1568"><a href="#cb53-1568" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-1569"><a href="#cb53-1569" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb53-1570"><a href="#cb53-1570" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_danceability_1000)</span>
<span id="cb53-1571"><a href="#cb53-1571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1572"><a href="#cb53-1572" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a histogram of the resample means</span></span>
<span id="cb53-1573"><a href="#cb53-1573" aria-hidden="true" tabindex="-1"></a>plt.hist(mean_danceability_1000)</span>
<span id="cb53-1574"><a href="#cb53-1574" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-1575"><a href="#cb53-1575" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1576"><a href="#cb53-1576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1577"><a href="#cb53-1577" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 4.2: Comparing sampling and bootstrap distributions</span></span>
<span id="cb53-1578"><a href="#cb53-1578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1579"><a href="#cb53-1579" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Coffee focused subset {.unnumbered}</span></span>
<span id="cb53-1580"><a href="#cb53-1580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1581"><a href="#cb53-1581" aria-hidden="true" tabindex="-1"></a>we took a focused subset of the coffee dataset. Here's a five hundred row sample from it.</span>
<span id="cb53-1582"><a href="#cb53-1582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1583"><a href="#cb53-1583" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The bootstrap of mean coffee flavors {.unnumbered}</span></span>
<span id="cb53-1584"><a href="#cb53-1584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1585"><a href="#cb53-1585" aria-hidden="true" tabindex="-1"></a>Here, we generate a bootstrap distribution of the mean coffee flavor scores from that sample. <span class="in">`.sample`</span> generates a resample, <span class="in">`np.mean`</span> calculates the statistic, and the for loop with <span class="in">`.append`</span> repeats these steps to produce a distribution of bootstrap statistics.</span>
<span id="cb53-1586"><a href="#cb53-1586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1587"><a href="#cb53-1587" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Mean flavor bootstrap distribution {.unnumbered}</span></span>
<span id="cb53-1588"><a href="#cb53-1588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1589"><a href="#cb53-1589" aria-hidden="true" tabindex="-1"></a>Observing the histogram of the bootstrap distribution, which is close to a normal distribution.</span>
<span id="cb53-1590"><a href="#cb53-1590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1591"><a href="#cb53-1591" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Sample, bootstrap distribution, population means {.unnumbered}</span></span>
<span id="cb53-1592"><a href="#cb53-1592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1593"><a href="#cb53-1593" aria-hidden="true" tabindex="-1"></a>Here's the mean flavor score from the original sample. In the bootstrap distribution, each value is an estimate of the mean flavor score. Recall that each of these values corresponds to one potential sample mean from the theoretical population. If we take the mean of those means, we get our best guess of the population mean. The two values are really close. However, there's a problem. The true population mean is actually a little different.</span>
<span id="cb53-1594"><a href="#cb53-1594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1595"><a href="#cb53-1595" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interpreting the means {.unnumbered}</span></span>
<span id="cb53-1596"><a href="#cb53-1596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1597"><a href="#cb53-1597" aria-hidden="true" tabindex="-1"></a>The behavior that you just saw is typical. The bootstrap distribution mean is usually almost identical to the original sample mean. However, that is not often a good thing. If the original sample wasn't closely representative of the population, then the bootstrap distribution mean won't be a good estimate of the population mean. Bootstrapping cannot correct any potential biases due to differences between the sample and the population.</span>
<span id="cb53-1598"><a href="#cb53-1598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1599"><a href="#cb53-1599" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Sample `sd` vs. bootstrap distribution `sd` {.unnumbered}</span></span>
<span id="cb53-1600"><a href="#cb53-1600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1601"><a href="#cb53-1601" aria-hidden="true" tabindex="-1"></a>While we do have that limitation in estimating the population mean, one great thing about distributions is that we can also quantify variation. The standard deviation of the sample flavors is around <span class="in">`0.354`</span>. Recall that pandas <span class="in">`.std`</span> calculates a sample standard deviation by default. If we calculate the standard deviation of the bootstrap distribution, specifying a <span class="in">`ddof`</span> of one, then we get a completely different number. So what's going on here?</span>
<span id="cb53-1602"><a href="#cb53-1602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1603"><a href="#cb53-1603" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Sample, bootstrap dist'n, pop'n standard deviations {.unnumbered}</span></span>
<span id="cb53-1604"><a href="#cb53-1604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1605"><a href="#cb53-1605" aria-hidden="true" tabindex="-1"></a>Remember that one goal of bootstrapping is to quantify what variability we might expect in our sample statistic as we go from one sample to another. Recall that this quantity is called the standard error as measured by the standard deviation of the sampling distribution of that statistic. The standard deviation of the bootstrap means can be used as a way to estimate this measure of uncertainty. If we multiply that standard error by the square root of the sample size, we get an estimate of the standard deviation in the original population. Our estimate of the standard deviation is around point-three-five-three. The true standard deviation is around point-three-four-one, so our estimate is pretty close. In fact, it is closer than just using the sample standard deviation alone.</span>
<span id="cb53-1606"><a href="#cb53-1606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1607"><a href="#cb53-1607" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interpreting the standard errors {.unnumbered}</span></span>
<span id="cb53-1608"><a href="#cb53-1608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1609"><a href="#cb53-1609" aria-hidden="true" tabindex="-1"></a>To recap, the estimated standard error is the standard deviation of the bootstrap distribution values for our statistic of interest. This estimated standard error times the square root of the sample size gives a really good estimate of the standard deviation of the population. That is, although bootstrapping was poor at estimating the population mean, it is generally great for estimating the population standard deviation.</span>
<span id="cb53-1610"><a href="#cb53-1610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1611"><a href="#cb53-1611" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.2.1</span></span>
<span id="cb53-1612"><a href="#cb53-1612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1613"><a href="#cb53-1613" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Sampling distribution vs. bootstrap distribution {.unnumbered}</span></span>
<span id="cb53-1614"><a href="#cb53-1614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1615"><a href="#cb53-1615" aria-hidden="true" tabindex="-1"></a>The sampling distribution and bootstrap distribution are closely linked. In situations where you can repeatedly sample from a population (these occasions are rare), it's helpful to generate both the sampling distribution and the bootstrap distribution, one after the other, to see how they are related.</span>
<span id="cb53-1616"><a href="#cb53-1616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1617"><a href="#cb53-1617" aria-hidden="true" tabindex="-1"></a>Here, the statistic you are interested in is the mean <span class="in">`popularity`</span> score of the songs.</span>
<span id="cb53-1618"><a href="#cb53-1618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1619"><a href="#cb53-1619" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-1620"><a href="#cb53-1620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1621"><a href="#cb53-1621" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Generate a sampling distribution of 2000 replicates.</span>
<span id="cb53-1622"><a href="#cb53-1622" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Sample 500 rows of the population without replacement and calculate the mean <span class="in">`popularity`</span>.</span>
<span id="cb53-1623"><a href="#cb53-1623" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Generate a bootstrap distribution of 2000 replicates.</span>
<span id="cb53-1624"><a href="#cb53-1624" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Sample 500 rows of the sample with replacement and calculate the mean <span class="in">`popularity`</span>.</span>
<span id="cb53-1625"><a href="#cb53-1625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1628"><a href="#cb53-1628" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-1629"><a href="#cb53-1629" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-1630"><a href="#cb53-1630" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-1631"><a href="#cb53-1631" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-1632"><a href="#cb53-1632" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-1633"><a href="#cb53-1633" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-1634"><a href="#cb53-1634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1635"><a href="#cb53-1635" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course array</span></span>
<span id="cb53-1636"><a href="#cb53-1636" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb53-1637"><a href="#cb53-1637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1638"><a href="#cb53-1638" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify.sample(n<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb53-1639"><a href="#cb53-1639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1640"><a href="#cb53-1640" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_samp <span class="op">=</span> []</span>
<span id="cb53-1641"><a href="#cb53-1641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1642"><a href="#cb53-1642" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a sampling distribution of 2000 replicates</span></span>
<span id="cb53-1643"><a href="#cb53-1643" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb53-1644"><a href="#cb53-1644" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_samp.append(</span>
<span id="cb53-1645"><a href="#cb53-1645" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample 500 rows and calculate the mean popularity </span></span>
<span id="cb53-1646"><a href="#cb53-1646" aria-hidden="true" tabindex="-1"></a>        spotify.sample(n<span class="op">=</span><span class="dv">500</span>)[<span class="st">'popularity'</span>].mean()</span>
<span id="cb53-1647"><a href="#cb53-1647" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1648"><a href="#cb53-1648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1649"><a href="#cb53-1649" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sampling distribution results</span></span>
<span id="cb53-1650"><a href="#cb53-1650" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_popularity_2000_samp)</span>
<span id="cb53-1651"><a href="#cb53-1651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1652"><a href="#cb53-1652" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_boot <span class="op">=</span> []</span>
<span id="cb53-1653"><a href="#cb53-1653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1654"><a href="#cb53-1654" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a bootstrap distribution of 2000 replicates</span></span>
<span id="cb53-1655"><a href="#cb53-1655" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb53-1656"><a href="#cb53-1656" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_boot.append(</span>
<span id="cb53-1657"><a href="#cb53-1657" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resample 500 rows and calculate the mean popularity     </span></span>
<span id="cb53-1658"><a href="#cb53-1658" aria-hidden="true" tabindex="-1"></a>        np.mean(spotify_sample.sample(frac<span class="op">=</span><span class="dv">1</span>, replace<span class="op">=</span><span class="va">True</span>)[<span class="st">'popularity'</span>])</span>
<span id="cb53-1659"><a href="#cb53-1659" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1660"><a href="#cb53-1660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1661"><a href="#cb53-1661" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the bootstrap distribution results</span></span>
<span id="cb53-1662"><a href="#cb53-1662" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_popularity_2000_boot)</span>
<span id="cb53-1663"><a href="#cb53-1663" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1664"><a href="#cb53-1664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1665"><a href="#cb53-1665" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb53-1666"><a href="#cb53-1666" aria-hidden="true" tabindex="-1"></a>*The sampling distribution and bootstrap distribution are closely related, and so is the code to generate them.*</span>
<span id="cb53-1667"><a href="#cb53-1667" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1668"><a href="#cb53-1668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1669"><a href="#cb53-1669" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.2.2</span></span>
<span id="cb53-1670"><a href="#cb53-1670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1671"><a href="#cb53-1671" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Compare sampling and bootstrap means {.unnumbered}</span></span>
<span id="cb53-1672"><a href="#cb53-1672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1673"><a href="#cb53-1673" aria-hidden="true" tabindex="-1"></a>To make calculation easier, distributions similar to those calculated from the previous exercise have been included, this time using a sample size of 5000.</span>
<span id="cb53-1674"><a href="#cb53-1674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1675"><a href="#cb53-1675" aria-hidden="true" tabindex="-1"></a>spotify_population, spotify_sample, sampling_distribution, and bootstrap_distribution are available; pandas and numpy are loaded with their usual aliases.</span>
<span id="cb53-1676"><a href="#cb53-1676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1677"><a href="#cb53-1677" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-1678"><a href="#cb53-1678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1679"><a href="#cb53-1679" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Calculate the mean <span class="in">`popularity`</span> in 4 ways:</span>
<span id="cb53-1680"><a href="#cb53-1680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1681"><a href="#cb53-1681" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Population: from <span class="in">`spotify`</span>, take the mean of <span class="in">`popularity`</span>.</span>
<span id="cb53-1682"><a href="#cb53-1682" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sample: from <span class="in">`spotify_sample`</span>, take the mean of <span class="in">`popularity`</span>.</span>
<span id="cb53-1683"><a href="#cb53-1683" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sampling distribution: from <span class="in">`sampling_distribution`</span>, take its mean.</span>
<span id="cb53-1684"><a href="#cb53-1684" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bootstrap distribution: from `<span class="in">`bootstrap_distribution`</span>, take its mean.</span>
<span id="cb53-1685"><a href="#cb53-1685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1688"><a href="#cb53-1688" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-1689"><a href="#cb53-1689" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-1690"><a href="#cb53-1690" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-1691"><a href="#cb53-1691" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-1692"><a href="#cb53-1692" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-1693"><a href="#cb53-1693" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-1694"><a href="#cb53-1694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1695"><a href="#cb53-1695" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course array</span></span>
<span id="cb53-1696"><a href="#cb53-1696" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb53-1697"><a href="#cb53-1697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1698"><a href="#cb53-1698" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify.sample(n<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb53-1699"><a href="#cb53-1699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1700"><a href="#cb53-1700" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_samp <span class="op">=</span> []</span>
<span id="cb53-1701"><a href="#cb53-1701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1702"><a href="#cb53-1702" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a sampling distribution of 2000 replicates</span></span>
<span id="cb53-1703"><a href="#cb53-1703" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb53-1704"><a href="#cb53-1704" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_samp.append(</span>
<span id="cb53-1705"><a href="#cb53-1705" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample 500 rows and calculate the mean popularity </span></span>
<span id="cb53-1706"><a href="#cb53-1706" aria-hidden="true" tabindex="-1"></a>        spotify.sample(n<span class="op">=</span><span class="dv">500</span>)[<span class="st">'popularity'</span>].mean()</span>
<span id="cb53-1707"><a href="#cb53-1707" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1708"><a href="#cb53-1708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1709"><a href="#cb53-1709" aria-hidden="true" tabindex="-1"></a><span class="co"># The sampling distribution results</span></span>
<span id="cb53-1710"><a href="#cb53-1710" aria-hidden="true" tabindex="-1"></a>sampling_distribution <span class="op">=</span> mean_popularity_2000_samp </span>
<span id="cb53-1711"><a href="#cb53-1711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1712"><a href="#cb53-1712" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_boot <span class="op">=</span> []</span>
<span id="cb53-1713"><a href="#cb53-1713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1714"><a href="#cb53-1714" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a bootstrap distribution of 2000 replicates</span></span>
<span id="cb53-1715"><a href="#cb53-1715" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb53-1716"><a href="#cb53-1716" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_boot.append(</span>
<span id="cb53-1717"><a href="#cb53-1717" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resample 500 rows and calculate the mean popularity     </span></span>
<span id="cb53-1718"><a href="#cb53-1718" aria-hidden="true" tabindex="-1"></a>        np.mean(spotify_sample.sample(frac<span class="op">=</span><span class="dv">1</span>, replace<span class="op">=</span><span class="va">True</span>)[<span class="st">'popularity'</span>])</span>
<span id="cb53-1719"><a href="#cb53-1719" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1720"><a href="#cb53-1720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1721"><a href="#cb53-1721" aria-hidden="true" tabindex="-1"></a><span class="co"># The bootstrap distribution results</span></span>
<span id="cb53-1722"><a href="#cb53-1722" aria-hidden="true" tabindex="-1"></a>bootstrap_distribution <span class="op">=</span> mean_popularity_2000_boot</span>
<span id="cb53-1723"><a href="#cb53-1723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1724"><a href="#cb53-1724" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the population mean popularity</span></span>
<span id="cb53-1725"><a href="#cb53-1725" aria-hidden="true" tabindex="-1"></a>pop_mean <span class="op">=</span> spotify[<span class="st">'popularity'</span>].mean()</span>
<span id="cb53-1726"><a href="#cb53-1726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1727"><a href="#cb53-1727" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the original sample mean popularity</span></span>
<span id="cb53-1728"><a href="#cb53-1728" aria-hidden="true" tabindex="-1"></a>samp_mean <span class="op">=</span> spotify_sample[<span class="st">'popularity'</span>].mean()</span>
<span id="cb53-1729"><a href="#cb53-1729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1730"><a href="#cb53-1730" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the sampling dist'n estimate of mean popularity</span></span>
<span id="cb53-1731"><a href="#cb53-1731" aria-hidden="true" tabindex="-1"></a>samp_distn_mean <span class="op">=</span> np.mean(sampling_distribution)</span>
<span id="cb53-1732"><a href="#cb53-1732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1733"><a href="#cb53-1733" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the bootstrap dist'n estimate of mean popularity</span></span>
<span id="cb53-1734"><a href="#cb53-1734" aria-hidden="true" tabindex="-1"></a>boot_distn_mean <span class="op">=</span> np.mean(bootstrap_distribution)</span>
<span id="cb53-1735"><a href="#cb53-1735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1736"><a href="#cb53-1736" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the means</span></span>
<span id="cb53-1737"><a href="#cb53-1737" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([pop_mean, samp_mean, samp_distn_mean, boot_distn_mean])</span>
<span id="cb53-1738"><a href="#cb53-1738" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1739"><a href="#cb53-1739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1740"><a href="#cb53-1740" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb53-1741"><a href="#cb53-1741" aria-hidden="true" tabindex="-1"></a>*The sampling distribution mean can be used to estimate the population mean, but that is not the case with the bootstrap distribution.*</span>
<span id="cb53-1742"><a href="#cb53-1742" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1743"><a href="#cb53-1743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1744"><a href="#cb53-1744" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.2.3</span></span>
<span id="cb53-1745"><a href="#cb53-1745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1746"><a href="#cb53-1746" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Compare sampling and bootstrap standard deviations {.unnumbered}</span></span>
<span id="cb53-1747"><a href="#cb53-1747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1748"><a href="#cb53-1748" aria-hidden="true" tabindex="-1"></a>In the same way that you looked at how the sampling distribution and bootstrap distribution could be used to estimate the population mean, you'll now take a look at how they can be used to estimate variation, or more specifically, the standard deviation, in the population.</span>
<span id="cb53-1749"><a href="#cb53-1749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1750"><a href="#cb53-1750" aria-hidden="true" tabindex="-1"></a>Recall that the sample size is 5000.</span>
<span id="cb53-1751"><a href="#cb53-1751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1752"><a href="#cb53-1752" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unnumbered}</span></span>
<span id="cb53-1753"><a href="#cb53-1753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1754"><a href="#cb53-1754" aria-hidden="true" tabindex="-1"></a>Calculate the standard deviation of <span class="in">`popularity`</span> in 4 ways.</span>
<span id="cb53-1755"><a href="#cb53-1755" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Population: from <span class="in">`spotify`</span>, take the standard deviation of <span class="in">`popularity`</span>.</span>
<span id="cb53-1756"><a href="#cb53-1756" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Original sample: from <span class="in">`spotify_sample`</span>, take the standard deviation of <span class="in">`popularity`</span>.</span>
<span id="cb53-1757"><a href="#cb53-1757" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sampling distribution: from <span class="in">`sampling_distribution`</span>, take its standard deviation and multiply by the square root of the sample size (5000).</span>
<span id="cb53-1758"><a href="#cb53-1758" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bootstrap distribution: from <span class="in">`bootstrap_distribution`</span>, take its standard deviation and multiply by the square root of the sample size.</span>
<span id="cb53-1759"><a href="#cb53-1759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1762"><a href="#cb53-1762" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-1763"><a href="#cb53-1763" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-1764"><a href="#cb53-1764" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-1765"><a href="#cb53-1765" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-1766"><a href="#cb53-1766" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb53-1767"><a href="#cb53-1767" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-1768"><a href="#cb53-1768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1769"><a href="#cb53-1769" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course array</span></span>
<span id="cb53-1770"><a href="#cb53-1770" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb53-1771"><a href="#cb53-1771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1772"><a href="#cb53-1772" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify.sample(n<span class="op">=</span><span class="dv">5000</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb53-1773"><a href="#cb53-1773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1774"><a href="#cb53-1774" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_samp <span class="op">=</span> []</span>
<span id="cb53-1775"><a href="#cb53-1775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1776"><a href="#cb53-1776" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a sampling distribution of 2000 replicates</span></span>
<span id="cb53-1777"><a href="#cb53-1777" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb53-1778"><a href="#cb53-1778" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_samp.append(</span>
<span id="cb53-1779"><a href="#cb53-1779" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample 500 rows and calculate the mean popularity </span></span>
<span id="cb53-1780"><a href="#cb53-1780" aria-hidden="true" tabindex="-1"></a>        spotify.sample(n<span class="op">=</span><span class="dv">5000</span>)[<span class="st">'popularity'</span>].mean()</span>
<span id="cb53-1781"><a href="#cb53-1781" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1782"><a href="#cb53-1782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1783"><a href="#cb53-1783" aria-hidden="true" tabindex="-1"></a><span class="co"># The sampling distribution results</span></span>
<span id="cb53-1784"><a href="#cb53-1784" aria-hidden="true" tabindex="-1"></a>sampling_distribution <span class="op">=</span> mean_popularity_2000_samp </span>
<span id="cb53-1785"><a href="#cb53-1785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1786"><a href="#cb53-1786" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_boot <span class="op">=</span> []</span>
<span id="cb53-1787"><a href="#cb53-1787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1788"><a href="#cb53-1788" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a bootstrap distribution of 2000 replicates</span></span>
<span id="cb53-1789"><a href="#cb53-1789" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb53-1790"><a href="#cb53-1790" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_boot.append(</span>
<span id="cb53-1791"><a href="#cb53-1791" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resample 500 rows and calculate the mean popularity     </span></span>
<span id="cb53-1792"><a href="#cb53-1792" aria-hidden="true" tabindex="-1"></a>        np.mean(spotify_sample.sample(frac<span class="op">=</span><span class="dv">1</span>, replace<span class="op">=</span><span class="va">True</span>)[<span class="st">'popularity'</span>])</span>
<span id="cb53-1793"><a href="#cb53-1793" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1794"><a href="#cb53-1794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1795"><a href="#cb53-1795" aria-hidden="true" tabindex="-1"></a><span class="co"># The bootstrap distribution results</span></span>
<span id="cb53-1796"><a href="#cb53-1796" aria-hidden="true" tabindex="-1"></a>bootstrap_distribution <span class="op">=</span> mean_popularity_2000_boot</span>
<span id="cb53-1797"><a href="#cb53-1797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1798"><a href="#cb53-1798" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the population std dev popularity</span></span>
<span id="cb53-1799"><a href="#cb53-1799" aria-hidden="true" tabindex="-1"></a>pop_sd <span class="op">=</span> spotify[<span class="st">'popularity'</span>].std(ddof<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb53-1800"><a href="#cb53-1800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1801"><a href="#cb53-1801" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the original sample std dev popularity</span></span>
<span id="cb53-1802"><a href="#cb53-1802" aria-hidden="true" tabindex="-1"></a>samp_sd <span class="op">=</span> spotify_sample[<span class="st">'popularity'</span>].std(ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb53-1803"><a href="#cb53-1803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1804"><a href="#cb53-1804" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the sampling dist'n estimate of std dev popularity</span></span>
<span id="cb53-1805"><a href="#cb53-1805" aria-hidden="true" tabindex="-1"></a>samp_distn_sd <span class="op">=</span> np.std(sampling_distribution, ddof<span class="op">=</span><span class="dv">1</span>) <span class="op">*</span> np.sqrt(<span class="dv">5000</span>)</span>
<span id="cb53-1806"><a href="#cb53-1806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1807"><a href="#cb53-1807" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the bootstrap dist'n estimate of std dev popularity</span></span>
<span id="cb53-1808"><a href="#cb53-1808" aria-hidden="true" tabindex="-1"></a>boot_distn_sd <span class="op">=</span> np.std(bootstrap_distribution, ddof<span class="op">=</span><span class="dv">1</span>) <span class="op">*</span> np.sqrt(<span class="dv">5000</span>)</span>
<span id="cb53-1809"><a href="#cb53-1809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1810"><a href="#cb53-1810" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the standard deviations</span></span>
<span id="cb53-1811"><a href="#cb53-1811" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([pop_sd, samp_sd, samp_distn_sd, boot_distn_sd])</span>
<span id="cb53-1812"><a href="#cb53-1812" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1813"><a href="#cb53-1813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1814"><a href="#cb53-1814" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 4.3: Confidence intervals</span></span>
<span id="cb53-1815"><a href="#cb53-1815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1816"><a href="#cb53-1816" aria-hidden="true" tabindex="-1"></a>In the last few exercises, you looked at relationships between the sampling distribution and the bootstrap distribution.</span>
<span id="cb53-1817"><a href="#cb53-1817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1818"><a href="#cb53-1818" aria-hidden="true" tabindex="-1"></a>One way to quantify these distributions is the idea of "values within one standard deviation of the mean", which gives a good sense of where most of the values in a distribution lie. In this final lesson, we'll formalize the idea of values close to a statistic by defining the term "confidence interval".</span>
<span id="cb53-1819"><a href="#cb53-1819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1820"><a href="#cb53-1820" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Predicting the weather {.unnumbered}</span></span>
<span id="cb53-1821"><a href="#cb53-1821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1822"><a href="#cb53-1822" aria-hidden="true" tabindex="-1"></a>Consider meteorologists predicting weather in one of the world's most unpredictable regions - the northern Great Plains of the US and Canada. Rapid City, South Dakota was ranked as the least predictable of the 120 US cities with a National Weather Service forecast office. Suppose we've taken a job as a meteorologist at a news station in Rapid City. Our job is to predict tomorrow's high temperature.</span>
<span id="cb53-1823"><a href="#cb53-1823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1824"><a href="#cb53-1824" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Our weather prediction {.unnumbered}</span></span>
<span id="cb53-1825"><a href="#cb53-1825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1826"><a href="#cb53-1826" aria-hidden="true" tabindex="-1"></a>We analyze the weather data using the best forecasting tools available to us and predict a high temperature of 47 degrees Fahrenheit. In this case, 47 degrees is our point estimate. Since the weather is variable, and many South Dakotans will plan their day tomorrow based on our forecast, we'd instead like to present a range of plausible values for the high temperature. On our weather show, we report that the high temperature will be between forty and fifty-four degrees tomorrow.</span>
<span id="cb53-1827"><a href="#cb53-1827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1828"><a href="#cb53-1828" aria-hidden="true" tabindex="-1"></a><span class="fu">#### We just reported a confidence interval! {.unnumbered}</span></span>
<span id="cb53-1829"><a href="#cb53-1829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1830"><a href="#cb53-1830" aria-hidden="true" tabindex="-1"></a>This prediction of forty to fifty-four degrees can be thought of as a confidence interval for the unknown quantity of tomorrow's high temperature. Although we can't be sure of the exact temperature, we are confident that it will be in that range. These results are often written as the point estimate followed by the confidence interval's lower and upper bounds in parentheses or square brackets. When the confidence interval is symmetric around the point estimate, we can represent it as the point estimate plus or minus the margin of error, in this case, seven degrees.</span>
<span id="cb53-1831"><a href="#cb53-1831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1832"><a href="#cb53-1832" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bootstrap distribution of mean flavor {.unnumbered}</span></span>
<span id="cb53-1833"><a href="#cb53-1833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1834"><a href="#cb53-1834" aria-hidden="true" tabindex="-1"></a>Here's the bootstrap distribution of the mean flavor from the coffee dataset.</span>
<span id="cb53-1835"><a href="#cb53-1835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1836"><a href="#cb53-1836" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Mean of the resamples {.unnumbered}</span></span>
<span id="cb53-1837"><a href="#cb53-1837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1838"><a href="#cb53-1838" aria-hidden="true" tabindex="-1"></a>We can calculate the mean of these resampled mean flavors.</span>
<span id="cb53-1839"><a href="#cb53-1839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1840"><a href="#cb53-1840" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Mean plus or minus one standard deviation {.unnumbered}</span></span>
<span id="cb53-1841"><a href="#cb53-1841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1842"><a href="#cb53-1842" aria-hidden="true" tabindex="-1"></a>If we create a confidence interval by adding and subtracting one standard deviation from the mean, we see that there are lots of values in the bootstrap distribution outside of this one standard deviation confidence interval.</span>
<span id="cb53-1843"><a href="#cb53-1843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1844"><a href="#cb53-1844" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Quantile method for confidence intervals {.unnumbered}</span></span>
<span id="cb53-1845"><a href="#cb53-1845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1846"><a href="#cb53-1846" aria-hidden="true" tabindex="-1"></a>If we want to include ninety-five percent of the values in the confidence interval, we can use quantiles. Recall that quantiles split distributions into sections containing a particular proportion of the total data. To get the middle ninety-five percent of values, we go from the point-zero-two-five quantile to the point-nine-seven-five quantile since the difference between those two numbers is point-nine-five. To calculate the lower and upper bounds for this confidence interval, we call quantile from NumPy, passing the distribution values and the quantile values to use. The confidence interval is from around seven-point-four-eight to seven-point-five-four.</span>
<span id="cb53-1847"><a href="#cb53-1847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1848"><a href="#cb53-1848" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Inverse cumulative distribution function {.unnumbered}</span></span>
<span id="cb53-1849"><a href="#cb53-1849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1850"><a href="#cb53-1850" aria-hidden="true" tabindex="-1"></a>There is a second method to calculate confidence intervals. To understand it, we need to be familiar with the normal distribution's inverse cumulative distribution function. The bell curve we've seen before is the probability density function or PDF. Using calculus, if we integrate this, we get the cumulative distribution function or CDF. If we flip the x and y axes, we get the inverse <span class="in">`CDF`</span>. We can use <span class="in">`scipy.stats`</span> and call <span class="in">`norm.ppf`</span> to get the inverse <span class="in">`CDF`</span>. It takes a quantile between zero and one and returns the values of the normal distribution for that quantile. The parameters of <span class="in">`loc`</span> and <span class="in">`scale`</span> are set to 0 and 1 by default, corresponding to the standard normal distribution. Notice that the values corresponding to point-zero-two-five and point-nine-seven-five are about minus and plus two for the standard normal distribution.</span>
<span id="cb53-1851"><a href="#cb53-1851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1852"><a href="#cb53-1852" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Standard error method for confidence interval {.unnumbered}</span></span>
<span id="cb53-1853"><a href="#cb53-1853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1854"><a href="#cb53-1854" aria-hidden="true" tabindex="-1"></a>This second method for calculating a confidence interval is called the standard error method. First, we calculate the point estimate, which is the mean of the bootstrap distribution, and the standard error, which is estimated by the standard deviation of the bootstrap distribution. Then we call <span class="in">`norm.ppf`</span> to get the inverse <span class="in">`CDF`</span> of the normal distribution with the same mean and standard deviation as the bootstrap distribution. Again, the confidence interval is from seven-point-four-eight to seven-point-five-four, though the numbers differ slightly from last time since our bootstrap distribution isn't perfectly normal.</span>
<span id="cb53-1855"><a href="#cb53-1855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1856"><a href="#cb53-1856" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.3.1</span></span>
<span id="cb53-1857"><a href="#cb53-1857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1858"><a href="#cb53-1858" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Calculating confidence intervals</span></span>
<span id="cb53-1859"><a href="#cb53-1859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1860"><a href="#cb53-1860" aria-hidden="true" tabindex="-1"></a>You have learned about two methods for calculating confidence intervals: *the quantile method* and *the standard error method*. The standard error method involves using the inverse cumulative distribution function (inverse CDF) of the normal distribution to calculate confidence intervals. In this exercise, you'll perform these two methods on the Spotify data.</span>
<span id="cb53-1861"><a href="#cb53-1861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1862"><a href="#cb53-1862" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions </span></span>
<span id="cb53-1863"><a href="#cb53-1863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1864"><a href="#cb53-1864" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Generate a 95% confidence interval using the quantile method on the bootstrap distribution, setting the <span class="in">`0.025`</span> quantile as <span class="in">`lower_quant`</span> and the <span class="in">`0.975`</span> quantile as <span class="in">`upper_quant`</span>.</span>
<span id="cb53-1865"><a href="#cb53-1865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1866"><a href="#cb53-1866" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Generate a 95% confidence interval using the standard error method from the bootstrap distribution.</span>
<span id="cb53-1867"><a href="#cb53-1867" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate <span class="in">`point_estimate`</span> as the mean of <span class="in">`bootstrap_distribution`</span>, and <span class="in">`standard_error`</span> as the standard deviation of <span class="in">`bootstrap_distribution`</span>.</span>
<span id="cb53-1868"><a href="#cb53-1868" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate <span class="in">`lower_se`</span> as the <span class="in">`0.025`</span> quantile of an inv. CDF from a normal distribution with mean <span class="in">`point_estimate`</span> and standard deviation <span class="in">`standard_error`</span>.</span>
<span id="cb53-1869"><a href="#cb53-1869" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate <span class="in">`upper_se`</span> as the <span class="in">`0.975`</span> quantile of that same inv. CDF.</span>
<span id="cb53-1870"><a href="#cb53-1870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1873"><a href="#cb53-1873" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb53-1874"><a href="#cb53-1874" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb53-1875"><a href="#cb53-1875" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-1876"><a href="#cb53-1876" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-1877"><a href="#cb53-1877" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb53-1878"><a href="#cb53-1878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1879"><a href="#cb53-1879" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course array</span></span>
<span id="cb53-1880"><a href="#cb53-1880" aria-hidden="true" tabindex="-1"></a>spotify <span class="op">=</span> pd.read_feather(<span class="st">"datasets/spotify_2000_2020.feather"</span>)</span>
<span id="cb53-1881"><a href="#cb53-1881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1882"><a href="#cb53-1882" aria-hidden="true" tabindex="-1"></a>spotify_sample <span class="op">=</span> spotify.sample(n<span class="op">=</span><span class="dv">5000</span>, random_state<span class="op">=</span><span class="dv">2022</span>)</span>
<span id="cb53-1883"><a href="#cb53-1883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1884"><a href="#cb53-1884" aria-hidden="true" tabindex="-1"></a>mean_popularity_2000_boot <span class="op">=</span> []</span>
<span id="cb53-1885"><a href="#cb53-1885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1886"><a href="#cb53-1886" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a bootstrap distribution of 2000 replicates</span></span>
<span id="cb53-1887"><a href="#cb53-1887" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb53-1888"><a href="#cb53-1888" aria-hidden="true" tabindex="-1"></a>    mean_popularity_2000_boot.append(</span>
<span id="cb53-1889"><a href="#cb53-1889" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resample 500 rows and calculate the mean popularity     </span></span>
<span id="cb53-1890"><a href="#cb53-1890" aria-hidden="true" tabindex="-1"></a>        np.mean(spotify_sample.sample(frac<span class="op">=</span><span class="dv">1</span>, replace<span class="op">=</span><span class="va">True</span>)[<span class="st">'popularity'</span>])</span>
<span id="cb53-1891"><a href="#cb53-1891" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1892"><a href="#cb53-1892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1893"><a href="#cb53-1893" aria-hidden="true" tabindex="-1"></a><span class="co"># The bootstrap distribution results</span></span>
<span id="cb53-1894"><a href="#cb53-1894" aria-hidden="true" tabindex="-1"></a>bootstrap_distribution <span class="op">=</span> mean_popularity_2000_boot</span>
<span id="cb53-1895"><a href="#cb53-1895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1896"><a href="#cb53-1896" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a 95% confidence interval using the quantile method</span></span>
<span id="cb53-1897"><a href="#cb53-1897" aria-hidden="true" tabindex="-1"></a>lower_quant <span class="op">=</span> np.quantile(bootstrap_distribution, <span class="fl">0.025</span>)</span>
<span id="cb53-1898"><a href="#cb53-1898" aria-hidden="true" tabindex="-1"></a>upper_quant <span class="op">=</span> np.quantile(bootstrap_distribution, <span class="fl">0.975</span>)</span>
<span id="cb53-1899"><a href="#cb53-1899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1900"><a href="#cb53-1900" aria-hidden="true" tabindex="-1"></a><span class="co"># Print quantile method confidence interval</span></span>
<span id="cb53-1901"><a href="#cb53-1901" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>((lower_quant, upper_quant))</span>
<span id="cb53-1902"><a href="#cb53-1902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1903"><a href="#cb53-1903" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the mean and std dev of the bootstrap distribution</span></span>
<span id="cb53-1904"><a href="#cb53-1904" aria-hidden="true" tabindex="-1"></a>point_estimate <span class="op">=</span> np.mean(bootstrap_distribution)</span>
<span id="cb53-1905"><a href="#cb53-1905" aria-hidden="true" tabindex="-1"></a>standard_error <span class="op">=</span> np.std(bootstrap_distribution, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb53-1906"><a href="#cb53-1906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1907"><a href="#cb53-1907" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the lower limit of the confidence interval</span></span>
<span id="cb53-1908"><a href="#cb53-1908" aria-hidden="true" tabindex="-1"></a>lower_se <span class="op">=</span> norm.ppf(<span class="fl">0.025</span>, loc<span class="op">=</span>point_estimate, scale<span class="op">=</span>standard_error)</span>
<span id="cb53-1909"><a href="#cb53-1909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1910"><a href="#cb53-1910" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the upper limit of the confidence interval</span></span>
<span id="cb53-1911"><a href="#cb53-1911" aria-hidden="true" tabindex="-1"></a>upper_se <span class="op">=</span> norm.ppf(<span class="fl">0.975</span>, loc<span class="op">=</span>point_estimate, scale<span class="op">=</span>standard_error)</span>
<span id="cb53-1912"><a href="#cb53-1912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1913"><a href="#cb53-1913" aria-hidden="true" tabindex="-1"></a><span class="co"># Print standard error method confidence interval</span></span>
<span id="cb53-1914"><a href="#cb53-1914" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>((lower_se, upper_se))</span>
<span id="cb53-1915"><a href="#cb53-1915" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1916"><a href="#cb53-1916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1917"><a href="#cb53-1917" aria-hidden="true" tabindex="-1"></a><span class="fu">## Reference</span></span>
<span id="cb53-1918"><a href="#cb53-1918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1919"><a href="#cb53-1919" aria-hidden="true" tabindex="-1"></a>Sampling in Python in Intermediate Python Course for Associate Data Scientist in Python Carrer Track in DataCamp Inc by James Chapman.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>